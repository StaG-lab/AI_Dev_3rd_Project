{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21c18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 다운로드 받고 모델이 제대로 작동하는지 확인하는 명령어입니다.\n",
    "# 가상환경이 활성화된 터미널에서 아래 명령어를 실행하세요.\n",
    "# python -m unittest tests/test_model.py\n",
    "#-------------------------\n",
    "# Ran 1 test in 8.179s\n",
    "# OK\n",
    "#-------------------------\n",
    "# 위와 같은 결과를 얻었다면 모델이 정상적으로 작동하는 것입니다.\n",
    "# 코드 중 담당하신 MODEL_NAME에 해당하는 주석(#)을 제거하고 실행해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66595eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch                                            # [S1]\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "# from core.training.trainer import train_model              # [S2] 필요 시 주석 해제\n",
    "from core.training.trainer_v3 import train_model            # [S2] AMP/비동기/클리핑 지원\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import random, numpy as np, torch\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "def seed_worker(worker_id:int):\n",
    "    s = torch.initial_seed() % 2**32\n",
    "    random.seed(s); np.random.seed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16998a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # 속도 유지 버전: 결정론 강제는 생략 (필요시 아래 “옵션” 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c17a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b038e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "ctx = mp.get_context(\"spawn\")\n",
    "g = torch.Generator().manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff955ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NUM_WORKERS:  4\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 33664\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'resnet18' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "[E1 B50] loss=1.5671 acc=0.408\n",
      "[E1 B100] loss=1.4197 acc=0.479\n",
      "[E1 B150] loss=1.3376 acc=0.520\n",
      "[E1 B200] loss=1.2872 acc=0.546\n",
      "[E1 B250] loss=1.2501 acc=0.563\n",
      "Train Loss: 1.2428 Acc: 0.5670\n",
      "Val Loss: 1.0881 Acc: 0.6347\n",
      "\n",
      "Macro-F1: 0.6216\n",
      "Per-class F1: 0:0.934, 1:0.707, 2:0.654, 3:0.420, 4:0.313, 5:0.638, 6:0.684\n",
      "ConfMat (rows=true, cols=pred):\n",
      " [[960  16  12  10   1  11   7]\n",
      " [ 11 770  49  96  41   2  42]\n",
      " [ 19  49 653  86  61  22  73]\n",
      " [ 18 193 139 388 104  65 148]\n",
      " [  9 111 110 125 254 233 150]\n",
      " [ 13  14  47  62 143 644 116]\n",
      " [  8  14  23  24  25   4 686]]\n",
      "  -> Val Loss 개선됨! (1.0881)\n",
      "Epoch 2/100\n",
      "----------\n",
      "[E2 B50] loss=1.0427 acc=0.662\n",
      "[E2 B100] loss=1.0346 acc=0.666\n",
      "[E2 B150] loss=1.0328 acc=0.665\n",
      "[E2 B200] loss=1.0292 acc=0.666\n",
      "[E2 B250] loss=1.0303 acc=0.665\n",
      "Train Loss: 1.0297 Acc: 0.6650\n",
      "Val Loss: 1.0466 Acc: 0.6543\n",
      "\n",
      "Macro-F1: 0.6552\n",
      "Per-class F1: 0:0.941, 1:0.732, 2:0.678, 3:0.446, 4:0.405, 5:0.631, 6:0.752\n",
      "ConfMat (rows=true, cols=pred):\n",
      " [[995   5   9   2   2   2   2]\n",
      " [ 14 724  47 130  62   4  30]\n",
      " [ 20  26 678  79  95  19  46]\n",
      " [ 22 119 132 417 235  50  80]\n",
      " [ 12  68  98 124 441 177  72]\n",
      " [ 21   8  46  36 288 596  44]\n",
      " [ 13  16  26  25  63   3 638]]\n",
      "  -> Val Loss 개선됨! (1.0466)\n",
      "Epoch 3/100\n",
      "----------\n",
      "[E3 B50] loss=0.9603 acc=0.704\n",
      "[E3 B100] loss=0.9567 acc=0.702\n",
      "[E3 B150] loss=0.9549 acc=0.701\n",
      "[E3 B200] loss=0.9560 acc=0.700\n",
      "[E3 B250] loss=0.9558 acc=0.701\n",
      "Train Loss: 0.9545 Acc: 0.7012\n",
      "Val Loss: 1.0494 Acc: 0.6573\n",
      "\n",
      "Macro-F1: 0.6581\n",
      "Per-class F1: 0:0.941, 1:0.716, 2:0.662, 3:0.490, 4:0.411, 5:0.639, 6:0.748\n",
      "ConfMat (rows=true, cols=pred):\n",
      " [[984   8  15   6   0   3   1]\n",
      " [ 11 727  62 136  54   3  18]\n",
      " [ 15  31 682 112  67  19  37]\n",
      " [ 19 144 140 514 130  46  62]\n",
      " [  8  80 109 159 401 179  56]\n",
      " [ 16  10  53  78 236 606  40]\n",
      " [ 21  20  36  38  70   3 596]]\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/6\n",
      "Epoch 4/100\n",
      "----------\n",
      "[E4 B50] loss=0.8988 acc=0.728\n",
      "[E4 B100] loss=0.8957 acc=0.728\n",
      "[E4 B150] loss=0.8918 acc=0.728\n",
      "[E4 B200] loss=0.8909 acc=0.728\n",
      "[E4 B250] loss=0.8914 acc=0.727\n",
      "Train Loss: 0.8928 Acc: 0.7264\n",
      "Val Loss: 1.1095 Acc: 0.6388\n",
      "\n",
      "Macro-F1: 0.6426\n",
      "Per-class F1: 0:0.943, 1:0.701, 2:0.638, 3:0.448, 4:0.400, 5:0.621, 6:0.746\n",
      "ConfMat (rows=true, cols=pred):\n",
      " [[949  29   7   8   8   6  10]\n",
      " [ 10 717  22 101 121   5  35]\n",
      " [ 14  45 504 115 164  49  72]\n",
      " [  5 147  39 403 276  75 110]\n",
      " [  1  76  32  86 473 214 110]\n",
      " [  7  11   9  23 290 627  72]\n",
      " [  9   9   3   9  39   5 710]]\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/6\n",
      "Epoch 5/100\n",
      "----------\n",
      "[E5 B50] loss=0.8322 acc=0.754\n",
      "[E5 B100] loss=0.8272 acc=0.756\n",
      "[E5 B150] loss=0.8263 acc=0.757\n",
      "[E5 B200] loss=0.8256 acc=0.755\n",
      "[E5 B250] loss=0.8316 acc=0.752\n",
      "Train Loss: 0.8315 Acc: 0.7525\n",
      "Val Loss: 1.1224 Acc: 0.6302\n",
      "\n",
      "Macro-F1: 0.6319\n",
      "Per-class F1: 0:0.942, 1:0.691, 2:0.648, 3:0.376, 4:0.415, 5:0.616, 6:0.735\n",
      "ConfMat (rows=true, cols=pred):\n",
      " [[966  28  11   0   6   3   3]\n",
      " [  7 714  77  97 102   3  11]\n",
      " [ 11  38 710  51 112  22  19]\n",
      " [ 12 169 200 305 251  69  49]\n",
      " [  4  73 120  70 499 189  37]\n",
      " [ 14  13  49  15 335 594  19]\n",
      " [ 20  21  60  30 108   9 536]]\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/6\n",
      "Epoch 6/100\n",
      "----------\n",
      "[E6 B50] loss=0.7756 acc=0.782\n",
      "[E6 B100] loss=0.7741 acc=0.783\n",
      "[E6 B150] loss=0.7782 acc=0.780\n",
      "[E6 B200] loss=0.7780 acc=0.780\n",
      "[E6 B250] loss=0.7775 acc=0.780\n",
      "Train Loss: 0.7774 Acc: 0.7802\n",
      "Val Loss: 1.1232 Acc: 0.6455\n",
      "\n",
      "Macro-F1: 0.6435\n",
      "Per-class F1: 0:0.944, 1:0.698, 2:0.680, 3:0.465, 4:0.344, 5:0.623, 6:0.751\n",
      "ConfMat (rows=true, cols=pred):\n",
      " [[986   9   5   2   2   8   5]\n",
      " [ 12 655  55 146  94   9  40]\n",
      " [ 22  29 629  99  92  43  49]\n",
      " [ 19 105 100 450 169 108 104]\n",
      " [  7  53  61 140 332 289 110]\n",
      " [ 12   9  22  31 218 682  65]\n",
      " [ 15   5  14  14  30  11 695]]\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/6\n",
      "Epoch 7/100\n",
      "----------\n",
      "[E7 B50] loss=0.7178 acc=0.810\n",
      "[E7 B100] loss=0.7105 acc=0.812\n",
      "[E7 B150] loss=0.7112 acc=0.812\n",
      "[E7 B200] loss=0.7181 acc=0.807\n",
      "[E7 B250] loss=0.7230 acc=0.806\n",
      "Train Loss: 0.7244 Acc: 0.8050\n",
      "Val Loss: 1.1533 Acc: 0.6511\n",
      "\n",
      "Macro-F1: 0.6502\n",
      "Per-class F1: 0:0.946, 1:0.681, 2:0.689, 3:0.498, 4:0.365, 5:0.615, 6:0.757\n",
      "ConfMat (rows=true, cols=pred):\n",
      " [[961  15  14   7   7   4   9]\n",
      " [ 11 639  45 241  40   7  28]\n",
      " [ 10  27 683 128  50  25  40]\n",
      " [ 10  91 121 585  82  72  94]\n",
      " [  3  66  94 211 314 207  97]\n",
      " [  8  12  45  88 216 604  66]\n",
      " [ 12  15  18  33  19   6 681]]\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/6\n",
      "Epoch 8/100\n",
      "----------\n",
      "[E8 B50] loss=0.6557 acc=0.838\n",
      "[E8 B100] loss=0.6562 acc=0.838\n",
      "[E8 B150] loss=0.6649 acc=0.834\n",
      "[E8 B200] loss=0.6639 acc=0.833\n",
      "[E8 B250] loss=0.6690 acc=0.830\n",
      "Train Loss: 0.6683 Acc: 0.8304\n",
      "Val Loss: 1.2114 Acc: 0.6492\n",
      "\n",
      "Macro-F1: 0.6423\n",
      "Per-class F1: 0:0.947, 1:0.703, 2:0.663, 3:0.448, 4:0.372, 5:0.611, 6:0.752\n",
      "ConfMat (rows=true, cols=pred):\n",
      " [[981  13   9   4   4   1   5]\n",
      " [ 10 727  70 144  24   5  31]\n",
      " [ 11  42 731  74  48  17  40]\n",
      " [ 18 146 214 444  72  57 104]\n",
      " [  5 101 128 166 319 174  99]\n",
      " [ 18  15  54  73 241 571  67]\n",
      " [ 12  14  37  20  15   5 681]]\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/6\n",
      "\n",
      "Early stopping! 6 에폭 동안 성능 개선이 없었습니다.\n",
      "Training complete in 6m 22s\n",
      "Best Val Loss: 1.0466\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 시드 고정\n",
    "    \n",
    "    set_global_seed(SEED)\n",
    "    # 장치\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # 전역 성능 플래그 (Ampere↑ 권장)                           # [S3]\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    if hasattr(torch.backends.cudnn, \"allow_tf32\"):\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')  # PyTorch 2.0+\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 데이터 경로/모델 선택\n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_10_percent_SI\")   \n",
    "    MODEL_NAME = 'resnet18'\n",
    "    # MODEL_NAME = 'resnet50'\n",
    "    # MODEL_NAME = 'mobilenet_v3_small'\n",
    "    # MODEL_NAME = 'shufflenet_v2'\n",
    "    # MODEL_NAME = 'efficientnet_v2_s'\n",
    "    # MODEL_NAME = 'squeezenet'\n",
    "\n",
    "    NUM_CLASSES = 7\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "    RUN_DIR = Path(f\"./runs/{MODEL_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    BEST_CKPT_PATH = RUN_DIR / \"best.ckpt\"\n",
    "\n",
    "    # --- Epoch 슬라이더(가능 시) --------------------------------  # [S4]\n",
    "    NUM_EPOCHS_DEFAULT = 100\n",
    "    try:\n",
    "        from ipywidgets import IntSlider, display\n",
    "        _epoch_slider = IntSlider(description='Epochs', value=NUM_EPOCHS_DEFAULT, min=1, max=200, step=1)\n",
    "        display(_epoch_slider)\n",
    "        NUM_EPOCHS = _epoch_slider.value\n",
    "    except Exception:\n",
    "        NUM_EPOCHS = NUM_EPOCHS_DEFAULT\n",
    "\n",
    "    EARLY_STOPPING_PATIENCE = 6\n",
    "\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Transform\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ])\n",
    "\n",
    "    # Dataset\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset   = EmotionDataset(data_dir=DATA_DIR / \"val\",   transform=val_transform)\n",
    "\n",
    "    # DataLoader I/O 튜닝                                         # [S5]\n",
    "    NUM_WORKERS = min(4, os.cpu_count() or 2)\n",
    "    PIN_MEMORY = True\n",
    "    PERSISTENT = True if NUM_WORKERS > 0 else False\n",
    "    PREFETCH = 2 if NUM_WORKERS > 0 else None\n",
    "    print('NUM_WORKERS: ',NUM_WORKERS)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "\n",
    "    def build_loader(ds, bs, shuffle):\n",
    "        return DataLoader(\n",
    "            ds, batch_size=bs, shuffle=shuffle,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            persistent_workers=PERSISTENT,\n",
    "            prefetch_factor=PREFETCH,\n",
    "            drop_last=shuffle,  # train에서만 True → BN 안정화(선택)\n",
    "            # worker_init_fn=seed_worker,        # ★ 추가\n",
    "            generator=g,                       # ★ 추가\n",
    "            # multiprocessing_context=ctx         # ★\n",
    "        )\n",
    "    \n",
    "\n",
    "    train_loader = build_loader(train_dataset, BATCH_SIZE, True)\n",
    "    val_loader   = build_loader(val_dataset,   BATCH_SIZE, False)\n",
    "\n",
    "    # 클래스 수 동기화(폴더명 기반)                                # [S6]\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델/손실/옵티마\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    # # --- Progressive Unfreeze 설정 ---\n",
    "    # UNFREEZE_AT = 6          # 6에폭부터 백본 학습 시작\n",
    "    # LR_HEAD = 1e-3\n",
    "    # LR_BACKBONE = 1e-4\n",
    "    # WEIGHT_DECAY = 1e-4\n",
    "    LABEL_SMOOTH = 0.05\n",
    "\n",
    "    # # 1) 헤드만 학습: 백본 freeze\n",
    "    # for name, p in model.named_parameters():\n",
    "    #     if not any(k in name for k in ['fc', 'classifier']):  # resnet은 fc, mobilenet/shuffle은 classifier가 헤드\n",
    "    #         p.requires_grad = False\n",
    "\n",
    "    # # 2) 헤드 파라미터만 옵티마이저에\n",
    "    # head_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    # optimizer = optim.AdamW([{'params': head_params, 'lr': LR_HEAD, 'weight_decay': WEIGHT_DECAY}])\n",
    "\n",
    "    # 3) 라벨 스무딩 적용(권장)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "    \n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 학습 실행 (AMP/클리핑/로깅/체크포인트/TF32·cudnn)              # [S7]\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    # trained_model = train_model(\n",
    "    #     model, train_loader, val_loader,\n",
    "    #     criterion, optimizer, DEVICE,\n",
    "    #     num_epochs=NUM_EPOCHS,\n",
    "    #     patience=EARLY_STOPPING_PATIENCE,\n",
    "    #     use_amp=True, amp_in_val=True,\n",
    "    #     grad_clip=1.0, log_interval=50,\n",
    "    #     save_best_path=\"checkpoints/best.pt\",\n",
    "    #     set_tf32=True, cudnn_benchmark=True\n",
    "    # )\n",
    "    trained_model = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, DEVICE,\n",
    "    num_epochs=NUM_EPOCHS, patience=EARLY_STOPPING_PATIENCE,\n",
    "    use_amp=True, amp_in_val=True, grad_clip=1.0,\n",
    "    # 아래 3줄이 신규\n",
    "    # unfreeze_at=UNFREEZE_AT,\n",
    "    # lr_backbone=LR_BACKBONE,\n",
    "    print_metrics=['acc','macro_f1','per_class_f1','confmat'],\n",
    "    save_best_path=str(BEST_CKPT_PATH)\n",
    "    )\n",
    "\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55177100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Re-Eval] Val Acc: 0.6543  (4489/6861)\n",
      "→ CSV 저장: runs\\resnet18_20250818_030701\\val_predictions.csv\n",
      "→ 오분류 이미지 폴더: runs\\resnet18_20250818_030701\\misclassified\n",
      "클래스별 acc:\n",
      "    기쁨: 0.978  (correct 995/1017)\n",
      "    당황: 0.716  (correct 724/1011)\n",
      "    분노: 0.704  (correct 678/963)\n",
      "    불안: 0.395  (correct 417/1055)\n",
      "    상처: 0.445  (correct 441/992)\n",
      "    슬픔: 0.574  (correct 596/1039)\n",
      "    중립: 0.814  (correct 638/784)\n",
      "→ 혼동행렬 NPY: runs\\resnet18_20250818_030701\\confusion_matrix.npy\n",
      "→ 클래스별 acc JSON: runs\\resnet18_20250818_030701\\per_class_acc.json\n"
     ]
    }
   ],
   "source": [
    "# ===== 재검증 + 오분류 저장 + 혼동행렬/클래스별 acc =====\n",
    "import os, csv, json, shutil\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "\n",
    "# 0) 경로/런폴더 정리 (RUN_DIR/BEST_CKPT_PATH 없으면 만들어줌)\n",
    "if 'RUN_DIR' not in globals():\n",
    "    from datetime import datetime\n",
    "    RUN_DIR = Path(f\"./runs/{MODEL_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if 'BEST_CKPT_PATH' not in globals():\n",
    "    BEST_CKPT_PATH = RUN_DIR / \"best.ckpt\"\n",
    "\n",
    "MIS_DIR = RUN_DIR / \"misclassified\"\n",
    "MIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_PATH = RUN_DIR / \"val_predictions.csv\"\n",
    "CM_NPY  = RUN_DIR / \"confusion_matrix.npy\"\n",
    "PCA_JSON = RUN_DIR / \"per_class_acc.json\"\n",
    "\n",
    "# 1) 모델 로드 (체크포인트 있으면 ckpt, 없으면 trained_model로)\n",
    "device = DEVICE\n",
    "model = create_model(MODEL_NAME, NUM_CLASSES, pretrained=False).to(device)\n",
    "\n",
    "state_loaded = False\n",
    "if Path(BEST_CKPT_PATH).exists():\n",
    "    ckpt = torch.load(BEST_CKPT_PATH, map_location=device)\n",
    "    # trainer_v2 저장 포맷: {'model': state_dict, 'epoch': int, 'val_loss': float}\n",
    "    sd = ckpt['model'] if isinstance(ckpt, dict) and 'model' in ckpt else ckpt\n",
    "    model.load_state_dict(sd)\n",
    "    state_loaded = True\n",
    "elif 'trained_model' in globals():\n",
    "    model.load_state_dict(trained_model.state_dict())\n",
    "    state_loaded = True\n",
    "\n",
    "if not state_loaded:\n",
    "    print(\"⚠️ 체크포인트/훈련모델이 없어 현재 파라미터로 검증합니다.\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 2) 검증 데이터로더 준비 (있으면 재사용, 없으면 생성)\n",
    "if 'val_loader' not in globals():\n",
    "    val_dataset = EmotionDataset(DATA_DIR / \"val\", transform=val_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "else:\n",
    "    val_dataset = val_loader.dataset\n",
    "\n",
    "label_names = getattr(val_dataset, 'classes', [str(i) for i in range(NUM_CLASSES)])\n",
    "\n",
    "# 3) 단일 검증 패스 + 오분류 저장(원본 파일 복사) + CSV 기록\n",
    "total, correct = 0, 0\n",
    "rows = []  # CSV용\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bidx, (images, targets) in enumerate(val_loader):\n",
    "        images = images.to(device, non_blocking=True) if images.is_cuda else images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = model(images)\n",
    "        probs  = F.softmax(logits, dim=1)\n",
    "        confs, preds = probs.max(dim=1)\n",
    "\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total   += targets.size(0)\n",
    "\n",
    "        # 전제: val_loader는 shuffle=False → dataset.image_paths 순서 == 배치 순서\n",
    "        base = bidx * val_loader.batch_size\n",
    "        for i in range(targets.size(0)):\n",
    "            gidx     = base + i\n",
    "            true_idx = targets[i].item()\n",
    "            pred_idx = preds[i].item()\n",
    "            conf     = float(confs[i].item())\n",
    "\n",
    "            img_path = Path(val_dataset.image_paths[gidx])  # 원본 경로\n",
    "            true_nm, pred_nm = label_names[true_idx], label_names[pred_idx]\n",
    "\n",
    "            # 오분류면 원본 이미지를 mis_dir로 복사 (덮어쓰기 방지 이름)\n",
    "            if pred_idx != true_idx:\n",
    "                dst = MIS_DIR / f\"{gidx:06d}_true-{true_nm}_pred-{pred_nm}_{conf:.2f}{img_path.suffix}\"\n",
    "                try:\n",
    "                    shutil.copy2(img_path, dst)\n",
    "                except Exception as e:\n",
    "                    print(f\"copy fail: {img_path} -> {dst} ({e})\")\n",
    "\n",
    "            rows.append({\n",
    "                \"index\": gidx,\n",
    "                \"path\": str(img_path),\n",
    "                \"true\": true_nm,\n",
    "                \"pred\": pred_nm,\n",
    "                \"conf\": conf,\n",
    "                \"is_correct\": int(pred_idx == true_idx),\n",
    "            })\n",
    "\n",
    "acc = correct / max(1, total)\n",
    "print(f\"[Re-Eval] Val Acc: {acc:.4f}  ({correct}/{total})\")\n",
    "\n",
    "# CSV 저장\n",
    "with open(CSV_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"index\",\"path\",\"true\",\"pred\",\"conf\",\"is_correct\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "print(f\"→ CSV 저장: {CSV_PATH}\")\n",
    "print(f\"→ 오분류 이미지 폴더: {MIS_DIR}\")\n",
    "\n",
    "# 4) 혼동행렬/클래스별 acc 저장\n",
    "cm = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=int)\n",
    "for r in rows:\n",
    "    ti = label_names.index(r[\"true\"])\n",
    "    pi = label_names.index(r[\"pred\"])\n",
    "    cm[ti, pi] += 1\n",
    "\n",
    "np.save(CM_NPY, cm)\n",
    "\n",
    "per_class_acc = {}\n",
    "for i, name in enumerate(label_names):\n",
    "    denom = int(cm[i, :].sum())\n",
    "    per_class_acc[name] = float(cm[i, i] / denom) if denom > 0 else 0.0\n",
    "\n",
    "with open(PCA_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(per_class_acc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"클래스별 acc:\")\n",
    "for name in label_names:\n",
    "    print(f\"  {name:>4s}: {per_class_acc[name]:.3f}  (correct {cm[label_names.index(name), label_names.index(name)]}/{cm[label_names.index(name), :].sum()})\")\n",
    "print(f\"→ 혼동행렬 NPY: {CM_NPY}\")\n",
    "print(f\"→ 클래스별 acc JSON: {PCA_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37edf9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feellog-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
