{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d0d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 17975\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'shufflenet_v2' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.8634 Acc: 0.2969\n",
      "  [Batch 40/280] Train Loss: 1.7557 Acc: 0.3594\n",
      "  [Batch 60/280] Train Loss: 1.4752 Acc: 0.4688\n",
      "  [Batch 80/280] Train Loss: 1.3863 Acc: 0.4062\n",
      "  [Batch 100/280] Train Loss: 1.2870 Acc: 0.5469\n",
      "  [Batch 120/280] Train Loss: 1.2748 Acc: 0.4688\n",
      "  [Batch 140/280] Train Loss: 1.2890 Acc: 0.5156\n",
      "  [Batch 160/280] Train Loss: 1.1638 Acc: 0.5938\n",
      "  [Batch 180/280] Train Loss: 1.2568 Acc: 0.5312\n",
      "  [Batch 200/280] Train Loss: 1.2797 Acc: 0.4375\n",
      "  [Batch 220/280] Train Loss: 1.1182 Acc: 0.5469\n",
      "  [Batch 240/280] Train Loss: 0.9777 Acc: 0.6250\n",
      "  [Batch 260/280] Train Loss: 1.1293 Acc: 0.6094\n",
      "  [Batch 280/280] Train Loss: 1.0380 Acc: 0.6406\n",
      "Train Loss: 1.3606 Acc: 0.4816\n",
      "Val Loss: 1.0804 Acc: 0.5972\n",
      "\n",
      "  -> Val Loss 개선됨! (1.0804) 모델 저장.\n",
      "Epoch 2/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.1779 Acc: 0.5312\n",
      "  [Batch 40/280] Train Loss: 1.1680 Acc: 0.5312\n",
      "  [Batch 60/280] Train Loss: 1.1423 Acc: 0.5625\n",
      "  [Batch 80/280] Train Loss: 1.0747 Acc: 0.6250\n",
      "  [Batch 100/280] Train Loss: 1.0552 Acc: 0.5781\n",
      "  [Batch 120/280] Train Loss: 1.2491 Acc: 0.5156\n",
      "  [Batch 140/280] Train Loss: 1.0496 Acc: 0.6562\n",
      "  [Batch 160/280] Train Loss: 1.3610 Acc: 0.4688\n",
      "  [Batch 180/280] Train Loss: 1.0604 Acc: 0.5781\n",
      "  [Batch 200/280] Train Loss: 1.0642 Acc: 0.5781\n",
      "  [Batch 220/280] Train Loss: 1.1058 Acc: 0.5156\n",
      "  [Batch 240/280] Train Loss: 1.1797 Acc: 0.5938\n",
      "  [Batch 260/280] Train Loss: 1.1470 Acc: 0.5469\n",
      "  [Batch 280/280] Train Loss: 1.1968 Acc: 0.6094\n",
      "Train Loss: 1.0945 Acc: 0.5893\n",
      "Val Loss: 1.0026 Acc: 0.6296\n",
      "\n",
      "  -> Val Loss 개선됨! (1.0026) 모델 저장.\n",
      "Epoch 3/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.9128 Acc: 0.6250\n",
      "  [Batch 40/280] Train Loss: 1.0743 Acc: 0.5781\n",
      "  [Batch 60/280] Train Loss: 0.9482 Acc: 0.6562\n",
      "  [Batch 80/280] Train Loss: 1.0564 Acc: 0.5781\n",
      "  [Batch 100/280] Train Loss: 1.1760 Acc: 0.5781\n",
      "  [Batch 120/280] Train Loss: 1.0157 Acc: 0.6719\n",
      "  [Batch 140/280] Train Loss: 1.0722 Acc: 0.5625\n",
      "  [Batch 160/280] Train Loss: 0.8978 Acc: 0.6719\n",
      "  [Batch 180/280] Train Loss: 0.8396 Acc: 0.6719\n",
      "  [Batch 200/280] Train Loss: 1.0347 Acc: 0.6094\n",
      "  [Batch 220/280] Train Loss: 0.9460 Acc: 0.6562\n",
      "  [Batch 240/280] Train Loss: 0.8442 Acc: 0.7031\n",
      "  [Batch 260/280] Train Loss: 0.9490 Acc: 0.6719\n",
      "  [Batch 280/280] Train Loss: 1.4227 Acc: 0.4688\n",
      "Train Loss: 1.0254 Acc: 0.6207\n",
      "Val Loss: 0.9552 Acc: 0.6466\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9552) 모델 저장.\n",
      "Epoch 4/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.2412 Acc: 0.5625\n",
      "  [Batch 40/280] Train Loss: 1.0090 Acc: 0.6250\n",
      "  [Batch 60/280] Train Loss: 0.9312 Acc: 0.6406\n",
      "  [Batch 80/280] Train Loss: 1.1870 Acc: 0.5156\n",
      "  [Batch 100/280] Train Loss: 1.1083 Acc: 0.5469\n",
      "  [Batch 120/280] Train Loss: 0.9995 Acc: 0.5938\n",
      "  [Batch 140/280] Train Loss: 1.2087 Acc: 0.6094\n",
      "  [Batch 160/280] Train Loss: 0.8697 Acc: 0.7344\n",
      "  [Batch 180/280] Train Loss: 0.9130 Acc: 0.7188\n",
      "  [Batch 200/280] Train Loss: 0.9900 Acc: 0.6094\n",
      "  [Batch 220/280] Train Loss: 0.8634 Acc: 0.7188\n",
      "  [Batch 240/280] Train Loss: 1.1091 Acc: 0.5469\n",
      "  [Batch 260/280] Train Loss: 0.9404 Acc: 0.6875\n",
      "  [Batch 280/280] Train Loss: 0.9776 Acc: 0.6719\n",
      "Train Loss: 0.9664 Acc: 0.6393\n",
      "Val Loss: 0.9535 Acc: 0.6566\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9535) 모델 저장.\n",
      "Epoch 5/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7193 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.8662 Acc: 0.7188\n",
      "  [Batch 60/280] Train Loss: 0.7888 Acc: 0.6719\n",
      "  [Batch 80/280] Train Loss: 1.0090 Acc: 0.5625\n",
      "  [Batch 100/280] Train Loss: 0.9345 Acc: 0.6250\n",
      "  [Batch 120/280] Train Loss: 1.0015 Acc: 0.6094\n",
      "  [Batch 140/280] Train Loss: 0.8070 Acc: 0.6719\n",
      "  [Batch 160/280] Train Loss: 0.7478 Acc: 0.7500\n",
      "  [Batch 180/280] Train Loss: 1.0875 Acc: 0.5938\n",
      "  [Batch 200/280] Train Loss: 0.8716 Acc: 0.7031\n",
      "  [Batch 220/280] Train Loss: 0.6905 Acc: 0.7656\n",
      "  [Batch 240/280] Train Loss: 1.0504 Acc: 0.5938\n",
      "  [Batch 260/280] Train Loss: 1.1041 Acc: 0.6094\n",
      "  [Batch 280/280] Train Loss: 0.8330 Acc: 0.7188\n",
      "Train Loss: 0.9288 Acc: 0.6538\n",
      "Val Loss: 1.0060 Acc: 0.6376\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 6/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7432 Acc: 0.6875\n",
      "  [Batch 40/280] Train Loss: 0.7956 Acc: 0.7031\n",
      "  [Batch 60/280] Train Loss: 0.9596 Acc: 0.6250\n",
      "  [Batch 80/280] Train Loss: 1.0296 Acc: 0.6875\n",
      "  [Batch 100/280] Train Loss: 0.9496 Acc: 0.7031\n",
      "  [Batch 120/280] Train Loss: 0.8868 Acc: 0.6250\n",
      "  [Batch 140/280] Train Loss: 0.9597 Acc: 0.6250\n",
      "  [Batch 160/280] Train Loss: 1.0022 Acc: 0.7344\n",
      "  [Batch 180/280] Train Loss: 0.9391 Acc: 0.5938\n",
      "  [Batch 200/280] Train Loss: 0.9581 Acc: 0.7031\n",
      "  [Batch 220/280] Train Loss: 0.8964 Acc: 0.6562\n",
      "  [Batch 240/280] Train Loss: 0.8848 Acc: 0.6719\n",
      "  [Batch 260/280] Train Loss: 0.7755 Acc: 0.7188\n",
      "  [Batch 280/280] Train Loss: 0.7591 Acc: 0.7344\n",
      "Train Loss: 0.8979 Acc: 0.6684\n",
      "Val Loss: 0.9970 Acc: 0.6376\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 7/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7274 Acc: 0.7344\n",
      "  [Batch 40/280] Train Loss: 0.9518 Acc: 0.6250\n",
      "  [Batch 60/280] Train Loss: 0.7586 Acc: 0.6875\n",
      "  [Batch 80/280] Train Loss: 1.1256 Acc: 0.6562\n",
      "  [Batch 100/280] Train Loss: 0.7841 Acc: 0.6875\n",
      "  [Batch 120/280] Train Loss: 0.8593 Acc: 0.6562\n",
      "  [Batch 140/280] Train Loss: 0.7378 Acc: 0.7344\n",
      "  [Batch 160/280] Train Loss: 1.0748 Acc: 0.5938\n",
      "  [Batch 180/280] Train Loss: 0.7795 Acc: 0.7188\n",
      "  [Batch 200/280] Train Loss: 1.0452 Acc: 0.5781\n",
      "  [Batch 220/280] Train Loss: 0.9736 Acc: 0.6250\n",
      "  [Batch 240/280] Train Loss: 0.8158 Acc: 0.7188\n",
      "  [Batch 260/280] Train Loss: 0.8858 Acc: 0.6875\n",
      "  [Batch 280/280] Train Loss: 0.9511 Acc: 0.6562\n",
      "Train Loss: 0.8722 Acc: 0.6778\n",
      "Val Loss: 0.9888 Acc: 0.6537\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 8/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8074 Acc: 0.7188\n",
      "  [Batch 40/280] Train Loss: 0.8133 Acc: 0.6719\n",
      "  [Batch 60/280] Train Loss: 0.7934 Acc: 0.7031\n",
      "  [Batch 80/280] Train Loss: 0.7166 Acc: 0.7500\n",
      "  [Batch 100/280] Train Loss: 0.8078 Acc: 0.6875\n",
      "  [Batch 120/280] Train Loss: 0.6745 Acc: 0.7500\n",
      "  [Batch 140/280] Train Loss: 0.6772 Acc: 0.7188\n",
      "  [Batch 160/280] Train Loss: 0.6673 Acc: 0.8125\n",
      "  [Batch 180/280] Train Loss: 0.6078 Acc: 0.7500\n",
      "  [Batch 200/280] Train Loss: 0.7572 Acc: 0.6875\n",
      "  [Batch 220/280] Train Loss: 0.7527 Acc: 0.7188\n",
      "  [Batch 240/280] Train Loss: 0.6752 Acc: 0.7188\n",
      "  [Batch 260/280] Train Loss: 0.8326 Acc: 0.6719\n",
      "  [Batch 280/280] Train Loss: 0.7003 Acc: 0.6875\n",
      "Train Loss: 0.7506 Acc: 0.7243\n",
      "Val Loss: 0.8545 Acc: 0.6965\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8545) 모델 저장.\n",
      "Epoch 9/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6766 Acc: 0.7188\n",
      "  [Batch 40/280] Train Loss: 0.6245 Acc: 0.7812\n",
      "  [Batch 60/280] Train Loss: 0.6553 Acc: 0.7500\n",
      "  [Batch 80/280] Train Loss: 0.7545 Acc: 0.7344\n",
      "  [Batch 100/280] Train Loss: 0.6641 Acc: 0.7500\n",
      "  [Batch 120/280] Train Loss: 0.6507 Acc: 0.8125\n",
      "  [Batch 140/280] Train Loss: 0.5406 Acc: 0.8281\n",
      "  [Batch 160/280] Train Loss: 0.6886 Acc: 0.7969\n",
      "  [Batch 180/280] Train Loss: 0.6775 Acc: 0.7188\n",
      "  [Batch 200/280] Train Loss: 0.7975 Acc: 0.7344\n",
      "  [Batch 220/280] Train Loss: 0.6771 Acc: 0.7656\n",
      "  [Batch 240/280] Train Loss: 0.6660 Acc: 0.7031\n",
      "  [Batch 260/280] Train Loss: 0.7350 Acc: 0.6875\n",
      "  [Batch 280/280] Train Loss: 0.5669 Acc: 0.7969\n",
      "Train Loss: 0.7104 Acc: 0.7337\n",
      "Val Loss: 0.8492 Acc: 0.6995\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8492) 모델 저장.\n",
      "Epoch 10/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5694 Acc: 0.7812\n",
      "  [Batch 40/280] Train Loss: 0.6377 Acc: 0.7500\n",
      "  [Batch 60/280] Train Loss: 0.6514 Acc: 0.7812\n",
      "  [Batch 80/280] Train Loss: 0.6606 Acc: 0.7656\n",
      "  [Batch 100/280] Train Loss: 0.6014 Acc: 0.8125\n",
      "  [Batch 120/280] Train Loss: 0.5919 Acc: 0.8438\n",
      "  [Batch 140/280] Train Loss: 0.4985 Acc: 0.8281\n",
      "  [Batch 160/280] Train Loss: 0.5419 Acc: 0.8125\n",
      "  [Batch 180/280] Train Loss: 0.7428 Acc: 0.7500\n",
      "  [Batch 200/280] Train Loss: 0.5628 Acc: 0.7656\n",
      "  [Batch 220/280] Train Loss: 1.0423 Acc: 0.5781\n",
      "  [Batch 240/280] Train Loss: 0.7095 Acc: 0.7500\n",
      "  [Batch 260/280] Train Loss: 0.9183 Acc: 0.7188\n",
      "  [Batch 280/280] Train Loss: 0.8330 Acc: 0.6719\n",
      "Train Loss: 0.6783 Acc: 0.7508\n",
      "Val Loss: 0.8609 Acc: 0.7020\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 11/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5046 Acc: 0.8125\n",
      "  [Batch 40/280] Train Loss: 0.5955 Acc: 0.7969\n",
      "  [Batch 60/280] Train Loss: 0.7982 Acc: 0.7188\n",
      "  [Batch 80/280] Train Loss: 0.5079 Acc: 0.8281\n",
      "  [Batch 100/280] Train Loss: 0.4737 Acc: 0.8438\n",
      "  [Batch 120/280] Train Loss: 0.5963 Acc: 0.7500\n",
      "  [Batch 140/280] Train Loss: 0.5849 Acc: 0.7812\n",
      "  [Batch 160/280] Train Loss: 0.6430 Acc: 0.7656\n",
      "  [Batch 180/280] Train Loss: 0.4093 Acc: 0.9062\n",
      "  [Batch 200/280] Train Loss: 0.5667 Acc: 0.7656\n",
      "  [Batch 220/280] Train Loss: 0.6347 Acc: 0.7656\n",
      "  [Batch 240/280] Train Loss: 0.6756 Acc: 0.7812\n",
      "  [Batch 260/280] Train Loss: 0.5995 Acc: 0.7500\n",
      "  [Batch 280/280] Train Loss: 0.5918 Acc: 0.7812\n",
      "Train Loss: 0.6514 Acc: 0.7621\n",
      "Val Loss: 0.8642 Acc: 0.7020\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 12/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7465 Acc: 0.6875\n",
      "  [Batch 40/280] Train Loss: 0.9351 Acc: 0.6250\n",
      "  [Batch 60/280] Train Loss: 0.7461 Acc: 0.7500\n",
      "  [Batch 80/280] Train Loss: 0.5324 Acc: 0.7812\n",
      "  [Batch 100/280] Train Loss: 0.5773 Acc: 0.8438\n",
      "  [Batch 120/280] Train Loss: 0.6477 Acc: 0.7812\n",
      "  [Batch 140/280] Train Loss: 0.8133 Acc: 0.6719\n",
      "  [Batch 160/280] Train Loss: 0.6697 Acc: 0.7500\n",
      "  [Batch 180/280] Train Loss: 0.5969 Acc: 0.7188\n",
      "  [Batch 200/280] Train Loss: 0.8820 Acc: 0.7344\n",
      "  [Batch 220/280] Train Loss: 0.5329 Acc: 0.7969\n",
      "  [Batch 240/280] Train Loss: 0.8026 Acc: 0.7188\n",
      "  [Batch 260/280] Train Loss: 0.4903 Acc: 0.8125\n",
      "  [Batch 280/280] Train Loss: 0.5992 Acc: 0.8125\n",
      "Train Loss: 0.6346 Acc: 0.7651\n",
      "Val Loss: 0.8712 Acc: 0.7008\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 13/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5341 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.5665 Acc: 0.8125\n",
      "  [Batch 60/280] Train Loss: 0.5691 Acc: 0.7500\n",
      "  [Batch 80/280] Train Loss: 0.5039 Acc: 0.8125\n",
      "  [Batch 100/280] Train Loss: 0.6941 Acc: 0.7656\n",
      "  [Batch 120/280] Train Loss: 0.7781 Acc: 0.6875\n",
      "  [Batch 140/280] Train Loss: 0.6547 Acc: 0.7344\n",
      "  [Batch 160/280] Train Loss: 0.6353 Acc: 0.8281\n",
      "  [Batch 180/280] Train Loss: 0.4992 Acc: 0.8906\n",
      "  [Batch 200/280] Train Loss: 0.5484 Acc: 0.7969\n",
      "  [Batch 220/280] Train Loss: 0.7949 Acc: 0.7344\n",
      "  [Batch 240/280] Train Loss: 0.4569 Acc: 0.7969\n",
      "  [Batch 260/280] Train Loss: 0.5798 Acc: 0.7344\n",
      "  [Batch 280/280] Train Loss: 0.6361 Acc: 0.7500\n",
      "Train Loss: 0.6102 Acc: 0.7729\n",
      "Val Loss: 0.8790 Acc: 0.7072\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 14/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4477 Acc: 0.8281\n",
      "  [Batch 40/280] Train Loss: 0.5993 Acc: 0.8125\n",
      "  [Batch 60/280] Train Loss: 0.5722 Acc: 0.8125\n",
      "  [Batch 80/280] Train Loss: 0.5975 Acc: 0.8125\n",
      "  [Batch 100/280] Train Loss: 0.6092 Acc: 0.7188\n",
      "  [Batch 120/280] Train Loss: 0.4688 Acc: 0.8281\n",
      "  [Batch 140/280] Train Loss: 0.6374 Acc: 0.7656\n",
      "  [Batch 160/280] Train Loss: 0.5603 Acc: 0.7656\n",
      "  [Batch 180/280] Train Loss: 0.8002 Acc: 0.6875\n",
      "  [Batch 200/280] Train Loss: 0.6166 Acc: 0.7969\n",
      "  [Batch 220/280] Train Loss: 0.5828 Acc: 0.7969\n",
      "  [Batch 240/280] Train Loss: 0.6128 Acc: 0.7656\n",
      "  [Batch 260/280] Train Loss: 0.6761 Acc: 0.7500\n",
      "  [Batch 280/280] Train Loss: 0.5464 Acc: 0.8125\n",
      "Train Loss: 0.5994 Acc: 0.7814\n",
      "Val Loss: 0.8982 Acc: 0.6958\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/10\n",
      "Epoch 15/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4532 Acc: 0.8750\n",
      "  [Batch 40/280] Train Loss: 0.7149 Acc: 0.7344\n",
      "  [Batch 60/280] Train Loss: 0.5178 Acc: 0.8125\n",
      "  [Batch 80/280] Train Loss: 0.7132 Acc: 0.7188\n",
      "  [Batch 100/280] Train Loss: 0.6745 Acc: 0.7500\n",
      "  [Batch 120/280] Train Loss: 0.7767 Acc: 0.7812\n",
      "  [Batch 140/280] Train Loss: 0.5639 Acc: 0.7969\n",
      "  [Batch 160/280] Train Loss: 0.8524 Acc: 0.7031\n",
      "  [Batch 180/280] Train Loss: 0.5871 Acc: 0.7500\n",
      "  [Batch 200/280] Train Loss: 0.4278 Acc: 0.8125\n",
      "  [Batch 220/280] Train Loss: 0.4844 Acc: 0.8281\n",
      "  [Batch 240/280] Train Loss: 0.5198 Acc: 0.8125\n",
      "  [Batch 260/280] Train Loss: 0.6746 Acc: 0.7344\n",
      "  [Batch 280/280] Train Loss: 0.4225 Acc: 0.8125\n",
      "Train Loss: 0.5718 Acc: 0.7916\n",
      "Val Loss: 0.8956 Acc: 0.7047\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/10\n",
      "Epoch 16/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7007 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.5649 Acc: 0.7344\n",
      "  [Batch 60/280] Train Loss: 0.8331 Acc: 0.6250\n",
      "  [Batch 80/280] Train Loss: 0.5292 Acc: 0.7656\n",
      "  [Batch 100/280] Train Loss: 0.5268 Acc: 0.7969\n",
      "  [Batch 120/280] Train Loss: 0.7478 Acc: 0.7188\n",
      "  [Batch 140/280] Train Loss: 0.4998 Acc: 0.8125\n",
      "  [Batch 160/280] Train Loss: 0.8907 Acc: 0.6875\n",
      "  [Batch 180/280] Train Loss: 0.5772 Acc: 0.8281\n",
      "  [Batch 200/280] Train Loss: 0.6343 Acc: 0.7344\n",
      "  [Batch 220/280] Train Loss: 0.4852 Acc: 0.8281\n",
      "  [Batch 240/280] Train Loss: 0.5815 Acc: 0.7656\n",
      "  [Batch 260/280] Train Loss: 0.5136 Acc: 0.8125\n",
      "  [Batch 280/280] Train Loss: 0.5884 Acc: 0.7969\n",
      "Train Loss: 0.5653 Acc: 0.7922\n",
      "Val Loss: 0.8929 Acc: 0.7047\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 7/10\n",
      "Epoch 17/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4216 Acc: 0.8438\n",
      "  [Batch 40/280] Train Loss: 0.4783 Acc: 0.7812\n",
      "  [Batch 60/280] Train Loss: 0.4546 Acc: 0.8594\n",
      "  [Batch 80/280] Train Loss: 0.5441 Acc: 0.8125\n",
      "  [Batch 100/280] Train Loss: 0.6810 Acc: 0.7188\n",
      "  [Batch 120/280] Train Loss: 0.5704 Acc: 0.8125\n",
      "  [Batch 140/280] Train Loss: 0.4375 Acc: 0.8438\n",
      "  [Batch 160/280] Train Loss: 0.3268 Acc: 0.8750\n",
      "  [Batch 180/280] Train Loss: 0.3991 Acc: 0.8594\n",
      "  [Batch 200/280] Train Loss: 0.7085 Acc: 0.7188\n",
      "  [Batch 220/280] Train Loss: 0.6827 Acc: 0.8281\n",
      "  [Batch 240/280] Train Loss: 0.6303 Acc: 0.7188\n",
      "  [Batch 260/280] Train Loss: 0.3724 Acc: 0.9219\n",
      "  [Batch 280/280] Train Loss: 0.5530 Acc: 0.8281\n",
      "Train Loss: 0.5711 Acc: 0.7859\n",
      "Val Loss: 0.8932 Acc: 0.7056\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 8/10\n",
      "Epoch 18/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4770 Acc: 0.7812\n",
      "  [Batch 40/280] Train Loss: 0.5774 Acc: 0.8125\n",
      "  [Batch 60/280] Train Loss: 0.6239 Acc: 0.7188\n",
      "  [Batch 80/280] Train Loss: 0.6020 Acc: 0.7969\n",
      "  [Batch 100/280] Train Loss: 0.6973 Acc: 0.7656\n",
      "  [Batch 120/280] Train Loss: 0.5772 Acc: 0.7656\n",
      "  [Batch 140/280] Train Loss: 0.6674 Acc: 0.7969\n",
      "  [Batch 160/280] Train Loss: 0.6135 Acc: 0.7812\n",
      "  [Batch 180/280] Train Loss: 0.7864 Acc: 0.7031\n",
      "  [Batch 200/280] Train Loss: 0.3758 Acc: 0.8594\n",
      "  [Batch 220/280] Train Loss: 0.7165 Acc: 0.7031\n",
      "  [Batch 240/280] Train Loss: 0.5866 Acc: 0.7969\n",
      "  [Batch 260/280] Train Loss: 0.3601 Acc: 0.8906\n",
      "  [Batch 280/280] Train Loss: 0.6254 Acc: 0.7969\n",
      "Train Loss: 0.5667 Acc: 0.7907\n",
      "Val Loss: 0.9038 Acc: 0.7042\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 9/10\n",
      "Epoch 19/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4571 Acc: 0.8438\n",
      "  [Batch 40/280] Train Loss: 0.4712 Acc: 0.7812\n",
      "  [Batch 60/280] Train Loss: 0.5303 Acc: 0.8125\n",
      "  [Batch 80/280] Train Loss: 0.5871 Acc: 0.7969\n",
      "  [Batch 100/280] Train Loss: 0.4815 Acc: 0.8750\n",
      "  [Batch 120/280] Train Loss: 0.5764 Acc: 0.8125\n",
      "  [Batch 140/280] Train Loss: 0.4383 Acc: 0.8438\n",
      "  [Batch 160/280] Train Loss: 0.4672 Acc: 0.7969\n",
      "  [Batch 180/280] Train Loss: 0.5071 Acc: 0.7969\n",
      "  [Batch 200/280] Train Loss: 0.7982 Acc: 0.7031\n",
      "  [Batch 220/280] Train Loss: 0.6329 Acc: 0.7188\n",
      "  [Batch 240/280] Train Loss: 0.7891 Acc: 0.7188\n",
      "  [Batch 260/280] Train Loss: 0.5536 Acc: 0.8125\n",
      "  [Batch 280/280] Train Loss: 0.6105 Acc: 0.7969\n",
      "Train Loss: 0.5627 Acc: 0.7934\n",
      "Val Loss: 0.9054 Acc: 0.7033\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 10/10\n",
      "\n",
      "Early stopping! 10 에폭 동안 성능 개선이 없었습니다.\n",
      "--------------------------------------------------\n",
      "Training complete in 10m 52s\n",
      "Saved Epoch: 9\n",
      "--------------------------------------------------\n",
      "Saved Train Loss: 0.7104\n",
      "Saved Train Acc: 0.7337\n",
      "Saved Val Loss: 0.8492\n",
      "Saved Val Acc: 0.6995\n",
      "--------------------------------------------------\n",
      "Best Train Loss: 0.5653\n",
      "Best Train Acc: 0.7922\n",
      "Best Val Loss: 0.8492\n",
      "Best Val Acc: 0.7072\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer_speedup import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_5_percent_verified_processed\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'            #철원\n",
    "    #MODEL_NAME = 'resnet50'            #철원\n",
    "    MODEL_NAME = 'shufflenet_v2'        #철원\n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   #승현님\n",
    "    #MODEL_NAME = 'squeezenet'          #승현님\n",
    "    \n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "\n",
    "    \n",
    "    # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "    #train_transform = transforms.Compose([\n",
    "    #    transforms.Resize((224, 224)),\n",
    "    #    transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "    #    transforms.RandomRotation(15),           # -15도 ~ 15도 사이로 랜덤 회전\n",
    "    #    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 밝기, 대비, 채도 조절\n",
    "    #    transforms.ToTensor(),\n",
    "    #    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    #])\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        # TrivialAugmentWide 추가, 이미지에 다양한 변형(자르기, 색상 왜곡, 회전 등)을 알아서 최적의 강도로 적용, 과적합 방지.\n",
    "        transforms.TrivialAugmentWide(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 증강이 없는 검증/테스트용 Transform 정의\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1) # 7 에폭마다 학습률을 0.1배로 감소\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS, \n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1ef8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 32407\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'shufflenet_v2' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.8510 Acc: 0.3438\n",
      "  [Batch 506/506] Train Loss: 1.0784 Acc: 0.6562\n",
      "Train Loss: 1.2648 Acc: 0.5181\n",
      "Val Loss: 1.0263 Acc: 0.6116\n",
      "\n",
      "  -> Val Loss 개선됨! (1.0263) 모델 저장.\n",
      "Epoch 2/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.1346 Acc: 0.5625\n",
      "  [Batch 506/506] Train Loss: 1.0369 Acc: 0.5938\n",
      "Train Loss: 1.0666 Acc: 0.5973\n",
      "Val Loss: 0.9939 Acc: 0.6269\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9939) 모델 저장.\n",
      "Epoch 3/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.0994 Acc: 0.5469\n",
      "  [Batch 506/506] Train Loss: 0.9762 Acc: 0.6094\n",
      "Train Loss: 1.0044 Acc: 0.6257\n",
      "Val Loss: 1.0342 Acc: 0.6121\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 4/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.1124 Acc: 0.6719\n",
      "  [Batch 506/506] Train Loss: 0.8319 Acc: 0.6875\n",
      "Train Loss: 0.9743 Acc: 0.6353\n",
      "Val Loss: 0.9563 Acc: 0.6409\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9563) 모델 저장.\n",
      "Epoch 5/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.1203 Acc: 0.5781\n",
      "  [Batch 506/506] Train Loss: 0.8306 Acc: 0.6875\n",
      "Train Loss: 0.9446 Acc: 0.6432\n",
      "Val Loss: 0.9406 Acc: 0.6465\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9406) 모델 저장.\n",
      "Epoch 6/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.8389 Acc: 0.6562\n",
      "  [Batch 506/506] Train Loss: 0.9943 Acc: 0.6406\n",
      "Train Loss: 0.9254 Acc: 0.6520\n",
      "Val Loss: 0.9631 Acc: 0.6476\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 7/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.9689 Acc: 0.7656\n",
      "  [Batch 506/506] Train Loss: 1.1444 Acc: 0.5625\n",
      "Train Loss: 0.8962 Acc: 0.6625\n",
      "Val Loss: 0.9108 Acc: 0.6675\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9108) 모델 저장.\n",
      "Epoch 8/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6413 Acc: 0.8125\n",
      "  [Batch 506/506] Train Loss: 0.7254 Acc: 0.7188\n",
      "Train Loss: 0.8032 Acc: 0.7012\n",
      "Val Loss: 0.8461 Acc: 0.6881\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8461) 모델 저장.\n",
      "Epoch 9/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6236 Acc: 0.8281\n",
      "  [Batch 506/506] Train Loss: 0.7620 Acc: 0.7031\n",
      "Train Loss: 0.7562 Acc: 0.7166\n",
      "Val Loss: 0.8438 Acc: 0.6906\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8438) 모델 저장.\n",
      "Epoch 10/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.5241 Acc: 0.8125\n",
      "  [Batch 506/506] Train Loss: 0.9199 Acc: 0.7031\n",
      "Train Loss: 0.7319 Acc: 0.7267\n",
      "Val Loss: 0.8460 Acc: 0.6911\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 11/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.8083 Acc: 0.6250\n",
      "  [Batch 506/506] Train Loss: 0.6954 Acc: 0.7500\n",
      "Train Loss: 0.7172 Acc: 0.7311\n",
      "Val Loss: 0.8492 Acc: 0.6879\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 12/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6961 Acc: 0.7500\n",
      "  [Batch 506/506] Train Loss: 0.7546 Acc: 0.6875\n",
      "Train Loss: 0.6965 Acc: 0.7376\n",
      "Val Loss: 0.8585 Acc: 0.6874\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 13/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.5886 Acc: 0.7969\n",
      "  [Batch 506/506] Train Loss: 0.9294 Acc: 0.6719\n",
      "Train Loss: 0.6824 Acc: 0.7447\n",
      "Val Loss: 0.8710 Acc: 0.6891\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 14/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.7444 Acc: 0.7188\n",
      "  [Batch 506/506] Train Loss: 1.0017 Acc: 0.6250\n",
      "Train Loss: 0.6660 Acc: 0.7507\n",
      "Val Loss: 0.8713 Acc: 0.6952\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/10\n",
      "Epoch 15/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.7812 Acc: 0.6562\n",
      "  [Batch 506/506] Train Loss: 0.5547 Acc: 0.7812\n",
      "Train Loss: 0.6435 Acc: 0.7571\n",
      "Val Loss: 0.8668 Acc: 0.6955\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/10\n",
      "Epoch 16/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.8403 Acc: 0.6875\n",
      "  [Batch 506/506] Train Loss: 0.4648 Acc: 0.8125\n",
      "Train Loss: 0.6416 Acc: 0.7606\n",
      "Val Loss: 0.8718 Acc: 0.6917\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 7/10\n",
      "Epoch 17/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6081 Acc: 0.7344\n",
      "  [Batch 506/506] Train Loss: 0.6377 Acc: 0.7969\n",
      "Train Loss: 0.6394 Acc: 0.7626\n",
      "Val Loss: 0.8713 Acc: 0.6912\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 8/10\n",
      "Epoch 18/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.9700 Acc: 0.6094\n",
      "  [Batch 506/506] Train Loss: 0.5805 Acc: 0.7500\n",
      "Train Loss: 0.6372 Acc: 0.7612\n",
      "Val Loss: 0.8698 Acc: 0.6924\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 9/10\n",
      "Epoch 19/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.4791 Acc: 0.8125\n",
      "  [Batch 506/506] Train Loss: 0.6754 Acc: 0.7344\n",
      "Train Loss: 0.6304 Acc: 0.7640\n",
      "Val Loss: 0.8756 Acc: 0.6934\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 10/10\n",
      "\n",
      "Early stopping! 10 에폭 동안 성능 개선이 없었습니다.\n",
      "--------------------------------------------------\n",
      "Training complete in 10m 37s\n",
      "Saved Epoch: 9\n",
      "--------------------------------------------------\n",
      "Saved Train Loss: 0.7562\n",
      "Saved Train Acc: 0.7166\n",
      "Saved Val Loss: 0.8438\n",
      "Saved Val Acc: 0.6906\n",
      "--------------------------------------------------\n",
      "Best Train Loss: 0.6372\n",
      "Best Train Acc: 0.7626\n",
      "Best Val Loss: 0.8438\n",
      "Best Val Acc: 0.6955\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer_speedup import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_10_percent_verified_processed\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'            #철원\n",
    "    #MODEL_NAME = 'resnet50'            #철원\n",
    "    MODEL_NAME = 'shufflenet_v2'        #철원\n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   #승현님\n",
    "    #MODEL_NAME = 'squeezenet'          #승현님\n",
    "    \n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "\n",
    "    \n",
    "    # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "    #train_transform = transforms.Compose([\n",
    "    #    transforms.Resize((224, 224)),\n",
    "    #    transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "    #    transforms.RandomRotation(15),           # -15도 ~ 15도 사이로 랜덤 회전\n",
    "    #    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 밝기, 대비, 채도 조절\n",
    "    #    transforms.ToTensor(),\n",
    "    #    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    #])\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        # TrivialAugmentWide 추가, 이미지에 다양한 변형(자르기, 색상 왜곡, 회전 등)을 알아서 최적의 강도로 적용, 과적합 방지.\n",
    "        transforms.TrivialAugmentWide(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 증강이 없는 검증/테스트용 Transform 정의\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1) # 7 에폭마다 학습률을 0.1배로 감소\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS, \n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feellog-project (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
