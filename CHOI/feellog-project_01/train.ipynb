{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66595eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 17975\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'efficientnet_v2_s' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.5656 Acc: 0.5000\n",
      "  [Batch 40/280] Train Loss: 1.1736 Acc: 0.5625\n",
      "  [Batch 60/280] Train Loss: 1.1822 Acc: 0.5625\n",
      "  [Batch 80/280] Train Loss: 1.2532 Acc: 0.5312\n",
      "  [Batch 100/280] Train Loss: 1.0752 Acc: 0.5938\n",
      "  [Batch 120/280] Train Loss: 0.9997 Acc: 0.6250\n",
      "  [Batch 140/280] Train Loss: 1.1635 Acc: 0.6406\n",
      "  [Batch 160/280] Train Loss: 1.1796 Acc: 0.5938\n",
      "  [Batch 180/280] Train Loss: 1.0419 Acc: 0.6406\n",
      "  [Batch 200/280] Train Loss: 0.9427 Acc: 0.6719\n",
      "  [Batch 220/280] Train Loss: 1.2372 Acc: 0.4844\n",
      "  [Batch 240/280] Train Loss: 1.0628 Acc: 0.5000\n",
      "  [Batch 260/280] Train Loss: 0.9426 Acc: 0.7188\n",
      "  [Batch 280/280] Train Loss: 1.0034 Acc: 0.6094\n",
      "Train Loss: 1.1959 Acc: 0.5495\n",
      "Val Loss: 0.9999 Acc: 0.6262\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9999) 모델 저장.\n",
      "Epoch 2/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.0262 Acc: 0.6562\n",
      "  [Batch 40/280] Train Loss: 1.0342 Acc: 0.5938\n",
      "  [Batch 60/280] Train Loss: 0.9690 Acc: 0.6250\n",
      "  [Batch 80/280] Train Loss: 0.8998 Acc: 0.6562\n",
      "  [Batch 100/280] Train Loss: 1.0061 Acc: 0.6719\n",
      "  [Batch 120/280] Train Loss: 1.0992 Acc: 0.5938\n",
      "  [Batch 140/280] Train Loss: 1.1164 Acc: 0.6250\n",
      "  [Batch 160/280] Train Loss: 1.0507 Acc: 0.6406\n",
      "  [Batch 180/280] Train Loss: 1.0841 Acc: 0.5938\n",
      "  [Batch 200/280] Train Loss: 0.9535 Acc: 0.7188\n",
      "  [Batch 220/280] Train Loss: 1.0586 Acc: 0.5781\n",
      "  [Batch 240/280] Train Loss: 0.7949 Acc: 0.6875\n",
      "  [Batch 260/280] Train Loss: 0.8405 Acc: 0.7500\n",
      "  [Batch 280/280] Train Loss: 0.9292 Acc: 0.6094\n",
      "Train Loss: 0.9753 Acc: 0.6377\n",
      "Val Loss: 1.0191 Acc: 0.6310\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 3/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6170 Acc: 0.8125\n",
      "  [Batch 40/280] Train Loss: 1.0464 Acc: 0.6406\n",
      "  [Batch 60/280] Train Loss: 0.8005 Acc: 0.7500\n",
      "  [Batch 80/280] Train Loss: 0.9924 Acc: 0.5781\n",
      "  [Batch 100/280] Train Loss: 0.8235 Acc: 0.6719\n",
      "  [Batch 120/280] Train Loss: 0.8168 Acc: 0.7188\n",
      "  [Batch 140/280] Train Loss: 0.7750 Acc: 0.7500\n",
      "  [Batch 160/280] Train Loss: 1.0110 Acc: 0.6250\n",
      "  [Batch 180/280] Train Loss: 0.8704 Acc: 0.6406\n",
      "  [Batch 200/280] Train Loss: 0.9171 Acc: 0.6562\n",
      "  [Batch 220/280] Train Loss: 0.8258 Acc: 0.6719\n",
      "  [Batch 240/280] Train Loss: 1.1434 Acc: 0.5781\n",
      "  [Batch 260/280] Train Loss: 0.8311 Acc: 0.6875\n",
      "  [Batch 280/280] Train Loss: 0.9889 Acc: 0.6562\n",
      "Train Loss: 0.9098 Acc: 0.6636\n",
      "Val Loss: 0.9957 Acc: 0.6419\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9957) 모델 저장.\n",
      "Epoch 4/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8952 Acc: 0.6875\n",
      "  [Batch 40/280] Train Loss: 0.8554 Acc: 0.7031\n",
      "  [Batch 60/280] Train Loss: 1.0365 Acc: 0.6562\n",
      "  [Batch 80/280] Train Loss: 0.6595 Acc: 0.7500\n",
      "  [Batch 100/280] Train Loss: 1.1450 Acc: 0.5625\n",
      "  [Batch 120/280] Train Loss: 0.6809 Acc: 0.7812\n",
      "  [Batch 140/280] Train Loss: 0.8719 Acc: 0.7031\n",
      "  [Batch 160/280] Train Loss: 1.0125 Acc: 0.5469\n",
      "  [Batch 180/280] Train Loss: 0.8983 Acc: 0.7344\n",
      "  [Batch 200/280] Train Loss: 0.8164 Acc: 0.6719\n",
      "  [Batch 220/280] Train Loss: 0.6222 Acc: 0.7812\n",
      "  [Batch 240/280] Train Loss: 1.0888 Acc: 0.6406\n",
      "  [Batch 260/280] Train Loss: 0.6204 Acc: 0.7812\n",
      "  [Batch 280/280] Train Loss: 1.1239 Acc: 0.6250\n",
      "Train Loss: 0.8641 Acc: 0.6787\n",
      "Val Loss: 0.9693 Acc: 0.6623\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9693) 모델 저장.\n",
      "Epoch 5/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.9310 Acc: 0.6250\n",
      "  [Batch 40/280] Train Loss: 0.7704 Acc: 0.7344\n",
      "  [Batch 60/280] Train Loss: 0.9401 Acc: 0.6562\n",
      "  [Batch 80/280] Train Loss: 0.8513 Acc: 0.6719\n",
      "  [Batch 100/280] Train Loss: 0.7774 Acc: 0.7344\n",
      "  [Batch 120/280] Train Loss: 0.9030 Acc: 0.6094\n",
      "  [Batch 140/280] Train Loss: 0.5693 Acc: 0.7969\n",
      "  [Batch 160/280] Train Loss: 0.6880 Acc: 0.7812\n",
      "  [Batch 180/280] Train Loss: 0.8306 Acc: 0.6875\n",
      "  [Batch 200/280] Train Loss: 0.8084 Acc: 0.7656\n",
      "  [Batch 220/280] Train Loss: 0.8828 Acc: 0.7344\n",
      "  [Batch 240/280] Train Loss: 0.8758 Acc: 0.6250\n",
      "  [Batch 260/280] Train Loss: 0.8611 Acc: 0.7031\n",
      "  [Batch 280/280] Train Loss: 0.8818 Acc: 0.7188\n",
      "Train Loss: 0.8241 Acc: 0.6980\n",
      "Val Loss: 0.9355 Acc: 0.6596\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9355) 모델 저장.\n",
      "Epoch 6/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.9087 Acc: 0.7188\n",
      "  [Batch 40/280] Train Loss: 0.7619 Acc: 0.7188\n",
      "  [Batch 60/280] Train Loss: 0.7990 Acc: 0.7344\n",
      "  [Batch 80/280] Train Loss: 0.8902 Acc: 0.6250\n",
      "  [Batch 100/280] Train Loss: 0.7467 Acc: 0.6719\n",
      "  [Batch 120/280] Train Loss: 0.9204 Acc: 0.6719\n",
      "  [Batch 140/280] Train Loss: 0.8353 Acc: 0.7031\n",
      "  [Batch 160/280] Train Loss: 0.6140 Acc: 0.7188\n",
      "  [Batch 180/280] Train Loss: 0.8231 Acc: 0.6562\n",
      "  [Batch 200/280] Train Loss: 0.7761 Acc: 0.6406\n",
      "  [Batch 220/280] Train Loss: 0.9336 Acc: 0.7031\n",
      "  [Batch 240/280] Train Loss: 0.7356 Acc: 0.7656\n",
      "  [Batch 260/280] Train Loss: 0.8288 Acc: 0.6406\n",
      "  [Batch 280/280] Train Loss: 0.8980 Acc: 0.6875\n",
      "Train Loss: 0.7933 Acc: 0.7035\n",
      "Val Loss: 0.8734 Acc: 0.6913\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8734) 모델 저장.\n",
      "Epoch 7/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5485 Acc: 0.8438\n",
      "  [Batch 40/280] Train Loss: 0.9785 Acc: 0.6406\n",
      "  [Batch 60/280] Train Loss: 0.7995 Acc: 0.6719\n",
      "  [Batch 80/280] Train Loss: 0.5471 Acc: 0.7344\n",
      "  [Batch 100/280] Train Loss: 0.6892 Acc: 0.7656\n",
      "  [Batch 120/280] Train Loss: 0.7358 Acc: 0.7031\n",
      "  [Batch 140/280] Train Loss: 0.7676 Acc: 0.7344\n",
      "  [Batch 160/280] Train Loss: 0.8202 Acc: 0.7188\n",
      "  [Batch 180/280] Train Loss: 0.7172 Acc: 0.7031\n",
      "  [Batch 200/280] Train Loss: 0.7902 Acc: 0.7188\n",
      "  [Batch 220/280] Train Loss: 0.9152 Acc: 0.6562\n",
      "  [Batch 240/280] Train Loss: 0.7706 Acc: 0.7031\n",
      "  [Batch 260/280] Train Loss: 0.7032 Acc: 0.7500\n",
      "  [Batch 280/280] Train Loss: 0.6900 Acc: 0.7500\n",
      "Train Loss: 0.7672 Acc: 0.7148\n",
      "Val Loss: 0.8782 Acc: 0.6836\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 8/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5390 Acc: 0.7812\n",
      "  [Batch 40/280] Train Loss: 0.4954 Acc: 0.7969\n",
      "  [Batch 60/280] Train Loss: 0.6505 Acc: 0.8125\n",
      "  [Batch 80/280] Train Loss: 0.6486 Acc: 0.7344\n",
      "  [Batch 100/280] Train Loss: 0.7234 Acc: 0.7969\n",
      "  [Batch 120/280] Train Loss: 0.9389 Acc: 0.6875\n",
      "  [Batch 140/280] Train Loss: 0.6695 Acc: 0.6719\n",
      "  [Batch 160/280] Train Loss: 0.6688 Acc: 0.8125\n",
      "  [Batch 180/280] Train Loss: 0.4842 Acc: 0.8281\n",
      "  [Batch 200/280] Train Loss: 0.5000 Acc: 0.7500\n",
      "  [Batch 220/280] Train Loss: 0.4373 Acc: 0.8750\n",
      "  [Batch 240/280] Train Loss: 0.5032 Acc: 0.7969\n",
      "  [Batch 260/280] Train Loss: 0.6211 Acc: 0.7656\n",
      "  [Batch 280/280] Train Loss: 0.6348 Acc: 0.7812\n",
      "Train Loss: 0.6221 Acc: 0.7677\n",
      "Val Loss: 0.7959 Acc: 0.7154\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7959) 모델 저장.\n",
      "Epoch 9/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5924 Acc: 0.7812\n",
      "  [Batch 40/280] Train Loss: 0.4287 Acc: 0.8594\n",
      "  [Batch 60/280] Train Loss: 0.6571 Acc: 0.7812\n",
      "  [Batch 80/280] Train Loss: 0.6065 Acc: 0.8125\n",
      "  [Batch 100/280] Train Loss: 0.8892 Acc: 0.6562\n",
      "  [Batch 120/280] Train Loss: 0.5629 Acc: 0.8438\n",
      "  [Batch 140/280] Train Loss: 0.5693 Acc: 0.8281\n",
      "  [Batch 160/280] Train Loss: 0.5351 Acc: 0.7812\n",
      "  [Batch 180/280] Train Loss: 0.6986 Acc: 0.7969\n",
      "  [Batch 200/280] Train Loss: 0.5785 Acc: 0.8438\n",
      "  [Batch 220/280] Train Loss: 0.4646 Acc: 0.8438\n",
      "  [Batch 240/280] Train Loss: 0.4682 Acc: 0.8594\n",
      "  [Batch 260/280] Train Loss: 0.6057 Acc: 0.7656\n",
      "  [Batch 280/280] Train Loss: 0.6102 Acc: 0.7656\n",
      "Train Loss: 0.5637 Acc: 0.7886\n",
      "Val Loss: 0.8086 Acc: 0.7167\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 10/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5968 Acc: 0.7656\n",
      "  [Batch 40/280] Train Loss: 0.4889 Acc: 0.8438\n",
      "  [Batch 60/280] Train Loss: 0.5391 Acc: 0.7969\n",
      "  [Batch 80/280] Train Loss: 0.4679 Acc: 0.8438\n",
      "  [Batch 100/280] Train Loss: 0.4917 Acc: 0.8125\n",
      "  [Batch 120/280] Train Loss: 0.6618 Acc: 0.7188\n",
      "  [Batch 140/280] Train Loss: 0.4522 Acc: 0.8750\n",
      "  [Batch 160/280] Train Loss: 0.4869 Acc: 0.8125\n",
      "  [Batch 180/280] Train Loss: 0.5444 Acc: 0.7969\n",
      "  [Batch 200/280] Train Loss: 0.4584 Acc: 0.8281\n",
      "  [Batch 220/280] Train Loss: 0.3782 Acc: 0.8906\n",
      "  [Batch 240/280] Train Loss: 0.4134 Acc: 0.8750\n",
      "  [Batch 260/280] Train Loss: 0.6729 Acc: 0.7344\n",
      "  [Batch 280/280] Train Loss: 0.4542 Acc: 0.8281\n",
      "Train Loss: 0.5271 Acc: 0.8021\n",
      "Val Loss: 0.8043 Acc: 0.7178\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 11/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6950 Acc: 0.7969\n",
      "  [Batch 40/280] Train Loss: 0.4086 Acc: 0.8594\n",
      "  [Batch 60/280] Train Loss: 0.6713 Acc: 0.7344\n",
      "  [Batch 80/280] Train Loss: 0.4887 Acc: 0.8594\n",
      "  [Batch 100/280] Train Loss: 0.5720 Acc: 0.7812\n",
      "  [Batch 120/280] Train Loss: 0.5083 Acc: 0.7969\n",
      "  [Batch 140/280] Train Loss: 0.4821 Acc: 0.7969\n",
      "  [Batch 160/280] Train Loss: 0.5127 Acc: 0.8281\n",
      "  [Batch 180/280] Train Loss: 0.3580 Acc: 0.8750\n",
      "  [Batch 200/280] Train Loss: 0.4810 Acc: 0.8125\n",
      "  [Batch 220/280] Train Loss: 0.4284 Acc: 0.8281\n",
      "  [Batch 240/280] Train Loss: 0.6631 Acc: 0.8281\n",
      "  [Batch 260/280] Train Loss: 0.2598 Acc: 0.9062\n",
      "  [Batch 280/280] Train Loss: 0.5263 Acc: 0.8438\n",
      "Train Loss: 0.4984 Acc: 0.8149\n",
      "Val Loss: 0.8304 Acc: 0.7169\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 12/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.3758 Acc: 0.8750\n",
      "  [Batch 40/280] Train Loss: 0.5267 Acc: 0.7500\n",
      "  [Batch 60/280] Train Loss: 0.4405 Acc: 0.8281\n",
      "  [Batch 80/280] Train Loss: 0.6575 Acc: 0.7656\n",
      "  [Batch 100/280] Train Loss: 0.4646 Acc: 0.8594\n",
      "  [Batch 120/280] Train Loss: 0.7720 Acc: 0.7969\n",
      "  [Batch 140/280] Train Loss: 0.5818 Acc: 0.7344\n",
      "  [Batch 160/280] Train Loss: 0.3926 Acc: 0.8438\n",
      "  [Batch 180/280] Train Loss: 0.5446 Acc: 0.7812\n",
      "  [Batch 200/280] Train Loss: 0.3241 Acc: 0.8594\n",
      "  [Batch 220/280] Train Loss: 0.6199 Acc: 0.7500\n",
      "  [Batch 240/280] Train Loss: 0.3307 Acc: 0.8906\n",
      "  [Batch 260/280] Train Loss: 0.4547 Acc: 0.8125\n",
      "  [Batch 280/280] Train Loss: 0.3747 Acc: 0.8594\n",
      "Train Loss: 0.4614 Acc: 0.8270\n",
      "Val Loss: 0.8441 Acc: 0.7181\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 13/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4502 Acc: 0.7969\n",
      "  [Batch 40/280] Train Loss: 0.3236 Acc: 0.9062\n",
      "  [Batch 60/280] Train Loss: 0.4725 Acc: 0.8281\n",
      "  [Batch 80/280] Train Loss: 0.5594 Acc: 0.7812\n",
      "  [Batch 100/280] Train Loss: 0.5039 Acc: 0.8125\n",
      "  [Batch 120/280] Train Loss: 0.4784 Acc: 0.8281\n",
      "  [Batch 140/280] Train Loss: 0.5245 Acc: 0.7656\n",
      "  [Batch 160/280] Train Loss: 0.5177 Acc: 0.8281\n",
      "  [Batch 180/280] Train Loss: 0.4216 Acc: 0.8750\n",
      "  [Batch 200/280] Train Loss: 0.3992 Acc: 0.8750\n",
      "  [Batch 220/280] Train Loss: 0.5195 Acc: 0.8125\n",
      "  [Batch 240/280] Train Loss: 0.4873 Acc: 0.7969\n",
      "  [Batch 260/280] Train Loss: 0.4183 Acc: 0.8125\n",
      "  [Batch 280/280] Train Loss: 0.3570 Acc: 0.8594\n",
      "Train Loss: 0.4236 Acc: 0.8428\n",
      "Val Loss: 0.8815 Acc: 0.7190\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/10\n",
      "Epoch 14/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4079 Acc: 0.8594\n",
      "  [Batch 40/280] Train Loss: 0.4451 Acc: 0.8281\n",
      "  [Batch 60/280] Train Loss: 0.3729 Acc: 0.8750\n",
      "  [Batch 80/280] Train Loss: 0.4248 Acc: 0.8594\n",
      "  [Batch 100/280] Train Loss: 0.5854 Acc: 0.7969\n",
      "  [Batch 120/280] Train Loss: 0.3916 Acc: 0.8281\n",
      "  [Batch 140/280] Train Loss: 0.5447 Acc: 0.8125\n",
      "  [Batch 160/280] Train Loss: 0.5156 Acc: 0.8125\n",
      "  [Batch 180/280] Train Loss: 0.3476 Acc: 0.8594\n",
      "  [Batch 200/280] Train Loss: 0.3147 Acc: 0.9219\n",
      "  [Batch 220/280] Train Loss: 0.3513 Acc: 0.8594\n",
      "  [Batch 240/280] Train Loss: 0.3356 Acc: 0.8906\n",
      "  [Batch 260/280] Train Loss: 0.4822 Acc: 0.8438\n",
      "  [Batch 280/280] Train Loss: 0.3264 Acc: 0.8906\n",
      "Train Loss: 0.3984 Acc: 0.8506\n",
      "Val Loss: 0.9054 Acc: 0.7167\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/10\n",
      "Epoch 15/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.2431 Acc: 0.9219\n",
      "  [Batch 40/280] Train Loss: 0.4301 Acc: 0.8594\n",
      "  [Batch 60/280] Train Loss: 0.2468 Acc: 0.9219\n",
      "  [Batch 80/280] Train Loss: 0.3205 Acc: 0.8438\n",
      "  [Batch 100/280] Train Loss: 0.4778 Acc: 0.8594\n",
      "  [Batch 120/280] Train Loss: 0.4285 Acc: 0.8281\n",
      "  [Batch 140/280] Train Loss: 0.3256 Acc: 0.8594\n",
      "  [Batch 160/280] Train Loss: 0.3308 Acc: 0.8906\n",
      "  [Batch 180/280] Train Loss: 0.2870 Acc: 0.8906\n",
      "  [Batch 200/280] Train Loss: 0.3387 Acc: 0.8906\n",
      "  [Batch 220/280] Train Loss: 0.2851 Acc: 0.8750\n",
      "  [Batch 240/280] Train Loss: 0.3578 Acc: 0.8750\n",
      "  [Batch 260/280] Train Loss: 0.4412 Acc: 0.8438\n",
      "  [Batch 280/280] Train Loss: 0.3929 Acc: 0.8281\n",
      "Train Loss: 0.3436 Acc: 0.8724\n",
      "Val Loss: 0.9135 Acc: 0.7208\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 7/10\n",
      "Epoch 16/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.3010 Acc: 0.8906\n",
      "  [Batch 40/280] Train Loss: 0.4404 Acc: 0.8281\n",
      "  [Batch 60/280] Train Loss: 0.3082 Acc: 0.8906\n",
      "  [Batch 80/280] Train Loss: 0.3477 Acc: 0.8594\n",
      "  [Batch 100/280] Train Loss: 0.2770 Acc: 0.8750\n",
      "  [Batch 120/280] Train Loss: 0.2757 Acc: 0.9062\n",
      "  [Batch 140/280] Train Loss: 0.3208 Acc: 0.9062\n",
      "  [Batch 160/280] Train Loss: 0.3911 Acc: 0.8438\n",
      "  [Batch 180/280] Train Loss: 0.3494 Acc: 0.8594\n",
      "  [Batch 200/280] Train Loss: 0.2692 Acc: 0.9062\n",
      "  [Batch 220/280] Train Loss: 0.3662 Acc: 0.8594\n",
      "  [Batch 240/280] Train Loss: 0.2394 Acc: 0.9062\n",
      "  [Batch 260/280] Train Loss: 0.2536 Acc: 0.9375\n",
      "  [Batch 280/280] Train Loss: 0.3394 Acc: 0.8906\n",
      "Train Loss: 0.3322 Acc: 0.8748\n",
      "Val Loss: 0.9307 Acc: 0.7194\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 8/10\n",
      "Epoch 17/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.3738 Acc: 0.8750\n",
      "  [Batch 40/280] Train Loss: 0.2680 Acc: 0.8906\n",
      "  [Batch 60/280] Train Loss: 0.2445 Acc: 0.9062\n",
      "  [Batch 80/280] Train Loss: 0.2727 Acc: 0.8438\n",
      "  [Batch 100/280] Train Loss: 0.1979 Acc: 0.9375\n",
      "  [Batch 120/280] Train Loss: 0.3526 Acc: 0.8594\n",
      "  [Batch 140/280] Train Loss: 0.3347 Acc: 0.8594\n",
      "  [Batch 160/280] Train Loss: 0.4324 Acc: 0.8281\n",
      "  [Batch 180/280] Train Loss: 0.6199 Acc: 0.7969\n",
      "  [Batch 200/280] Train Loss: 0.3229 Acc: 0.9375\n",
      "  [Batch 220/280] Train Loss: 0.2999 Acc: 0.8750\n",
      "  [Batch 240/280] Train Loss: 0.4332 Acc: 0.8281\n",
      "  [Batch 260/280] Train Loss: 0.2838 Acc: 0.9375\n",
      "  [Batch 280/280] Train Loss: 0.3020 Acc: 0.8750\n",
      "Train Loss: 0.3214 Acc: 0.8814\n",
      "Val Loss: 0.9368 Acc: 0.7197\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 9/10\n",
      "Epoch 18/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.3841 Acc: 0.8750\n",
      "  [Batch 40/280] Train Loss: 0.2978 Acc: 0.8438\n",
      "  [Batch 60/280] Train Loss: 0.1626 Acc: 0.9531\n",
      "  [Batch 80/280] Train Loss: 0.2942 Acc: 0.8594\n",
      "  [Batch 100/280] Train Loss: 0.2816 Acc: 0.9531\n",
      "  [Batch 120/280] Train Loss: 0.2565 Acc: 0.9219\n",
      "  [Batch 140/280] Train Loss: 0.2798 Acc: 0.8906\n",
      "  [Batch 160/280] Train Loss: 0.2515 Acc: 0.8906\n",
      "  [Batch 180/280] Train Loss: 0.3039 Acc: 0.9062\n",
      "  [Batch 200/280] Train Loss: 0.4224 Acc: 0.8750\n",
      "  [Batch 220/280] Train Loss: 0.5276 Acc: 0.8125\n",
      "  [Batch 240/280] Train Loss: 0.3249 Acc: 0.8750\n",
      "  [Batch 260/280] Train Loss: 0.2789 Acc: 0.9062\n",
      "  [Batch 280/280] Train Loss: 0.3697 Acc: 0.8750\n",
      "Train Loss: 0.3144 Acc: 0.8812\n",
      "Val Loss: 0.9501 Acc: 0.7197\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 10/10\n",
      "\n",
      "Early stopping! 10 에폭 동안 성능 개선이 없었습니다.\n",
      "Training complete in 20m 8s\n",
      "Best Val Loss: 0.7959\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer_speedup import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_5_percent_verified_processed\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'             #철원\n",
    "    #MODEL_NAME = 'resnet50' \n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'shufflenet_v2'       #철원\n",
    "    MODEL_NAME = 'efficientnet_v2_s'   #규진님\n",
    "    #MODEL_NAME = 'squeezenet'          #승희님\n",
    "    \n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "\n",
    "    \n",
    "    # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "        transforms.RandomRotation(15),           # -15도 ~ 15도 사이로 랜덤 회전\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 밝기, 대비, 채도 조절\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 증강이 없는 검증/테스트용 Transform 정의\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        #weight_decay=1e-4, #과적합 방지를 위한 가중치 감쇠를 넣었으나 오히려 학습에 방해가 되고 있음.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS, \n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 17975\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'efficientnet_v2_s' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.4657 Acc: 0.4375\n",
      "  [Batch 40/280] Train Loss: 1.5536 Acc: 0.5000\n",
      "  [Batch 60/280] Train Loss: 1.2598 Acc: 0.6094\n",
      "  [Batch 80/280] Train Loss: 1.1383 Acc: 0.6250\n",
      "  [Batch 100/280] Train Loss: 1.0596 Acc: 0.6250\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.4803 Acc: 0.1756\n",
      "Val Loss: 1.1897 Acc: 0.5725\n",
      "\n",
      "  -> Val Loss 개선됨! (1.1897) 모델 저장.\n",
      "Epoch 2/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.1181 Acc: 0.5469\n",
      "  [Batch 40/280] Train Loss: 1.2545 Acc: 0.5625\n",
      "  [Batch 60/280] Train Loss: 1.0339 Acc: 0.6094\n",
      "  [Batch 80/280] Train Loss: 1.1953 Acc: 0.5781\n",
      "  [Batch 100/280] Train Loss: 1.0215 Acc: 0.6094\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.3962 Acc: 0.2097\n",
      "Val Loss: 1.1546 Acc: 0.5729\n",
      "\n",
      "  -> Val Loss 개선됨! (1.1546) 모델 저장.\n",
      "Epoch 3/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.0715 Acc: 0.6406\n",
      "  [Batch 40/280] Train Loss: 0.9883 Acc: 0.5625\n",
      "  [Batch 60/280] Train Loss: 0.9275 Acc: 0.6406\n",
      "  [Batch 80/280] Train Loss: 1.0038 Acc: 0.6094\n",
      "  [Batch 100/280] Train Loss: 1.0339 Acc: 0.6562\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.3735 Acc: 0.2181\n",
      "Val Loss: 1.0506 Acc: 0.6081\n",
      "\n",
      "  -> Val Loss 개선됨! (1.0506) 모델 저장.\n",
      "Epoch 4/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8135 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.9559 Acc: 0.6250\n",
      "  [Batch 60/280] Train Loss: 1.1905 Acc: 0.5312\n",
      "  [Batch 80/280] Train Loss: 1.0462 Acc: 0.6562\n",
      "  [Batch 100/280] Train Loss: 1.0124 Acc: 0.6094\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.3463 Acc: 0.2266\n",
      "Val Loss: 1.0163 Acc: 0.6144\n",
      "\n",
      "  -> Val Loss 개선됨! (1.0163) 모델 저장.\n",
      "Epoch 5/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.2104 Acc: 0.6406\n",
      "  [Batch 40/280] Train Loss: 0.8397 Acc: 0.6875\n",
      "  [Batch 60/280] Train Loss: 1.0054 Acc: 0.5938\n",
      "  [Batch 80/280] Train Loss: 1.1161 Acc: 0.5625\n",
      "  [Batch 100/280] Train Loss: 0.9678 Acc: 0.6562\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.3405 Acc: 0.2294\n",
      "Val Loss: 0.9883 Acc: 0.6423\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9883) 모델 저장.\n",
      "Epoch 6/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.0903 Acc: 0.5469\n",
      "  [Batch 40/280] Train Loss: 0.8784 Acc: 0.6875\n",
      "  [Batch 60/280] Train Loss: 0.9350 Acc: 0.6250\n",
      "  [Batch 80/280] Train Loss: 1.0969 Acc: 0.6094\n",
      "  [Batch 100/280] Train Loss: 0.7596 Acc: 0.7031\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.3358 Acc: 0.2324\n",
      "Val Loss: 0.9243 Acc: 0.6559\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9243) 모델 저장.\n",
      "Epoch 7/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.9671 Acc: 0.6094\n",
      "  [Batch 40/280] Train Loss: 0.8927 Acc: 0.6562\n",
      "  [Batch 60/280] Train Loss: 0.7704 Acc: 0.6719\n",
      "  [Batch 80/280] Train Loss: 1.0685 Acc: 0.6250\n",
      "  [Batch 100/280] Train Loss: 0.8467 Acc: 0.6719\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.3167 Acc: 0.2403\n",
      "Val Loss: 1.0537 Acc: 0.6017\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 8/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6262 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.7571 Acc: 0.7656\n",
      "  [Batch 60/280] Train Loss: 0.8579 Acc: 0.5938\n",
      "  [Batch 80/280] Train Loss: 0.7450 Acc: 0.7656\n",
      "  [Batch 100/280] Train Loss: 0.7883 Acc: 0.6875\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2942 Acc: 0.2475\n",
      "Val Loss: 0.8516 Acc: 0.6845\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8516) 모델 저장.\n",
      "Epoch 9/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.9085 Acc: 0.6719\n",
      "  [Batch 40/280] Train Loss: 0.9375 Acc: 0.7500\n",
      "  [Batch 60/280] Train Loss: 0.7770 Acc: 0.6875\n",
      "  [Batch 80/280] Train Loss: 0.8429 Acc: 0.6719\n",
      "  [Batch 100/280] Train Loss: 0.7627 Acc: 0.7656\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2727 Acc: 0.2548\n",
      "Val Loss: 0.8368 Acc: 0.6947\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8368) 모델 저장.\n",
      "Epoch 10/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7927 Acc: 0.7188\n",
      "  [Batch 40/280] Train Loss: 0.6188 Acc: 0.7656\n",
      "  [Batch 60/280] Train Loss: 0.6321 Acc: 0.7656\n",
      "  [Batch 80/280] Train Loss: 0.7300 Acc: 0.6719\n",
      "  [Batch 100/280] Train Loss: 0.7898 Acc: 0.7344\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2602 Acc: 0.2595\n",
      "Val Loss: 0.8332 Acc: 0.6995\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8332) 모델 저장.\n",
      "Epoch 11/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5426 Acc: 0.8594\n",
      "  [Batch 40/280] Train Loss: 0.7244 Acc: 0.7500\n",
      "  [Batch 60/280] Train Loss: 0.8455 Acc: 0.7031\n",
      "  [Batch 80/280] Train Loss: 0.6989 Acc: 0.7500\n",
      "  [Batch 100/280] Train Loss: 0.6811 Acc: 0.8125\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2602 Acc: 0.2623\n",
      "Val Loss: 0.8240 Acc: 0.7008\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8240) 모델 저장.\n",
      "Epoch 12/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6728 Acc: 0.7188\n",
      "  [Batch 40/280] Train Loss: 0.6425 Acc: 0.7500\n",
      "  [Batch 60/280] Train Loss: 0.6556 Acc: 0.7344\n",
      "  [Batch 80/280] Train Loss: 0.9428 Acc: 0.6094\n",
      "  [Batch 100/280] Train Loss: 0.6949 Acc: 0.7188\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2519 Acc: 0.2625\n",
      "Val Loss: 0.8150 Acc: 0.7072\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8150) 모델 저장.\n",
      "Epoch 13/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8171 Acc: 0.7031\n",
      "  [Batch 40/280] Train Loss: 0.5411 Acc: 0.8125\n",
      "  [Batch 60/280] Train Loss: 0.8276 Acc: 0.6562\n",
      "  [Batch 80/280] Train Loss: 0.7589 Acc: 0.7031\n",
      "  [Batch 100/280] Train Loss: 0.6236 Acc: 0.7969\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2502 Acc: 0.2635\n",
      "Val Loss: 0.8051 Acc: 0.7081\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8051) 모델 저장.\n",
      "Epoch 14/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5009 Acc: 0.8125\n",
      "  [Batch 40/280] Train Loss: 0.7618 Acc: 0.7812\n",
      "  [Batch 60/280] Train Loss: 0.8060 Acc: 0.7188\n",
      "  [Batch 80/280] Train Loss: 0.6066 Acc: 0.7812\n",
      "  [Batch 100/280] Train Loss: 0.5673 Acc: 0.8281\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2367 Acc: 0.2703\n",
      "Val Loss: 0.8128 Acc: 0.7110\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 15/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6307 Acc: 0.7344\n",
      "  [Batch 40/280] Train Loss: 0.7582 Acc: 0.7188\n",
      "  [Batch 60/280] Train Loss: 0.7473 Acc: 0.7344\n",
      "  [Batch 80/280] Train Loss: 0.6842 Acc: 0.7344\n",
      "  [Batch 100/280] Train Loss: 0.6844 Acc: 0.7188\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2390 Acc: 0.2671\n",
      "Val Loss: 0.8074 Acc: 0.7131\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 16/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8590 Acc: 0.6875\n",
      "  [Batch 40/280] Train Loss: 0.5374 Acc: 0.8281\n",
      "  [Batch 60/280] Train Loss: 0.6133 Acc: 0.7656\n",
      "  [Batch 80/280] Train Loss: 0.7832 Acc: 0.6406\n",
      "  [Batch 100/280] Train Loss: 0.5152 Acc: 0.8125\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2306 Acc: 0.2700\n",
      "Val Loss: 0.8075 Acc: 0.7131\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 17/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6987 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.6028 Acc: 0.7656\n",
      "  [Batch 60/280] Train Loss: 0.7353 Acc: 0.7656\n",
      "  [Batch 80/280] Train Loss: 0.6134 Acc: 0.7500\n",
      "  [Batch 100/280] Train Loss: 0.6156 Acc: 0.7188\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2251 Acc: 0.2740\n",
      "Val Loss: 0.8042 Acc: 0.7124\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8042) 모델 저장.\n",
      "Epoch 18/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5876 Acc: 0.8281\n",
      "  [Batch 40/280] Train Loss: 0.6784 Acc: 0.7812\n",
      "  [Batch 60/280] Train Loss: 0.5955 Acc: 0.7500\n",
      "  [Batch 80/280] Train Loss: 0.8092 Acc: 0.6875\n",
      "  [Batch 100/280] Train Loss: 0.4553 Acc: 0.8594\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2387 Acc: 0.2676\n",
      "Val Loss: 0.8041 Acc: 0.7122\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8041) 모델 저장.\n",
      "Epoch 19/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7274 Acc: 0.7344\n",
      "  [Batch 40/280] Train Loss: 0.6572 Acc: 0.7031\n",
      "  [Batch 60/280] Train Loss: 0.6982 Acc: 0.7656\n",
      "  [Batch 80/280] Train Loss: 0.8582 Acc: 0.6719\n",
      "  [Batch 100/280] Train Loss: 0.6579 Acc: 0.7500\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2276 Acc: 0.2734\n",
      "Val Loss: 0.8017 Acc: 0.7129\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8017) 모델 저장.\n",
      "Epoch 20/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6605 Acc: 0.6719\n",
      "  [Batch 40/280] Train Loss: 0.7390 Acc: 0.6406\n",
      "  [Batch 60/280] Train Loss: 0.5524 Acc: 0.8125\n",
      "  [Batch 80/280] Train Loss: 0.6513 Acc: 0.6719\n",
      "  [Batch 100/280] Train Loss: 0.5297 Acc: 0.7969\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2326 Acc: 0.2701\n",
      "Val Loss: 0.8035 Acc: 0.7149\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 21/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4857 Acc: 0.8438\n",
      "  [Batch 40/280] Train Loss: 0.7539 Acc: 0.6875\n",
      "  [Batch 60/280] Train Loss: 0.5251 Acc: 0.8281\n",
      "  [Batch 80/280] Train Loss: 0.4267 Acc: 0.8281\n",
      "  [Batch 100/280] Train Loss: 0.6103 Acc: 0.7188\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2260 Acc: 0.2743\n",
      "Val Loss: 0.8041 Acc: 0.7154\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 22/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4904 Acc: 0.8281\n",
      "  [Batch 40/280] Train Loss: 0.8756 Acc: 0.7656\n",
      "  [Batch 60/280] Train Loss: 0.7702 Acc: 0.7031\n",
      "  [Batch 80/280] Train Loss: 0.7137 Acc: 0.7656\n",
      "  [Batch 100/280] Train Loss: 0.5560 Acc: 0.7500\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2288 Acc: 0.2709\n",
      "Val Loss: 0.8052 Acc: 0.7167\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 23/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.9256 Acc: 0.6406\n",
      "  [Batch 40/280] Train Loss: 0.6654 Acc: 0.7188\n",
      "  [Batch 60/280] Train Loss: 0.5625 Acc: 0.8281\n",
      "  [Batch 80/280] Train Loss: 0.6269 Acc: 0.8750\n",
      "  [Batch 100/280] Train Loss: 0.3694 Acc: 0.8906\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2239 Acc: 0.2714\n",
      "Val Loss: 0.8026 Acc: 0.7169\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 24/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6317 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.8443 Acc: 0.6875\n",
      "  [Batch 60/280] Train Loss: 0.7857 Acc: 0.6562\n",
      "  [Batch 80/280] Train Loss: 0.4614 Acc: 0.7969\n",
      "  [Batch 100/280] Train Loss: 0.7866 Acc: 0.7188\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2253 Acc: 0.2719\n",
      "Val Loss: 0.8036 Acc: 0.7165\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/10\n",
      "Epoch 25/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7470 Acc: 0.7344\n",
      "  [Batch 40/280] Train Loss: 0.5321 Acc: 0.7812\n",
      "  [Batch 60/280] Train Loss: 0.5971 Acc: 0.7500\n",
      "  [Batch 80/280] Train Loss: 0.6144 Acc: 0.7969\n",
      "  [Batch 100/280] Train Loss: 1.0287 Acc: 0.6250\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2315 Acc: 0.2712\n",
      "Val Loss: 0.8050 Acc: 0.7156\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/10\n",
      "Epoch 26/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5246 Acc: 0.8438\n",
      "  [Batch 40/280] Train Loss: 0.6595 Acc: 0.7812\n",
      "  [Batch 60/280] Train Loss: 0.5969 Acc: 0.8125\n",
      "  [Batch 80/280] Train Loss: 0.6417 Acc: 0.7812\n",
      "  [Batch 100/280] Train Loss: 0.6440 Acc: 0.8125\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2256 Acc: 0.2748\n",
      "Val Loss: 0.8034 Acc: 0.7156\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 7/10\n",
      "Epoch 27/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7732 Acc: 0.7344\n",
      "  [Batch 40/280] Train Loss: 0.7580 Acc: 0.7656\n",
      "  [Batch 60/280] Train Loss: 0.5494 Acc: 0.7656\n",
      "  [Batch 80/280] Train Loss: 0.5648 Acc: 0.7344\n",
      "  [Batch 100/280] Train Loss: 0.6630 Acc: 0.7656\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2225 Acc: 0.2744\n",
      "Val Loss: 0.8022 Acc: 0.7154\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 8/10\n",
      "Epoch 28/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7840 Acc: 0.7188\n",
      "  [Batch 40/280] Train Loss: 0.7186 Acc: 0.7188\n",
      "  [Batch 60/280] Train Loss: 0.7597 Acc: 0.7188\n",
      "  [Batch 80/280] Train Loss: 0.4796 Acc: 0.8750\n",
      "  [Batch 100/280] Train Loss: 0.4671 Acc: 0.8438\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2223 Acc: 0.2728\n",
      "Val Loss: 0.8036 Acc: 0.7142\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 9/10\n",
      "Epoch 29/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6369 Acc: 0.7812\n",
      "  [Batch 40/280] Train Loss: 0.4684 Acc: 0.8438\n",
      "  [Batch 60/280] Train Loss: 0.3798 Acc: 0.8750\n",
      "  [Batch 80/280] Train Loss: 0.6570 Acc: 0.7656\n",
      "  [Batch 100/280] Train Loss: 0.8664 Acc: 0.7344\n",
      "  -> Reached steps_per_epoch (100), moving to next epoch.\n",
      "Train Loss: 0.2297 Acc: 0.2729\n",
      "Val Loss: 0.8030 Acc: 0.7167\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 10/10\n",
      "\n",
      "Early stopping! 10 에폭 동안 성능 개선이 없었습니다.\n",
      "Training complete in 13m 17s\n",
      "Best Val Loss: 0.8017\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer_speedup import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_5_percent_verified_processed\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'             #철원\n",
    "    #MODEL_NAME = 'resnet50' \n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'shufflenet_v2'       #철원\n",
    "    MODEL_NAME = 'efficientnet_v2_s'   #규진님\n",
    "    #MODEL_NAME = 'squeezenet'          #승희님\n",
    "    \n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    #STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    \n",
    "    # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "        transforms.RandomRotation(15),           # -15도 ~ 15도 사이로 랜덤 회전\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 밝기, 대비, 채도 조절\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 증강이 없는 검증/테스트용 Transform 정의\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        #weight_decay=1e-4, #과적합 방지를 위한 가중치 감쇠를 넣었으나 오히려 학습에 방해가 되고 있음.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS, \n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 17975\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'shufflenet_v2' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.8093 Acc: 0.3438\n",
      "  [Batch 40/280] Train Loss: 1.7537 Acc: 0.2969\n",
      "  [Batch 60/280] Train Loss: 1.5658 Acc: 0.3906\n",
      "  [Batch 80/280] Train Loss: 1.2417 Acc: 0.5625\n",
      "  [Batch 100/280] Train Loss: 1.3650 Acc: 0.5469\n",
      "  [Batch 120/280] Train Loss: 1.5490 Acc: 0.4219\n",
      "  [Batch 140/280] Train Loss: 1.3711 Acc: 0.5000\n",
      "  [Batch 160/280] Train Loss: 1.2190 Acc: 0.5156\n",
      "  [Batch 180/280] Train Loss: 1.2276 Acc: 0.4844\n",
      "  [Batch 200/280] Train Loss: 1.2386 Acc: 0.5781\n",
      "  [Batch 220/280] Train Loss: 1.3378 Acc: 0.5469\n",
      "  [Batch 240/280] Train Loss: 1.2648 Acc: 0.5938\n",
      "  [Batch 260/280] Train Loss: 1.1463 Acc: 0.5938\n",
      "  [Batch 280/280] Train Loss: 1.1005 Acc: 0.6250\n",
      "Train Loss: 1.3686 Acc: 0.4821\n",
      "Val Loss: 1.1210 Acc: 0.5756\n",
      "\n",
      "  -> Val Loss 개선됨! (1.1210) 모델 저장.\n",
      "Epoch 2/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.0732 Acc: 0.5938\n",
      "  [Batch 40/280] Train Loss: 1.0991 Acc: 0.6250\n",
      "  [Batch 60/280] Train Loss: 1.0986 Acc: 0.5625\n",
      "  [Batch 80/280] Train Loss: 1.3518 Acc: 0.5000\n",
      "  [Batch 100/280] Train Loss: 1.1750 Acc: 0.6094\n",
      "  [Batch 120/280] Train Loss: 1.0607 Acc: 0.6406\n",
      "  [Batch 140/280] Train Loss: 1.2714 Acc: 0.5156\n",
      "  [Batch 160/280] Train Loss: 1.0259 Acc: 0.5781\n",
      "  [Batch 180/280] Train Loss: 0.9702 Acc: 0.6250\n",
      "  [Batch 200/280] Train Loss: 1.1924 Acc: 0.6094\n",
      "  [Batch 220/280] Train Loss: 0.8924 Acc: 0.7031\n",
      "  [Batch 240/280] Train Loss: 1.3121 Acc: 0.5312\n",
      "  [Batch 260/280] Train Loss: 0.9902 Acc: 0.6875\n",
      "  [Batch 280/280] Train Loss: 0.9699 Acc: 0.6406\n",
      "Train Loss: 1.1047 Acc: 0.5893\n",
      "Val Loss: 1.0546 Acc: 0.6083\n",
      "\n",
      "  -> Val Loss 개선됨! (1.0546) 모델 저장.\n",
      "Epoch 3/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.9719 Acc: 0.6719\n",
      "  [Batch 40/280] Train Loss: 1.1013 Acc: 0.5781\n",
      "  [Batch 60/280] Train Loss: 0.8058 Acc: 0.7188\n",
      "  [Batch 80/280] Train Loss: 0.9774 Acc: 0.6719\n",
      "  [Batch 100/280] Train Loss: 1.1312 Acc: 0.5469\n",
      "  [Batch 120/280] Train Loss: 0.9299 Acc: 0.6406\n",
      "  [Batch 140/280] Train Loss: 1.0367 Acc: 0.6094\n",
      "  [Batch 160/280] Train Loss: 1.2455 Acc: 0.5469\n",
      "  [Batch 180/280] Train Loss: 1.1268 Acc: 0.5625\n",
      "  [Batch 200/280] Train Loss: 1.0587 Acc: 0.6406\n",
      "  [Batch 220/280] Train Loss: 1.2021 Acc: 0.6094\n",
      "  [Batch 240/280] Train Loss: 1.0155 Acc: 0.6719\n",
      "  [Batch 260/280] Train Loss: 1.0950 Acc: 0.5938\n",
      "  [Batch 280/280] Train Loss: 0.9837 Acc: 0.6719\n",
      "Train Loss: 1.0192 Acc: 0.6211\n",
      "Val Loss: 1.0609 Acc: 0.6103\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 4/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7863 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.8839 Acc: 0.7500\n",
      "  [Batch 60/280] Train Loss: 0.8627 Acc: 0.6562\n",
      "  [Batch 80/280] Train Loss: 1.0688 Acc: 0.6719\n",
      "  [Batch 100/280] Train Loss: 0.7998 Acc: 0.7031\n",
      "  [Batch 120/280] Train Loss: 0.8274 Acc: 0.6719\n",
      "  [Batch 140/280] Train Loss: 1.0146 Acc: 0.6562\n",
      "  [Batch 160/280] Train Loss: 0.8702 Acc: 0.6406\n",
      "  [Batch 180/280] Train Loss: 0.8405 Acc: 0.6875\n",
      "  [Batch 200/280] Train Loss: 0.8722 Acc: 0.6719\n",
      "  [Batch 220/280] Train Loss: 0.8459 Acc: 0.7188\n",
      "  [Batch 240/280] Train Loss: 0.9756 Acc: 0.6875\n",
      "  [Batch 260/280] Train Loss: 0.8569 Acc: 0.6406\n",
      "  [Batch 280/280] Train Loss: 0.7427 Acc: 0.7031\n",
      "Train Loss: 0.9684 Acc: 0.6428\n",
      "Val Loss: 1.0956 Acc: 0.6024\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 5/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.0075 Acc: 0.5469\n",
      "  [Batch 40/280] Train Loss: 0.9856 Acc: 0.6562\n",
      "  [Batch 60/280] Train Loss: 0.9167 Acc: 0.7031\n",
      "  [Batch 80/280] Train Loss: 0.7942 Acc: 0.6875\n",
      "  [Batch 100/280] Train Loss: 1.2611 Acc: 0.5625\n",
      "  [Batch 120/280] Train Loss: 1.1763 Acc: 0.5781\n",
      "  [Batch 140/280] Train Loss: 1.0327 Acc: 0.6562\n",
      "  [Batch 160/280] Train Loss: 0.7329 Acc: 0.7656\n",
      "  [Batch 180/280] Train Loss: 0.7408 Acc: 0.7500\n",
      "  [Batch 200/280] Train Loss: 0.9810 Acc: 0.7188\n",
      "  [Batch 220/280] Train Loss: 0.8950 Acc: 0.6406\n",
      "  [Batch 240/280] Train Loss: 0.7375 Acc: 0.7344\n",
      "  [Batch 260/280] Train Loss: 0.9780 Acc: 0.5312\n",
      "  [Batch 280/280] Train Loss: 0.9900 Acc: 0.6094\n",
      "Train Loss: 0.9322 Acc: 0.6566\n",
      "Val Loss: 0.9183 Acc: 0.6621\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9183) 모델 저장.\n",
      "Epoch 6/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.9128 Acc: 0.6562\n",
      "  [Batch 40/280] Train Loss: 0.9341 Acc: 0.6562\n",
      "  [Batch 60/280] Train Loss: 0.8887 Acc: 0.6562\n",
      "  [Batch 80/280] Train Loss: 0.9993 Acc: 0.5469\n",
      "  [Batch 100/280] Train Loss: 0.8137 Acc: 0.6875\n",
      "  [Batch 120/280] Train Loss: 0.8888 Acc: 0.7344\n",
      "  [Batch 140/280] Train Loss: 0.9255 Acc: 0.6094\n",
      "  [Batch 160/280] Train Loss: 0.9470 Acc: 0.6250\n",
      "  [Batch 180/280] Train Loss: 0.8072 Acc: 0.7344\n",
      "  [Batch 200/280] Train Loss: 0.8634 Acc: 0.7344\n",
      "  [Batch 220/280] Train Loss: 0.7380 Acc: 0.7031\n",
      "  [Batch 240/280] Train Loss: 1.0669 Acc: 0.6094\n",
      "  [Batch 260/280] Train Loss: 0.7892 Acc: 0.7188\n",
      "  [Batch 280/280] Train Loss: 1.0519 Acc: 0.6406\n",
      "Train Loss: 0.9020 Acc: 0.6617\n",
      "Val Loss: 0.9723 Acc: 0.6507\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 7/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8526 Acc: 0.5938\n",
      "  [Batch 40/280] Train Loss: 0.6732 Acc: 0.7656\n",
      "  [Batch 60/280] Train Loss: 0.9262 Acc: 0.6094\n",
      "  [Batch 80/280] Train Loss: 0.8584 Acc: 0.7031\n",
      "  [Batch 100/280] Train Loss: 0.7281 Acc: 0.7188\n",
      "  [Batch 120/280] Train Loss: 0.8159 Acc: 0.7188\n",
      "  [Batch 140/280] Train Loss: 0.7736 Acc: 0.6406\n",
      "  [Batch 160/280] Train Loss: 0.8560 Acc: 0.6875\n",
      "  [Batch 180/280] Train Loss: 0.7554 Acc: 0.7500\n",
      "  [Batch 200/280] Train Loss: 0.8841 Acc: 0.6562\n",
      "  [Batch 220/280] Train Loss: 0.7204 Acc: 0.7188\n",
      "  [Batch 240/280] Train Loss: 0.9450 Acc: 0.6719\n",
      "  [Batch 260/280] Train Loss: 1.1293 Acc: 0.5938\n",
      "  [Batch 280/280] Train Loss: 0.8857 Acc: 0.5938\n",
      "Train Loss: 0.8759 Acc: 0.6761\n",
      "Val Loss: 0.9441 Acc: 0.6587\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 8/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8265 Acc: 0.6406\n",
      "  [Batch 40/280] Train Loss: 0.7492 Acc: 0.6875\n",
      "  [Batch 60/280] Train Loss: 0.6774 Acc: 0.7031\n",
      "  [Batch 80/280] Train Loss: 0.6617 Acc: 0.7656\n",
      "  [Batch 100/280] Train Loss: 0.6619 Acc: 0.7812\n",
      "  [Batch 120/280] Train Loss: 0.7501 Acc: 0.7188\n",
      "  [Batch 140/280] Train Loss: 0.7883 Acc: 0.7188\n",
      "  [Batch 160/280] Train Loss: 0.6257 Acc: 0.7344\n",
      "  [Batch 180/280] Train Loss: 0.9426 Acc: 0.6406\n",
      "  [Batch 200/280] Train Loss: 0.8032 Acc: 0.7031\n",
      "  [Batch 220/280] Train Loss: 0.7185 Acc: 0.7188\n",
      "  [Batch 240/280] Train Loss: 0.6570 Acc: 0.7188\n",
      "  [Batch 260/280] Train Loss: 0.6007 Acc: 0.8281\n",
      "  [Batch 280/280] Train Loss: 0.7896 Acc: 0.7031\n",
      "Train Loss: 0.7539 Acc: 0.7212\n",
      "Val Loss: 0.8487 Acc: 0.6979\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8487) 모델 저장.\n",
      "Epoch 9/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6168 Acc: 0.7188\n",
      "  [Batch 40/280] Train Loss: 0.5787 Acc: 0.7500\n",
      "  [Batch 60/280] Train Loss: 0.6642 Acc: 0.7031\n",
      "  [Batch 80/280] Train Loss: 0.7424 Acc: 0.7188\n",
      "  [Batch 100/280] Train Loss: 0.6934 Acc: 0.7812\n",
      "  [Batch 120/280] Train Loss: 0.5683 Acc: 0.8594\n",
      "  [Batch 140/280] Train Loss: 0.6875 Acc: 0.7656\n",
      "  [Batch 160/280] Train Loss: 0.8403 Acc: 0.7188\n",
      "  [Batch 180/280] Train Loss: 0.6322 Acc: 0.7812\n",
      "  [Batch 200/280] Train Loss: 0.8284 Acc: 0.6719\n",
      "  [Batch 220/280] Train Loss: 0.7052 Acc: 0.7812\n",
      "  [Batch 240/280] Train Loss: 0.9271 Acc: 0.7031\n",
      "  [Batch 260/280] Train Loss: 0.9849 Acc: 0.7031\n",
      "  [Batch 280/280] Train Loss: 0.4900 Acc: 0.8594\n",
      "Train Loss: 0.7028 Acc: 0.7397\n",
      "Val Loss: 0.8550 Acc: 0.6990\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 10/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6909 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.7185 Acc: 0.7500\n",
      "  [Batch 60/280] Train Loss: 0.6131 Acc: 0.7969\n",
      "  [Batch 80/280] Train Loss: 0.7376 Acc: 0.6875\n",
      "  [Batch 100/280] Train Loss: 0.7619 Acc: 0.7188\n",
      "  [Batch 120/280] Train Loss: 0.7482 Acc: 0.6875\n",
      "  [Batch 140/280] Train Loss: 0.5826 Acc: 0.7812\n",
      "  [Batch 160/280] Train Loss: 0.5093 Acc: 0.8125\n",
      "  [Batch 180/280] Train Loss: 0.7802 Acc: 0.6875\n",
      "  [Batch 200/280] Train Loss: 0.6411 Acc: 0.7812\n",
      "  [Batch 220/280] Train Loss: 0.6240 Acc: 0.7969\n",
      "  [Batch 240/280] Train Loss: 0.7025 Acc: 0.6719\n",
      "  [Batch 260/280] Train Loss: 0.7517 Acc: 0.7188\n",
      "  [Batch 280/280] Train Loss: 0.7420 Acc: 0.6562\n",
      "Train Loss: 0.6834 Acc: 0.7460\n",
      "Val Loss: 0.8558 Acc: 0.7017\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 11/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6356 Acc: 0.7656\n",
      "  [Batch 40/280] Train Loss: 0.7155 Acc: 0.7031\n",
      "  [Batch 60/280] Train Loss: 0.5348 Acc: 0.7812\n",
      "  [Batch 80/280] Train Loss: 0.5506 Acc: 0.8281\n",
      "  [Batch 100/280] Train Loss: 0.7315 Acc: 0.7812\n",
      "  [Batch 120/280] Train Loss: 0.5263 Acc: 0.8438\n",
      "  [Batch 140/280] Train Loss: 0.6005 Acc: 0.7812\n",
      "  [Batch 160/280] Train Loss: 0.4486 Acc: 0.8906\n",
      "  [Batch 180/280] Train Loss: 0.7943 Acc: 0.7344\n",
      "  [Batch 200/280] Train Loss: 0.4814 Acc: 0.8281\n",
      "  [Batch 220/280] Train Loss: 0.5746 Acc: 0.7188\n",
      "  [Batch 240/280] Train Loss: 0.9753 Acc: 0.7031\n",
      "  [Batch 260/280] Train Loss: 0.5050 Acc: 0.8281\n",
      "  [Batch 280/280] Train Loss: 0.6140 Acc: 0.7500\n",
      "Train Loss: 0.6577 Acc: 0.7576\n",
      "Val Loss: 0.8645 Acc: 0.7013\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 12/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5672 Acc: 0.8281\n",
      "  [Batch 40/280] Train Loss: 0.7866 Acc: 0.7031\n",
      "  [Batch 60/280] Train Loss: 0.8794 Acc: 0.6875\n",
      "  [Batch 80/280] Train Loss: 0.5401 Acc: 0.8125\n",
      "  [Batch 100/280] Train Loss: 0.5913 Acc: 0.7812\n",
      "  [Batch 120/280] Train Loss: 0.6832 Acc: 0.7500\n",
      "  [Batch 140/280] Train Loss: 0.6658 Acc: 0.7031\n",
      "  [Batch 160/280] Train Loss: 0.6751 Acc: 0.7812\n",
      "  [Batch 180/280] Train Loss: 0.6221 Acc: 0.7812\n",
      "  [Batch 200/280] Train Loss: 0.5488 Acc: 0.7812\n",
      "  [Batch 220/280] Train Loss: 0.5274 Acc: 0.7969\n",
      "  [Batch 240/280] Train Loss: 0.7806 Acc: 0.7188\n",
      "  [Batch 260/280] Train Loss: 0.5592 Acc: 0.7812\n",
      "  [Batch 280/280] Train Loss: 0.6458 Acc: 0.7344\n",
      "Train Loss: 0.6269 Acc: 0.7660\n",
      "Val Loss: 0.8869 Acc: 0.7054\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 13/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4779 Acc: 0.7812\n",
      "  [Batch 40/280] Train Loss: 0.4089 Acc: 0.8906\n",
      "  [Batch 60/280] Train Loss: 0.5511 Acc: 0.8438\n",
      "  [Batch 80/280] Train Loss: 0.7112 Acc: 0.7812\n",
      "  [Batch 100/280] Train Loss: 0.6435 Acc: 0.7031\n",
      "  [Batch 120/280] Train Loss: 0.6390 Acc: 0.7344\n",
      "  [Batch 140/280] Train Loss: 0.6227 Acc: 0.7656\n",
      "  [Batch 160/280] Train Loss: 0.4795 Acc: 0.8438\n",
      "  [Batch 180/280] Train Loss: 0.6727 Acc: 0.7500\n",
      "  [Batch 200/280] Train Loss: 0.4952 Acc: 0.8281\n",
      "  [Batch 220/280] Train Loss: 0.9348 Acc: 0.5938\n",
      "  [Batch 240/280] Train Loss: 0.5323 Acc: 0.7969\n",
      "  [Batch 260/280] Train Loss: 0.6478 Acc: 0.7812\n",
      "  [Batch 280/280] Train Loss: 0.5653 Acc: 0.7812\n",
      "Train Loss: 0.6147 Acc: 0.7737\n",
      "Val Loss: 0.8955 Acc: 0.7002\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/10\n",
      "Epoch 14/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5520 Acc: 0.8281\n",
      "  [Batch 40/280] Train Loss: 0.6543 Acc: 0.7500\n",
      "  [Batch 60/280] Train Loss: 0.5435 Acc: 0.8125\n",
      "  [Batch 80/280] Train Loss: 0.7023 Acc: 0.7188\n",
      "  [Batch 100/280] Train Loss: 0.6457 Acc: 0.7812\n",
      "  [Batch 120/280] Train Loss: 0.6424 Acc: 0.7500\n",
      "  [Batch 140/280] Train Loss: 0.4710 Acc: 0.8281\n",
      "  [Batch 160/280] Train Loss: 0.5811 Acc: 0.7656\n",
      "  [Batch 180/280] Train Loss: 0.4735 Acc: 0.8125\n",
      "  [Batch 200/280] Train Loss: 0.5025 Acc: 0.7969\n",
      "  [Batch 220/280] Train Loss: 0.5267 Acc: 0.8125\n",
      "  [Batch 240/280] Train Loss: 0.4960 Acc: 0.8125\n",
      "  [Batch 260/280] Train Loss: 0.6177 Acc: 0.7656\n",
      "  [Batch 280/280] Train Loss: 0.6342 Acc: 0.7656\n",
      "Train Loss: 0.6024 Acc: 0.7789\n",
      "Val Loss: 0.8995 Acc: 0.7015\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/10\n",
      "Epoch 15/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5796 Acc: 0.7344\n",
      "  [Batch 40/280] Train Loss: 0.6265 Acc: 0.7188\n",
      "  [Batch 60/280] Train Loss: 0.5097 Acc: 0.8281\n",
      "  [Batch 80/280] Train Loss: 0.6303 Acc: 0.7344\n",
      "  [Batch 100/280] Train Loss: 0.5659 Acc: 0.7812\n",
      "  [Batch 120/280] Train Loss: 0.5120 Acc: 0.8125\n",
      "  [Batch 140/280] Train Loss: 0.5333 Acc: 0.7812\n",
      "  [Batch 160/280] Train Loss: 0.7355 Acc: 0.6875\n",
      "  [Batch 180/280] Train Loss: 0.5249 Acc: 0.7656\n",
      "  [Batch 200/280] Train Loss: 0.5223 Acc: 0.8125\n",
      "  [Batch 220/280] Train Loss: 0.4209 Acc: 0.8281\n",
      "  [Batch 240/280] Train Loss: 0.6091 Acc: 0.7500\n",
      "  [Batch 260/280] Train Loss: 0.4115 Acc: 0.8281\n",
      "  [Batch 280/280] Train Loss: 0.5473 Acc: 0.7812\n",
      "Train Loss: 0.5716 Acc: 0.7899\n",
      "Val Loss: 0.8939 Acc: 0.7036\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 7/10\n",
      "Epoch 16/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5607 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.4716 Acc: 0.8594\n",
      "  [Batch 60/280] Train Loss: 0.2949 Acc: 0.9375\n",
      "  [Batch 80/280] Train Loss: 0.8328 Acc: 0.6875\n",
      "  [Batch 100/280] Train Loss: 0.4388 Acc: 0.8281\n",
      "  [Batch 120/280] Train Loss: 0.4889 Acc: 0.7812\n",
      "  [Batch 140/280] Train Loss: 0.6952 Acc: 0.7656\n",
      "  [Batch 160/280] Train Loss: 0.4580 Acc: 0.8438\n",
      "  [Batch 180/280] Train Loss: 0.3909 Acc: 0.8594\n",
      "  [Batch 200/280] Train Loss: 0.6167 Acc: 0.7969\n",
      "  [Batch 220/280] Train Loss: 0.5717 Acc: 0.8281\n",
      "  [Batch 240/280] Train Loss: 0.6454 Acc: 0.7500\n",
      "  [Batch 260/280] Train Loss: 0.6029 Acc: 0.7969\n",
      "  [Batch 280/280] Train Loss: 0.5103 Acc: 0.8281\n",
      "Train Loss: 0.5717 Acc: 0.7904\n",
      "Val Loss: 0.8985 Acc: 0.7017\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 8/10\n",
      "Epoch 17/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6196 Acc: 0.7500\n",
      "  [Batch 40/280] Train Loss: 0.4917 Acc: 0.8125\n",
      "  [Batch 60/280] Train Loss: 0.5906 Acc: 0.8281\n",
      "  [Batch 80/280] Train Loss: 0.5595 Acc: 0.8438\n",
      "  [Batch 100/280] Train Loss: 0.5056 Acc: 0.8281\n",
      "  [Batch 120/280] Train Loss: 0.5647 Acc: 0.7969\n",
      "  [Batch 140/280] Train Loss: 0.5873 Acc: 0.7969\n",
      "  [Batch 160/280] Train Loss: 0.5146 Acc: 0.8438\n",
      "  [Batch 180/280] Train Loss: 0.4220 Acc: 0.8594\n",
      "  [Batch 200/280] Train Loss: 0.6936 Acc: 0.7812\n",
      "  [Batch 220/280] Train Loss: 0.5794 Acc: 0.7969\n",
      "  [Batch 240/280] Train Loss: 0.4945 Acc: 0.8438\n",
      "  [Batch 260/280] Train Loss: 0.6022 Acc: 0.7188\n",
      "  [Batch 280/280] Train Loss: 0.3962 Acc: 0.8438\n",
      "Train Loss: 0.5668 Acc: 0.7927\n",
      "Val Loss: 0.8978 Acc: 0.7022\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 9/10\n",
      "Epoch 18/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6203 Acc: 0.7812\n",
      "  [Batch 40/280] Train Loss: 0.6683 Acc: 0.7500\n",
      "  [Batch 60/280] Train Loss: 0.6475 Acc: 0.7812\n",
      "  [Batch 80/280] Train Loss: 0.4410 Acc: 0.8281\n",
      "  [Batch 100/280] Train Loss: 0.5667 Acc: 0.7656\n",
      "  [Batch 120/280] Train Loss: 0.4316 Acc: 0.8906\n",
      "  [Batch 140/280] Train Loss: 0.5388 Acc: 0.8281\n",
      "  [Batch 160/280] Train Loss: 0.4847 Acc: 0.8125\n",
      "  [Batch 180/280] Train Loss: 0.5936 Acc: 0.7969\n",
      "  [Batch 200/280] Train Loss: 0.6011 Acc: 0.7344\n",
      "  [Batch 220/280] Train Loss: 0.6276 Acc: 0.7812\n",
      "  [Batch 240/280] Train Loss: 0.4555 Acc: 0.7812\n",
      "  [Batch 260/280] Train Loss: 0.6209 Acc: 0.7500\n",
      "  [Batch 280/280] Train Loss: 0.5458 Acc: 0.7969\n",
      "Train Loss: 0.5644 Acc: 0.7943\n",
      "Val Loss: 0.9076 Acc: 0.7042\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 10/10\n",
      "\n",
      "Early stopping! 10 에폭 동안 성능 개선이 없었습니다.\n",
      "--------------------------------------------------\n",
      "Training complete in 8m 23s\n",
      "Saved Val Loss: 0.8487\n",
      "Saved Train Loss: 0.7539\n",
      "Saved Val Acc: 0.6979\n",
      "Saved Train Acc: 0.7212\n",
      "Saved Epoch: 8\n",
      "--------------------------------------------------\n",
      "Best Val Loss: 0.8487\n",
      "Best Train Loss: 0.5668\n",
      "Best Val Acc: 0.7054\n",
      "Best Train Acc: 0.7927\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer_speedup import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_5_percent_verified_processed\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'            #철원\n",
    "    #MODEL_NAME = 'resnet50'            #철원\n",
    "    MODEL_NAME = 'shufflenet_v2'        #철원\n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   #승현님\n",
    "    #MODEL_NAME = 'squeezenet'          #승현님\n",
    "    \n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "\n",
    "    \n",
    "    # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "    #train_transform = transforms.Compose([\n",
    "    #    transforms.Resize((224, 224)),\n",
    "    #    transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "    #    transforms.RandomRotation(15),           # -15도 ~ 15도 사이로 랜덤 회전\n",
    "    #    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 밝기, 대비, 채도 조절\n",
    "    #    transforms.ToTensor(),\n",
    "    #    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    #])\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        # TrivialAugmentWide 추가, 이미지에 다양한 변형(자르기, 색상 왜곡, 회전 등)을 알아서 최적의 강도로 적용, 과적합 방지.\n",
    "        transforms.TrivialAugmentWide(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 증강이 없는 검증/테스트용 Transform 정의\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1) # 7 에폭마다 학습률을 0.1배로 감소\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS, \n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    sampling_percent = 100\n",
    "    DATA_DIR = Path(f\"./datasets/KECV_{sampling_percent}_percent_FaceCrop\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'             #철원\n",
    "    #MODEL_NAME = 'resnet50' \n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'shufflenet_v2'       #철원\n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   #규진님\n",
    "    #MODEL_NAME = 'squeezenet'          #승희님\n",
    "    #MODEL_NAME = 'emotionnet'           # 감정 인식 전용 모델\n",
    "    MODEL_NAME = 'emonet'               # 경량화된 감정 인식 모델\n",
    "\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "    train_transform = None\n",
    "    val_transform = None\n",
    "    \n",
    "    if MODEL_NAME == 'emotionnet':\n",
    "        # 48x48 크기, 흑백(Grayscale), 정규화\n",
    "        # RandomResizedCrop + TrivialAugmentWide (강력한 데이터 증강 방법)\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((48, 48)),\n",
    "            # 원본 이미지의 80% ~ 100% 사이를 무작위로 잘라 48x48 크기로 만듦\n",
    "            transforms.RandomResizedCrop(size=48, scale=(0.8, 1.0)),\n",
    "            # 잘라낸 이미지에 최적의 증강 정책을 자동으로 적용\n",
    "            transforms.TrivialAugmentWide(),\n",
    "            # 흑백으로 변환\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지 정규화\n",
    "        ])\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지는 채널이 1개\n",
    "        ])\n",
    "\n",
    "    elif MODEL_NAME == 'emonet':\n",
    "        # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((256, 256)),\n",
    "            transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "            transforms.TrivialAugmentWide(), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # 증강이 없는 검증/테스트용 Transform 정의\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    else:\n",
    "        # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((224, 224)),\n",
    "            transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "            transforms.TrivialAugmentWide(), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        # 증강이 없는 검증/테스트용 Transform 정의\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    START_EPOCH = 0\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1)   # 7 에폭마다 학습률을 0.1배로 감소\n",
    "\n",
    "    CHECKPOINT_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained.pth'\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"체크포인트를 불러옵니다...\")\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        START_EPOCH = checkpoint['epoch'] + 1 # 다음 에폭부터 시작\n",
    "        print(f\"체크포인트 로드 완료! {START_EPOCH} 에폭부터 훈련을 재개합니다.\")\n",
    "    else:\n",
    "        print(\"체크포인트가 존재하지 않습니다. 처음부터 훈련을 시작합니다.\")\n",
    "    \n",
    "    #model = torch.compile(model)   # Windows 환경에서 에러 발생\n",
    "    #print(\"모델 컴파일 완료!\")\n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS,\n",
    "                                start_epoch=START_EPOCH,\n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    torch.save(trained_model.state_dict(), f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained.pth')\n",
    "    print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feellog-project (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
