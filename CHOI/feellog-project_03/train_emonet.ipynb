{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d0d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "------ 1 번째 체크포인트 모델링 시작 -------\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 14043\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "사전 훈련된 EmoNet 가중치를 불러옵니다 (Fine-tuning)...\n",
      "체크포인트가 존재하지 않습니다. 처음부터 훈련을 시작합니다.\n",
      "'emonet' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 8.5658 Acc: 0.1406\n",
      "  [Batch 219/219] Train Loss: 1.7478 Acc: 0.5312\n",
      "Train Loss: 3.5571 Acc: 0.4717\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Val Loss: 1.5322 Acc: 0.6635 Macro-F1: 0.4916\n",
      "  -> Val Loss 개선됨! (1.5322) 모델 저장.\n",
      "Epoch 2/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 2.0043 Acc: 0.5312\n",
      "  [Batch 219/219] Train Loss: 1.1559 Acc: 0.7031\n",
      "Train Loss: 1.3943 Acc: 0.6341\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Val Loss: 1.1692 Acc: 0.7363 Macro-F1: 0.5368\n",
      "  -> Val Loss 개선됨! (1.1692) 모델 저장.\n",
      "Epoch 3/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 1.1202 Acc: 0.7188\n",
      "  [Batch 219/219] Train Loss: 1.0833 Acc: 0.7031\n",
      "Train Loss: 1.2212 Acc: 0.6802\n",
      "Warning: Invalid predicted class index 7 found.\n",
      "Val Loss: 1.0290 Acc: 0.7860 Macro-F1: 0.5791\n",
      "  -> Val Loss 개선됨! (1.0290) 모델 저장.\n",
      "Epoch 4/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.9769 Acc: 0.7969\n",
      "  [Batch 219/219] Train Loss: 1.1133 Acc: 0.6562\n",
      "Train Loss: 1.1283 Acc: 0.7110\n",
      "Val Loss: 0.9601 Acc: 0.8005 Macro-F1: 0.6729\n",
      "  -> Val Loss 개선됨! (0.9601) 모델 저장.\n",
      "Epoch 5/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 1.2787 Acc: 0.6875\n",
      "  [Batch 219/219] Train Loss: 1.1158 Acc: 0.7031\n",
      "Train Loss: 1.0768 Acc: 0.7270\n",
      "Val Loss: 0.9129 Acc: 0.8117 Macro-F1: 0.6868\n",
      "  -> Val Loss 개선됨! (0.9129) 모델 저장.\n",
      "Epoch 6/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 1.0791 Acc: 0.6719\n",
      "  [Batch 219/219] Train Loss: 1.0114 Acc: 0.7188\n",
      "Train Loss: 1.0277 Acc: 0.7462\n",
      "Val Loss: 0.9683 Acc: 0.7760 Macro-F1: 0.6749\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 7/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 1.1429 Acc: 0.6719\n",
      "  [Batch 219/219] Train Loss: 1.0965 Acc: 0.6562\n",
      "Train Loss: 1.0057 Acc: 0.7539\n",
      "Val Loss: 0.9281 Acc: 0.8077 Macro-F1: 0.6878\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 8/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.9224 Acc: 0.7344\n",
      "  [Batch 219/219] Train Loss: 0.8692 Acc: 0.8594\n",
      "Train Loss: 0.9493 Acc: 0.7826\n",
      "Val Loss: 0.8322 Acc: 0.8422 Macro-F1: 0.7282\n",
      "  -> Val Loss 개선됨! (0.8322) 모델 저장.\n",
      "Epoch 9/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.9011 Acc: 0.7969\n",
      "  [Batch 219/219] Train Loss: 0.8204 Acc: 0.8594\n",
      "Train Loss: 0.9382 Acc: 0.7890\n",
      "Val Loss: 0.8336 Acc: 0.8427 Macro-F1: 0.7379\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 10/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.9584 Acc: 0.7969\n",
      "  [Batch 219/219] Train Loss: 0.9752 Acc: 0.7031\n",
      "Train Loss: 0.9332 Acc: 0.7914\n",
      "Val Loss: 0.8227 Acc: 0.8447 Macro-F1: 0.7296\n",
      "  -> Val Loss 개선됨! (0.8227) 모델 저장.\n",
      "Epoch 11/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.9466 Acc: 0.7969\n",
      "  [Batch 219/219] Train Loss: 0.9210 Acc: 0.7969\n",
      "Train Loss: 0.9317 Acc: 0.7903\n",
      "Val Loss: 0.8244 Acc: 0.8458 Macro-F1: 0.7443\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 12/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.9248 Acc: 0.7500\n",
      "  [Batch 219/219] Train Loss: 0.9427 Acc: 0.7500\n",
      "Train Loss: 0.9169 Acc: 0.7978\n",
      "Val Loss: 0.8282 Acc: 0.8429 Macro-F1: 0.7459\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 13/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.8612 Acc: 0.8281\n",
      "  [Batch 219/219] Train Loss: 0.9261 Acc: 0.8125\n",
      "Train Loss: 0.9149 Acc: 0.7976\n",
      "Val Loss: 0.8298 Acc: 0.8413 Macro-F1: 0.7373\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 14/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.8894 Acc: 0.7969\n",
      "  [Batch 219/219] Train Loss: 0.7317 Acc: 0.8906\n",
      "Train Loss: 0.9053 Acc: 0.8019\n",
      "Val Loss: 0.8139 Acc: 0.8487 Macro-F1: 0.7430\n",
      "  -> Val Loss 개선됨! (0.8139) 모델 저장.\n",
      "Epoch 15/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.8900 Acc: 0.8125\n",
      "  [Batch 219/219] Train Loss: 0.8439 Acc: 0.8281\n",
      "Train Loss: 0.9035 Acc: 0.8063\n",
      "Val Loss: 0.8158 Acc: 0.8464 Macro-F1: 0.7422\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 16/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.7468 Acc: 0.8750\n",
      "  [Batch 219/219] Train Loss: 0.8927 Acc: 0.8438\n",
      "Train Loss: 0.9083 Acc: 0.8039\n",
      "Val Loss: 0.8179 Acc: 0.8485 Macro-F1: 0.7435\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 17/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.9481 Acc: 0.7344\n",
      "  [Batch 219/219] Train Loss: 0.9352 Acc: 0.7969\n",
      "Train Loss: 0.9074 Acc: 0.8017\n",
      "Val Loss: 0.8235 Acc: 0.8435 Macro-F1: 0.7393\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 18/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.9720 Acc: 0.7188\n",
      "  [Batch 219/219] Train Loss: 0.8849 Acc: 0.8594\n",
      "Train Loss: 0.9034 Acc: 0.8071\n",
      "Val Loss: 0.8158 Acc: 0.8482 Macro-F1: 0.7436\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 19/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.8406 Acc: 0.8594\n",
      "  [Batch 219/219] Train Loss: 0.8841 Acc: 0.8438\n",
      "Train Loss: 0.9006 Acc: 0.8068\n",
      "Val Loss: 0.8150 Acc: 0.8500 Macro-F1: 0.7453\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/10\n",
      "Epoch 20/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 1.0202 Acc: 0.7656\n",
      "  [Batch 219/219] Train Loss: 0.8990 Acc: 0.7969\n",
      "Train Loss: 0.8957 Acc: 0.8076\n",
      "Val Loss: 0.8175 Acc: 0.8482 Macro-F1: 0.7425\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/10\n",
      "Epoch 21/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.9111 Acc: 0.8281\n",
      "  [Batch 219/219] Train Loss: 0.8539 Acc: 0.8438\n",
      "Train Loss: 0.8973 Acc: 0.8084\n",
      "Val Loss: 0.8167 Acc: 0.8498 Macro-F1: 0.7478\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 7/10\n",
      "Epoch 22/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.9448 Acc: 0.7969\n",
      "  [Batch 219/219] Train Loss: 0.8692 Acc: 0.8594\n",
      "Train Loss: 0.8984 Acc: 0.8067\n",
      "Val Loss: 0.8171 Acc: 0.8487 Macro-F1: 0.7435\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 8/10\n",
      "Epoch 23/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 0.8684 Acc: 0.8281\n",
      "  [Batch 219/219] Train Loss: 0.9742 Acc: 0.7656\n",
      "Train Loss: 0.8961 Acc: 0.8110\n",
      "Val Loss: 0.8146 Acc: 0.8496 Macro-F1: 0.7441\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 9/10\n",
      "Epoch 24/100\n",
      "----------\n",
      "  [Batch 20/219] Train Loss: 1.0238 Acc: 0.7344\n",
      "  [Batch 219/219] Train Loss: 1.1061 Acc: 0.7031\n",
      "Train Loss: 0.9020 Acc: 0.8074\n",
      "Val Loss: 0.8213 Acc: 0.8500 Macro-F1: 0.7491\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 10/10\n",
      "\n",
      "Early stopping! 10 에폭 동안 성능 개선이 없었습니다.\n",
      "--------------------------------------------------\n",
      "Training complete in 35m 1s\n",
      "Saved Epoch: 14\n",
      "--------------------------------------------------\n",
      "Saved Train Loss: 0.9053\n",
      "Saved Train Acc: 0.8019\n",
      "Saved Val Loss: 0.8139\n",
      "Saved Val Acc: 0.8487\n",
      "--------------------------------------------------\n",
      "Best Train Loss: 0.8957\n",
      "Best Train Acc: 0.8110\n",
      "Best Val Loss: 0.8139\n",
      "Best Val Acc: 0.8500\n",
      "--------------------------------------------------\n",
      "훈련된 모델 가중치가 저장되었습니다.\n",
      "상세 분석 결과가 저장되었습니다: ./infrastructure/models/weights/checkpoints/emonet_5_percent_trained_1_metrics.json\n",
      "------ 1 번째 체크포인트 모델링 완료 -------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.image_dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    sampling_percent = \"5\"\n",
    "    \n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'            \n",
    "    #MODEL_NAME = 'resnet50' \n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  \n",
    "    #MODEL_NAME = 'shufflenet_v2'       \n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   \n",
    "    #MODEL_NAME = 'squeezenet'          \n",
    "    #MODEL_NAME = 'emotionnet'  # 감정 인식 전용 모델\n",
    "    MODEL_NAME = 'emonet'       # 경량화된 감정 인식 모델\n",
    "\n",
    "    MISCLASSIFIED_DIR = Path(f\"./datasets/misclassified_images/{MODEL_NAME}_{sampling_percent}_percent\") # 오답 이미지를 저장할 폴더 경로 정의\n",
    "    DATA_DIR = Path(f\"./datasets/KECV_{sampling_percent}\")\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "    train_transform = None\n",
    "    val_transform = None\n",
    "    start_index = 1\n",
    "    end_index = 2\n",
    "    for index in range(start_index, end_index):\n",
    "        print(f\"------ {index} 번째 체크포인트 모델링 시작 -------\")\n",
    "        if MODEL_NAME == 'emotionnet':\n",
    "            # 48x48 크기, 흑백(Grayscale), 정규화\n",
    "            # RandomResizedCrop + TrivialAugmentWide (강력한 데이터 증강 방법)\n",
    "            train_transform = transforms.Compose([\n",
    "                #transforms.Resize((48, 48)),\n",
    "                # 원본 이미지의 80% ~ 100% 사이를 무작위로 잘라 48x48 크기로 만듦\n",
    "                transforms.RandomResizedCrop(size=48, scale=(0.8, 1.0)),\n",
    "                # 잘라낸 이미지에 최적의 증강 정책을 자동으로 적용\n",
    "                transforms.TrivialAugmentWide(),\n",
    "                # 흑백으로 변환\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지 정규화\n",
    "            ])\n",
    "            val_transform = transforms.Compose([\n",
    "                transforms.Resize((48, 48)),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지는 채널이 1개\n",
    "            ])\n",
    "\n",
    "        elif MODEL_NAME == 'emonet':\n",
    "            # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "            train_transform = transforms.Compose([\n",
    "                #transforms.Resize((256, 256)),\n",
    "                transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "                transforms.TrivialAugmentWide(), \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            # 증강이 없는 검증/테스트용 Transform 정의\n",
    "            val_transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "        else:\n",
    "            # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "            train_transform = transforms.Compose([\n",
    "                #transforms.Resize((224, 224)),\n",
    "                transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "                transforms.TrivialAugmentWide(), \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "            # 증강이 없는 검증/테스트용 Transform 정의\n",
    "            val_transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        \n",
    "        # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "        train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "        val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "        # DataLoader I/O 튜닝\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True,\n",
    "            # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "            num_workers=min(8, os.cpu_count()), \n",
    "            pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "            persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "            prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "            drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=False,\n",
    "            num_workers=min(8, os.cpu_count()),\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2\n",
    "        )\n",
    "\n",
    "        NUM_CLASSES = len(train_dataset.classes)\n",
    "        \n",
    "        print(\"데이터 준비 완료!\")\n",
    "        print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "        print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "        # 모델, 손실 함수, 옵티마이저 준비\n",
    "        model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "        \n",
    "        # 모델을 지정된 장치로 이동\n",
    "        model.to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), \n",
    "            weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "            lr=LEARNING_RATE \n",
    "            ) \n",
    "        START_EPOCH = 0\n",
    "        \n",
    "        scheduler = StepLR(optimizer, step_size=7, gamma=0.1)   # 7 에폭마다 학습률을 0.1배로 감소\n",
    "        #ReduceLROnPlateau\n",
    "        #CosineAnnealingLR\n",
    "        if index > 0:\n",
    "            CHECKPOINT_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained_{index-1}.pth'\n",
    "            if os.path.exists(CHECKPOINT_PATH):\n",
    "                print(\"체크포인트를 불러옵니다...\")\n",
    "                checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "                model.load_state_dict(checkpoint)\n",
    "                print(\"체크포인트(모델 가중치) 로드 완료!\")\n",
    "            else:\n",
    "                print(\"체크포인트가 존재하지 않습니다. 처음부터 훈련을 시작합니다.\")\n",
    "        \n",
    "        #model = torch.compile(model)   # Windows 환경에서 에러 발생\n",
    "        #print(\"모델 컴파일 완료!\")\n",
    "        print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "        \n",
    "        #MISCLASSIFIED_DIR = f'{MISCLASSIFIED_DIR}_{index}'\n",
    "\n",
    "        # 모델 훈련 시작\n",
    "        print(\"\\n모델 훈련을 시작합니다...\")\n",
    "        trained_model, saved_metrics = train_model(model, \n",
    "                                    train_loader, \n",
    "                                    val_loader, \n",
    "                                    criterion, \n",
    "                                    optimizer, \n",
    "                                    scheduler,\n",
    "                                    DEVICE, \n",
    "                                    num_epochs=NUM_EPOCHS,\n",
    "                                    start_epoch=START_EPOCH,\n",
    "                                    patience=EARLY_STOPPING_PATIENCE,\n",
    "                                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                                    misclassified_dir=MISCLASSIFIED_DIR\n",
    "                                    )\n",
    "\n",
    "        # 훈련된 모델 저장 (옵션)\n",
    "        CHECKPOINT_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained_{index}.pth'\n",
    "        torch.save(trained_model.state_dict(), CHECKPOINT_PATH)\n",
    "        print(\"훈련된 모델 가중치가 저장되었습니다.\")\n",
    "        \n",
    "        # 최고 성능 시점의 상세 분석 결과를 JSON으로 저장\n",
    "        METRICS_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained_{index}_metrics.json'\n",
    "        with open(METRICS_PATH, 'w', encoding='utf-8') as f:\n",
    "            json.dump(saved_metrics, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"상세 분석 결과가 저장되었습니다: {METRICS_PATH}\")\n",
    "        \n",
    "        print(f\"------ {index} 번째 체크포인트 모델링 완료 -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb59643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2.7.1+cu126'}\n"
     ]
    }
   ],
   "source": [
    "print({torch.__version__})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15aedbed",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torchvision\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficientnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torchvision\\models\\convnext.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstochastic_depth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_presets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torchvision\\ops\\__init__.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgiou_loss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generalized_box_iou_loss\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpoolers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiScaleRoIAlign\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mps_roi_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ps_roi_align, PSRoIAlign\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mps_roi_pool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ps_roi_pool, PSRoIPool\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torchvision\\ops\\poolers.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboxes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m box_area\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroi_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roi_align\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# is not supported by ONNX tracing yet.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# that merges the levels to the right indices\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39munused\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_onnx_merge_levels\u001b[39m(levels: Tensor, unmerged_results: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torchvision\\ops\\roi_align.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_compile_supported\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BroadcastingList2\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pair\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torch\\_dynamo\\exc.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtypes\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torch\\_dynamo\\utils.py:69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytree\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fx\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ordered_set\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedSet\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     68\u001b[0m     Application,\n\u001b[0;32m     69\u001b[0m     CeilToInt,\n\u001b[0;32m     70\u001b[0m     CleanDiv,\n\u001b[0;32m     71\u001b[0m     FloorDiv,\n\u001b[0;32m     72\u001b[0m     FloorToInt,\n\u001b[0;32m     73\u001b[0m     IsNonOverlappingAndDenseIndicator,\n\u001b[0;32m     74\u001b[0m     Max,\n\u001b[0;32m     75\u001b[0m     Mod,\n\u001b[0;32m     76\u001b[0m     PythonMod,\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m int_oo\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprinters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CppPrinter, PythonPrinter\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\torch\\utils\\_sympy\\functions.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable, Optional, SupportsFloat, TYPE_CHECKING, TypeVar, Union\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeVarTuple, Unpack\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msympy\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m S\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sympify\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\sympy\\__init__.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m mpmath\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelease\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lazy_function\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m __version__:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21menable_warnings\u001b[39m():\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\sympy\\core\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Basic, Atom\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msingleton\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m S\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Expr, AtomicExpr, UnevaluatedExpr\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Symbol, Wild, Dummy, symbols, var\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Number, Float, Rational, Integer, NumberSymbol, \\\n\u001b[0;32m     12\u001b[0m     RealNumber, igcd, ilcm, seterr, E, I, nan, oo, pi, zoo, \\\n\u001b[0;32m     13\u001b[0m     AlgebraicNumber, comp, mod_inverse\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\sympy\\core\\expr.py:4157\u001b[0m\n\u001b[0;32m   4153\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m (i,)\n\u001b[0;32m   4154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmul\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mul\n\u001b[0;32m   4158\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Add\n\u001b[0;32m   4159\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpower\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pow\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\sympy\\core\\mul.py:2194\u001b[0m\n\u001b[0;32m   2190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m e\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bottom_up(e, do)\n\u001b[1;32m-> 2194\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Rational\n\u001b[0;32m   2195\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpower\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pow\n\u001b[0;32m   2196\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Add, _unevaluated_Add\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\sympy\\core\\numbers.py:4361\u001b[0m\n\u001b[0;32m   4357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m real \u001b[38;5;241m+\u001b[39m S\u001b[38;5;241m.\u001b[39mImaginaryUnit\u001b[38;5;241m*\u001b[39mimag\n\u001b[0;32m   4359\u001b[0m _sympy_converter[\u001b[38;5;28mcomplex\u001b[39m] \u001b[38;5;241m=\u001b[39m sympify_complex\n\u001b[1;32m-> 4361\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpower\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pow\n\u001b[0;32m   4362\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmul\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mul\n\u001b[0;32m   4363\u001b[0m Mul\u001b[38;5;241m.\u001b[39midentity \u001b[38;5;241m=\u001b[39m One()\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\sympy\\core\\power.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Expr\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevalf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrecisionExhausted\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (expand_complex, expand_multinomial,\n\u001b[0;32m     11\u001b[0m     expand_mul, _mexpand, PoleError)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fuzzy_bool, fuzzy_not, fuzzy_and, fuzzy_or\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m global_parameters\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\sympy\\core\\function.py:3393\u001b[0m\n\u001b[0;32m   3384\u001b[0m         rv \u001b[38;5;241m=\u001b[39m rv\u001b[38;5;241m.\u001b[39mxreplace(Transform(\n\u001b[0;32m   3385\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m x: Pow(x\u001b[38;5;241m.\u001b[39mbase, Float(x\u001b[38;5;241m.\u001b[39mexp, n)),\n\u001b[0;32m   3386\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mis_Pow \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mexp\u001b[38;5;241m.\u001b[39mis_Integer))\n\u001b[0;32m   3388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rv\u001b[38;5;241m.\u001b[39mxreplace(Transform(\n\u001b[0;32m   3389\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39mnfloat(x\u001b[38;5;241m.\u001b[39margs, n, exponent)),\n\u001b[0;32m   3390\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28misinstance\u001b[39m(x, Function) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, AppliedUndef)))\n\u001b[1;32m-> 3393\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dummy, Symbol\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\sympy\\core\\symbol.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msorting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ordered\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msympify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sympify\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Boolean\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sift, is_sequence\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m filldedent\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project_02\\.venv\\lib\\site-packages\\sympy\\logic\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (to_cnf, to_dnf, to_nnf, And, Or, Not, Xor, Nand, Nor, Implies,\n\u001b[0;32m      2\u001b[0m     Equivalent, ITE, POSform, SOPform, simplify_logic, bool_map, true, false,\n\u001b[0;32m      3\u001b[0m     gateinputcount)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m satisfiable\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_cnf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_dnf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_nnf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNand\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNor\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplies\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEquivalent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mITE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOSform\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSOPform\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimplify_logic\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msatisfiable\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m ]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:982\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1039\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.image_dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    sampling_percent = 10\n",
    "    DATA_DIR = Path(f\"./datasets/KECV_{sampling_percent}_percent_FaceCrop\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'             #철원\n",
    "    #MODEL_NAME = 'resnet50' \n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'shufflenet_v2'       #철원\n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   #규진님\n",
    "    #MODEL_NAME = 'squeezenet'          #승희님\n",
    "    #MODEL_NAME = 'emotionnet'           # 감정 인식 전용 모델\n",
    "    MODEL_NAME = 'emonet'               # 경량화된 감정 인식 모델\n",
    "\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "    train_transform = None\n",
    "    val_transform = None\n",
    "    \n",
    "    if MODEL_NAME == 'emotionnet':\n",
    "        # 48x48 크기, 흑백(Grayscale), 정규화\n",
    "        # RandomResizedCrop + TrivialAugmentWide (강력한 데이터 증강 방법)\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((48, 48)),\n",
    "            # 원본 이미지의 80% ~ 100% 사이를 무작위로 잘라 48x48 크기로 만듦\n",
    "            transforms.RandomResizedCrop(size=48, scale=(0.8, 1.0)),\n",
    "            # 잘라낸 이미지에 최적의 증강 정책을 자동으로 적용\n",
    "            transforms.TrivialAugmentWide(),\n",
    "            # 흑백으로 변환\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지 정규화\n",
    "        ])\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지는 채널이 1개\n",
    "        ])\n",
    "\n",
    "    elif MODEL_NAME == 'emonet':\n",
    "        # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((256, 256)),\n",
    "            transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "            transforms.TrivialAugmentWide(), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # 증강이 없는 검증/테스트용 Transform 정의\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    else:\n",
    "        # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((224, 224)),\n",
    "            transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "            transforms.TrivialAugmentWide(), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        # 증강이 없는 검증/테스트용 Transform 정의\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    START_EPOCH = 0\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1)   # 7 에폭마다 학습률을 0.1배로 감소\n",
    "\n",
    "    CHECKPOINT_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained.pth'\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"체크포인트를 불러옵니다...\")\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"체크포인트(모델 가중치) 로드 완료! 0 에폭부터 훈련을 시작합니다.\")\n",
    "    else:\n",
    "        print(\"체크포인트가 존재하지 않습니다. 처음부터 훈련을 시작합니다.\")\n",
    "        \n",
    "    #model = torch.compile(model)   # Windows 환경에서 에러 발생\n",
    "    #print(\"모델 컴파일 완료!\")\n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "    \n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS,\n",
    "                                start_epoch=START_EPOCH,\n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    torch.save(trained_model.state_dict(), f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained.pth')\n",
    "    print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d08d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 97313\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "사전 훈련된 EmoNet 가중치를 불러옵니다 (Fine-tuning)...\n",
      "체크포인트가 존재하지 않습니다. 처음부터 훈련을 시작합니다.\n",
      "'emonet' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 9.4913 Acc: 0.1719\n",
      "  [Batch 1520/1520] Train Loss: 0.9691 Acc: 0.6094\n",
      "Train Loss: 1.6004 Acc: 0.5138\n",
      "Val Loss: 0.9596 Acc: 0.6376\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9596) 모델 저장.\n",
      "Epoch 2/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 1.0442 Acc: 0.6562\n",
      "  [Batch 1520/1520] Train Loss: 0.9538 Acc: 0.6562\n",
      "Train Loss: 1.0715 Acc: 0.5957\n",
      "Val Loss: 0.9830 Acc: 0.6267\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 3/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.9631 Acc: 0.6250\n",
      "  [Batch 1520/1520] Train Loss: 1.0499 Acc: 0.6406\n",
      "Train Loss: 1.0107 Acc: 0.6183\n",
      "Val Loss: 0.9404 Acc: 0.6463\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9404) 모델 저장.\n",
      "Epoch 4/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.9370 Acc: 0.6250\n",
      "  [Batch 1520/1520] Train Loss: 0.8340 Acc: 0.7188\n",
      "Train Loss: 0.9766 Acc: 0.6321\n",
      "Val Loss: 0.8876 Acc: 0.6685\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8876) 모델 저장.\n",
      "Epoch 5/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.9602 Acc: 0.6250\n",
      "  [Batch 1520/1520] Train Loss: 0.9754 Acc: 0.6250\n",
      "Train Loss: 0.9565 Acc: 0.6401\n",
      "Val Loss: 0.8783 Acc: 0.6702\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8783) 모델 저장.\n",
      "Epoch 6/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 1.0346 Acc: 0.6250\n",
      "  [Batch 1520/1520] Train Loss: 0.8921 Acc: 0.6875\n",
      "Train Loss: 0.9380 Acc: 0.6484\n",
      "Val Loss: 0.8698 Acc: 0.6759\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8698) 모델 저장.\n",
      "Epoch 7/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.9611 Acc: 0.7031\n",
      "  [Batch 1520/1520] Train Loss: 0.9120 Acc: 0.6562\n",
      "Train Loss: 0.9221 Acc: 0.6541\n",
      "Val Loss: 0.8396 Acc: 0.6858\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8396) 모델 저장.\n",
      "Epoch 8/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 1.0250 Acc: 0.5938\n",
      "  [Batch 1520/1520] Train Loss: 0.9442 Acc: 0.5938\n",
      "Train Loss: 0.8447 Acc: 0.6799\n",
      "Val Loss: 0.7451 Acc: 0.7184\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7451) 모델 저장.\n",
      "Epoch 9/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.6904 Acc: 0.7188\n",
      "  [Batch 1520/1520] Train Loss: 0.8877 Acc: 0.6406\n",
      "Train Loss: 0.8171 Acc: 0.6936\n",
      "Val Loss: 0.7363 Acc: 0.7221\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7363) 모델 저장.\n",
      "Epoch 10/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.9268 Acc: 0.6406\n",
      "  [Batch 1520/1520] Train Loss: 0.6742 Acc: 0.7812\n",
      "Train Loss: 0.8015 Acc: 0.7004\n",
      "Val Loss: 0.7324 Acc: 0.7239\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7324) 모델 저장.\n",
      "Epoch 11/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 1.0682 Acc: 0.5625\n",
      "  [Batch 1520/1520] Train Loss: 0.9049 Acc: 0.6562\n",
      "Train Loss: 0.7899 Acc: 0.7022\n",
      "Val Loss: 0.7222 Acc: 0.7267\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7222) 모델 저장.\n",
      "Epoch 12/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.5966 Acc: 0.8281\n",
      "  [Batch 1520/1520] Train Loss: 0.9967 Acc: 0.6250\n",
      "Train Loss: 0.7774 Acc: 0.7080\n",
      "Val Loss: 0.7206 Acc: 0.7298\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7206) 모델 저장.\n",
      "Epoch 13/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7347 Acc: 0.7031\n",
      "  [Batch 1520/1520] Train Loss: 0.6733 Acc: 0.8125\n",
      "Train Loss: 0.7689 Acc: 0.7112\n",
      "Val Loss: 0.7162 Acc: 0.7312\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7162) 모델 저장.\n",
      "Epoch 14/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.8206 Acc: 0.7188\n",
      "  [Batch 1520/1520] Train Loss: 0.8086 Acc: 0.6875\n",
      "Train Loss: 0.7606 Acc: 0.7141\n",
      "Val Loss: 0.7115 Acc: 0.7349\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7115) 모델 저장.\n",
      "Epoch 15/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.8561 Acc: 0.6094\n",
      "  [Batch 1520/1520] Train Loss: 0.6284 Acc: 0.8125\n",
      "Train Loss: 0.7501 Acc: 0.7187\n",
      "Val Loss: 0.7042 Acc: 0.7372\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7042) 모델 저장.\n",
      "Epoch 16/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.6715 Acc: 0.7500\n",
      "  [Batch 1520/1520] Train Loss: 0.5588 Acc: 0.8438\n",
      "Train Loss: 0.7437 Acc: 0.7216\n",
      "Val Loss: 0.7058 Acc: 0.7366\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 17/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7215 Acc: 0.7344\n",
      "  [Batch 1520/1520] Train Loss: 0.7239 Acc: 0.7031\n",
      "Train Loss: 0.7392 Acc: 0.7220\n",
      "Val Loss: 0.7031 Acc: 0.7372\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7031) 모델 저장.\n",
      "Epoch 18/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7465 Acc: 0.6562\n",
      "  [Batch 1520/1520] Train Loss: 0.7422 Acc: 0.7656\n",
      "Train Loss: 0.7368 Acc: 0.7239\n",
      "Val Loss: 0.7033 Acc: 0.7375\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 19/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.8450 Acc: 0.6562\n",
      "  [Batch 1520/1520] Train Loss: 0.5545 Acc: 0.8125\n",
      "Train Loss: 0.7353 Acc: 0.7242\n",
      "Val Loss: 0.7013 Acc: 0.7381\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7013) 모델 저장.\n",
      "Epoch 20/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.9986 Acc: 0.6562\n",
      "  [Batch 1520/1520] Train Loss: 0.5485 Acc: 0.7812\n",
      "Train Loss: 0.7351 Acc: 0.7229\n",
      "Val Loss: 0.7006 Acc: 0.7386\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7006) 모델 저장.\n",
      "Epoch 21/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7948 Acc: 0.7188\n",
      "  [Batch 1520/1520] Train Loss: 0.7521 Acc: 0.7188\n",
      "Train Loss: 0.7321 Acc: 0.7246\n",
      "Val Loss: 0.7012 Acc: 0.7386\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 22/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.6496 Acc: 0.8125\n",
      "  [Batch 1520/1520] Train Loss: 0.8191 Acc: 0.7344\n",
      "Train Loss: 0.7322 Acc: 0.7254\n",
      "Val Loss: 0.7009 Acc: 0.7386\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 23/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7082 Acc: 0.7344\n",
      "  [Batch 1520/1520] Train Loss: 0.8026 Acc: 0.6875\n",
      "Train Loss: 0.7299 Acc: 0.7271\n",
      "Val Loss: 0.6995 Acc: 0.7382\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6995) 모델 저장.\n",
      "Epoch 24/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7877 Acc: 0.7031\n",
      "  [Batch 1520/1520] Train Loss: 1.0810 Acc: 0.5938\n",
      "Train Loss: 0.7284 Acc: 0.7258\n",
      "Val Loss: 0.7009 Acc: 0.7387\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 25/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7019 Acc: 0.7031\n",
      "  [Batch 1520/1520] Train Loss: 0.6811 Acc: 0.7656\n",
      "Train Loss: 0.7312 Acc: 0.7258\n",
      "Val Loss: 0.7010 Acc: 0.7381\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 26/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7756 Acc: 0.7500\n",
      "  [Batch 1520/1520] Train Loss: 0.7968 Acc: 0.6562\n",
      "Train Loss: 0.7271 Acc: 0.7280\n",
      "Val Loss: 0.7001 Acc: 0.7385\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 27/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.6931 Acc: 0.7188\n",
      "  [Batch 1520/1520] Train Loss: 0.7288 Acc: 0.7344\n",
      "Train Loss: 0.7301 Acc: 0.7281\n",
      "Val Loss: 0.7006 Acc: 0.7384\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 28/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7854 Acc: 0.7656\n",
      "  [Batch 1520/1520] Train Loss: 0.7881 Acc: 0.6719\n",
      "Train Loss: 0.7309 Acc: 0.7249\n",
      "Val Loss: 0.7016 Acc: 0.7376\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/10\n",
      "Epoch 29/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.8226 Acc: 0.7656\n",
      "  [Batch 1520/1520] Train Loss: 0.6433 Acc: 0.7969\n",
      "Train Loss: 0.7291 Acc: 0.7263\n",
      "Val Loss: 0.6999 Acc: 0.7388\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/10\n",
      "Epoch 30/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.8490 Acc: 0.6875\n",
      "  [Batch 1520/1520] Train Loss: 0.9054 Acc: 0.7031\n",
      "Train Loss: 0.7277 Acc: 0.7284\n",
      "Val Loss: 0.7006 Acc: 0.7383\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 7/10\n",
      "Epoch 31/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.8026 Acc: 0.7188\n",
      "  [Batch 1520/1520] Train Loss: 0.7730 Acc: 0.7500\n",
      "Train Loss: 0.7258 Acc: 0.7272\n",
      "Val Loss: 0.7003 Acc: 0.7383\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 8/10\n",
      "Epoch 32/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7423 Acc: 0.7344\n",
      "  [Batch 1520/1520] Train Loss: 0.6886 Acc: 0.7969\n",
      "Train Loss: 0.7306 Acc: 0.7268\n",
      "Val Loss: 0.7000 Acc: 0.7388\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 9/10\n",
      "Epoch 33/100\n",
      "----------\n",
      "  [Batch 20/1520] Train Loss: 0.7674 Acc: 0.7031\n",
      "  [Batch 1520/1520] Train Loss: 0.7388 Acc: 0.6406\n",
      "Train Loss: 0.7279 Acc: 0.7268\n",
      "Val Loss: 0.7005 Acc: 0.7380\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 10/10\n",
      "\n",
      "Early stopping! 10 에폭 동안 성능 개선이 없었습니다.\n",
      "--------------------------------------------------\n",
      "Training complete in 274m 3s\n",
      "Saved Epoch: 23\n",
      "--------------------------------------------------\n",
      "Saved Train Loss: 0.7299\n",
      "Saved Train Acc: 0.7271\n",
      "Saved Val Loss: 0.6995\n",
      "Saved Val Acc: 0.7382\n",
      "--------------------------------------------------\n",
      "Best Train Loss: 0.7258\n",
      "Best Train Acc: 0.7284\n",
      "Best Val Loss: 0.6995\n",
      "Best Val Acc: 0.7388\n",
      "--------------------------------------------------\n",
      "훈련된 모델 가중치가 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.image_dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    sampling_percent = 30\n",
    "    DATA_DIR = Path(f\"./datasets/KECV_{sampling_percent}_percent_FaceCrop\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'             #철원\n",
    "    #MODEL_NAME = 'resnet50' \n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'shufflenet_v2'       #철원\n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   #규진님\n",
    "    #MODEL_NAME = 'squeezenet'          #승희님\n",
    "    #MODEL_NAME = 'emotionnet'           # 감정 인식 전용 모델\n",
    "    MODEL_NAME = 'emonet'               # 경량화된 감정 인식 모델\n",
    "\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "    train_transform = None\n",
    "    val_transform = None\n",
    "    \n",
    "    if MODEL_NAME == 'emotionnet':\n",
    "        # 48x48 크기, 흑백(Grayscale), 정규화\n",
    "        # RandomResizedCrop + TrivialAugmentWide (강력한 데이터 증강 방법)\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((48, 48)),\n",
    "            # 원본 이미지의 80% ~ 100% 사이를 무작위로 잘라 48x48 크기로 만듦\n",
    "            transforms.RandomResizedCrop(size=48, scale=(0.8, 1.0)),\n",
    "            # 잘라낸 이미지에 최적의 증강 정책을 자동으로 적용\n",
    "            transforms.TrivialAugmentWide(),\n",
    "            # 흑백으로 변환\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지 정규화\n",
    "        ])\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지는 채널이 1개\n",
    "        ])\n",
    "\n",
    "    elif MODEL_NAME == 'emonet':\n",
    "        # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((256, 256)),\n",
    "            transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "            transforms.TrivialAugmentWide(), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # 증강이 없는 검증/테스트용 Transform 정의\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    else:\n",
    "        # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((224, 224)),\n",
    "            transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "            transforms.TrivialAugmentWide(), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        # 증강이 없는 검증/테스트용 Transform 정의\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    START_EPOCH = 0\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1)   # 7 에폭마다 학습률을 0.1배로 감소\n",
    "\n",
    "    CHECKPOINT_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained.pth'\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"체크포인트를 불러옵니다...\")\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"체크포인트(모델 가중치) 로드 완료! 0 에폭부터 훈련을 시작합니다.\")\n",
    "    else:\n",
    "        print(\"체크포인트가 존재하지 않습니다. 처음부터 훈련을 시작합니다.\")\n",
    "        \n",
    "    #model = torch.compile(model)   # Windows 환경에서 에러 발생\n",
    "    #print(\"모델 컴파일 완료!\")\n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS,\n",
    "                                start_epoch=START_EPOCH,\n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    torch.save(trained_model.state_dict(), f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained.pth')\n",
    "    print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 162173\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "사전 훈련된 EmoNet 가중치를 불러옵니다 (Fine-tuning)...\n",
      "체크포인트가 존재하지 않습니다. 처음부터 훈련을 시작합니다.\n",
      "'emonet' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 10.3376 Acc: 0.1406\n",
      "  [Batch 2533/2533] Train Loss: 0.9927 Acc: 0.5781\n",
      "Train Loss: 1.4008 Acc: 0.5413\n",
      "Val Loss: 0.9587 Acc: 0.6353\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9587) 모델 저장.\n",
      "Epoch 2/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.9834 Acc: 0.6562\n",
      "  [Batch 2533/2533] Train Loss: 0.8182 Acc: 0.6406\n",
      "Train Loss: 1.0165 Acc: 0.6161\n",
      "Val Loss: 0.8758 Acc: 0.6720\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8758) 모델 저장.\n",
      "Epoch 3/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 1.0060 Acc: 0.5625\n",
      "  [Batch 2533/2533] Train Loss: 0.9492 Acc: 0.6406\n",
      "Train Loss: 0.9683 Acc: 0.6350\n",
      "Val Loss: 0.8348 Acc: 0.6857\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8348) 모델 저장.\n",
      "Epoch 4/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 1.1009 Acc: 0.5469\n",
      "  [Batch 2533/2533] Train Loss: 0.9776 Acc: 0.6250\n",
      "Train Loss: 0.9413 Acc: 0.6466\n",
      "Val Loss: 0.9581 Acc: 0.6472\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 5/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.8320 Acc: 0.6562\n",
      "  [Batch 2533/2533] Train Loss: 1.0466 Acc: 0.5938\n",
      "Train Loss: 0.9204 Acc: 0.6553\n",
      "Val Loss: 0.8487 Acc: 0.6870\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 6/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 1.0082 Acc: 0.6094\n",
      "  [Batch 2533/2533] Train Loss: 0.8649 Acc: 0.6406\n",
      "Train Loss: 0.9077 Acc: 0.6590\n",
      "Val Loss: 0.8281 Acc: 0.6886\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8281) 모델 저장.\n",
      "Epoch 7/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.8700 Acc: 0.6719\n",
      "  [Batch 2533/2533] Train Loss: 0.7884 Acc: 0.7031\n",
      "Train Loss: 0.8976 Acc: 0.6619\n",
      "Val Loss: 0.8067 Acc: 0.6999\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8067) 모델 저장.\n",
      "Epoch 8/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.9149 Acc: 0.5781\n",
      "  [Batch 2533/2533] Train Loss: 0.7144 Acc: 0.6875\n",
      "Train Loss: 0.8150 Acc: 0.6947\n",
      "Val Loss: 0.7241 Acc: 0.7306\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7241) 모델 저장.\n",
      "Epoch 9/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.7818 Acc: 0.7344\n",
      "  [Batch 2533/2533] Train Loss: 0.7161 Acc: 0.7500\n",
      "Train Loss: 0.7901 Acc: 0.7043\n",
      "Val Loss: 0.7089 Acc: 0.7348\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7089) 모델 저장.\n",
      "Epoch 10/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.8418 Acc: 0.7031\n",
      "  [Batch 2533/2533] Train Loss: 0.7149 Acc: 0.7344\n",
      "Train Loss: 0.7762 Acc: 0.7093\n",
      "Val Loss: 0.7018 Acc: 0.7391\n",
      "\n",
      "  -> Val Loss 개선됨! (0.7018) 모델 저장.\n",
      "Epoch 11/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.6869 Acc: 0.7344\n",
      "  [Batch 2533/2533] Train Loss: 0.7517 Acc: 0.7188\n",
      "Train Loss: 0.7621 Acc: 0.7142\n",
      "Val Loss: 0.6983 Acc: 0.7405\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6983) 모델 저장.\n",
      "Epoch 12/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.8281 Acc: 0.6875\n",
      "  [Batch 2533/2533] Train Loss: 0.8745 Acc: 0.6719\n",
      "Train Loss: 0.7535 Acc: 0.7184\n",
      "Val Loss: 0.6906 Acc: 0.7449\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6906) 모델 저장.\n",
      "Epoch 13/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.7592 Acc: 0.6719\n",
      "  [Batch 2533/2533] Train Loss: 0.7556 Acc: 0.6562\n",
      "Train Loss: 0.7450 Acc: 0.7203\n",
      "Val Loss: 0.6948 Acc: 0.7438\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 14/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.9494 Acc: 0.6719\n",
      "  [Batch 2533/2533] Train Loss: 0.5061 Acc: 0.8594\n",
      "Train Loss: 0.7339 Acc: 0.7248\n",
      "Val Loss: 0.6822 Acc: 0.7453\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6822) 모델 저장.\n",
      "Epoch 15/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.9251 Acc: 0.7344\n",
      "  [Batch 2533/2533] Train Loss: 0.5274 Acc: 0.7969\n",
      "Train Loss: 0.7206 Acc: 0.7307\n",
      "Val Loss: 0.6767 Acc: 0.7498\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6767) 모델 저장.\n",
      "Epoch 16/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.4778 Acc: 0.8750\n",
      "  [Batch 2533/2533] Train Loss: 0.8485 Acc: 0.6719\n",
      "Train Loss: 0.7149 Acc: 0.7325\n",
      "Val Loss: 0.6753 Acc: 0.7504\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6753) 모델 저장.\n",
      "Epoch 17/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.5977 Acc: 0.7812\n",
      "  [Batch 2533/2533] Train Loss: 0.7988 Acc: 0.7031\n",
      "Train Loss: 0.7163 Acc: 0.7325\n",
      "Val Loss: 0.6755 Acc: 0.7509\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 18/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.6112 Acc: 0.7969\n",
      "  [Batch 2533/2533] Train Loss: 0.7408 Acc: 0.7812\n",
      "Train Loss: 0.7134 Acc: 0.7334\n",
      "Val Loss: 0.6738 Acc: 0.7508\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6738) 모델 저장.\n",
      "Epoch 19/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.7903 Acc: 0.6875\n",
      "  [Batch 2533/2533] Train Loss: 0.5999 Acc: 0.7812\n",
      "Train Loss: 0.7126 Acc: 0.7325\n",
      "Val Loss: 0.6735 Acc: 0.7507\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6735) 모델 저장.\n",
      "Epoch 20/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.6854 Acc: 0.7500\n",
      "  [Batch 2533/2533] Train Loss: 0.7722 Acc: 0.7188\n",
      "Train Loss: 0.7100 Acc: 0.7349\n",
      "Val Loss: 0.6738 Acc: 0.7510\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 21/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.7806 Acc: 0.7344\n",
      "  [Batch 2533/2533] Train Loss: 0.5837 Acc: 0.7188\n",
      "Train Loss: 0.7090 Acc: 0.7350\n",
      "Val Loss: 0.6735 Acc: 0.7514\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 22/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.5262 Acc: 0.8438\n",
      "  [Batch 2533/2533] Train Loss: 0.6500 Acc: 0.7812\n",
      "Train Loss: 0.7061 Acc: 0.7368\n",
      "Val Loss: 0.6729 Acc: 0.7509\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6729) 모델 저장.\n",
      "Epoch 23/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.8493 Acc: 0.6562\n",
      "  [Batch 2533/2533] Train Loss: 0.8234 Acc: 0.6875\n",
      "Train Loss: 0.7072 Acc: 0.7346\n",
      "Val Loss: 0.6731 Acc: 0.7510\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 24/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.5832 Acc: 0.7344\n",
      "  [Batch 2533/2533] Train Loss: 0.5280 Acc: 0.7969\n",
      "Train Loss: 0.7072 Acc: 0.7354\n",
      "Val Loss: 0.6738 Acc: 0.7503\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 25/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.7767 Acc: 0.7188\n",
      "  [Batch 2533/2533] Train Loss: 0.6304 Acc: 0.7344\n",
      "Train Loss: 0.7055 Acc: 0.7366\n",
      "Val Loss: 0.6720 Acc: 0.7514\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6720) 모델 저장.\n",
      "Epoch 26/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.6950 Acc: 0.7344\n",
      "  [Batch 2533/2533] Train Loss: 0.5987 Acc: 0.7656\n",
      "Train Loss: 0.7038 Acc: 0.7379\n",
      "Val Loss: 0.6711 Acc: 0.7520\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6711) 모델 저장.\n",
      "Epoch 27/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.6781 Acc: 0.7344\n",
      "  [Batch 2533/2533] Train Loss: 0.6167 Acc: 0.7031\n",
      "Train Loss: 0.7068 Acc: 0.7360\n",
      "Val Loss: 0.6712 Acc: 0.7523\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 28/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.6743 Acc: 0.7656\n",
      "  [Batch 2533/2533] Train Loss: 0.6443 Acc: 0.7656\n",
      "Train Loss: 0.7068 Acc: 0.7358\n",
      "Val Loss: 0.6719 Acc: 0.7520\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 29/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.8404 Acc: 0.7031\n",
      "  [Batch 2533/2533] Train Loss: 0.9110 Acc: 0.6719\n",
      "Train Loss: 0.7060 Acc: 0.7366\n",
      "Val Loss: 0.6730 Acc: 0.7509\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 30/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.7205 Acc: 0.7031\n",
      "  [Batch 2533/2533] Train Loss: 0.8205 Acc: 0.7188\n",
      "Train Loss: 0.7041 Acc: 0.7370\n",
      "Val Loss: 0.6713 Acc: 0.7517\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 31/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.7232 Acc: 0.7500\n",
      "  [Batch 2533/2533] Train Loss: 0.8946 Acc: 0.6719\n",
      "Train Loss: 0.7026 Acc: 0.7371\n",
      "Val Loss: 0.6718 Acc: 0.7517\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/10\n",
      "Epoch 32/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.6064 Acc: 0.7188\n",
      "  [Batch 2533/2533] Train Loss: 0.7225 Acc: 0.7500\n",
      "Train Loss: 0.7056 Acc: 0.7357\n",
      "Val Loss: 0.6718 Acc: 0.7520\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/10\n",
      "Epoch 33/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.9466 Acc: 0.6719\n",
      "  [Batch 2533/2533] Train Loss: 0.5228 Acc: 0.8594\n",
      "Train Loss: 0.7038 Acc: 0.7382\n",
      "Val Loss: 0.6710 Acc: 0.7522\n",
      "\n",
      "  -> Val Loss 개선됨! (0.6710) 모델 저장.\n",
      "Epoch 34/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.9648 Acc: 0.6562\n",
      "  [Batch 2533/2533] Train Loss: 0.6909 Acc: 0.7656\n",
      "Train Loss: 0.7055 Acc: 0.7367\n",
      "Val Loss: 0.6721 Acc: 0.7527\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 35/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.7349 Acc: 0.6875\n",
      "  [Batch 2533/2533] Train Loss: 0.7642 Acc: 0.7656\n",
      "Train Loss: 0.7054 Acc: 0.7358\n",
      "Val Loss: 0.6732 Acc: 0.7520\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 36/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.8440 Acc: 0.6562\n",
      "  [Batch 2533/2533] Train Loss: 0.8206 Acc: 0.6875\n",
      "Train Loss: 0.7051 Acc: 0.7368\n",
      "Val Loss: 0.6712 Acc: 0.7518\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 37/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.7840 Acc: 0.7344\n",
      "  [Batch 2533/2533] Train Loss: 0.5393 Acc: 0.7969\n",
      "Train Loss: 0.7043 Acc: 0.7368\n",
      "Val Loss: 0.6719 Acc: 0.7514\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 38/100\n",
      "----------\n",
      "  [Batch 20/2533] Train Loss: 0.7042 Acc: 0.7344\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.image_dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    sampling_percent = 50\n",
    "    DATA_DIR = Path(f\"./datasets/KECV_{sampling_percent}_percent_FaceCrop\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'             #철원\n",
    "    #MODEL_NAME = 'resnet50' \n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'shufflenet_v2'       #철원\n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   #규진님\n",
    "    #MODEL_NAME = 'squeezenet'          #승희님\n",
    "    #MODEL_NAME = 'emotionnet'           # 감정 인식 전용 모델\n",
    "    MODEL_NAME = 'emonet'               # 경량화된 감정 인식 모델\n",
    "\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "    train_transform = None\n",
    "    val_transform = None\n",
    "    \n",
    "    if MODEL_NAME == 'emotionnet':\n",
    "        # 48x48 크기, 흑백(Grayscale), 정규화\n",
    "        # RandomResizedCrop + TrivialAugmentWide (강력한 데이터 증강 방법)\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((48, 48)),\n",
    "            # 원본 이미지의 80% ~ 100% 사이를 무작위로 잘라 48x48 크기로 만듦\n",
    "            transforms.RandomResizedCrop(size=48, scale=(0.8, 1.0)),\n",
    "            # 잘라낸 이미지에 최적의 증강 정책을 자동으로 적용\n",
    "            transforms.TrivialAugmentWide(),\n",
    "            # 흑백으로 변환\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지 정규화\n",
    "        ])\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지는 채널이 1개\n",
    "        ])\n",
    "\n",
    "    elif MODEL_NAME == 'emonet':\n",
    "        # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((256, 256)),\n",
    "            transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "            transforms.TrivialAugmentWide(), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # 증강이 없는 검증/테스트용 Transform 정의\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    else:\n",
    "        # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((224, 224)),\n",
    "            transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "            transforms.TrivialAugmentWide(), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        # 증강이 없는 검증/테스트용 Transform 정의\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    START_EPOCH = 0\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1)   # 7 에폭마다 학습률을 0.1배로 감소\n",
    "\n",
    "    CHECKPOINT_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained.pth'\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"체크포인트를 불러옵니다...\")\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"체크포인트(모델 가중치) 로드 완료! 0 에폭부터 훈련을 시작합니다.\")\n",
    "    else:\n",
    "        print(\"체크포인트가 존재하지 않습니다. 처음부터 훈련을 시작합니다.\")\n",
    "    \n",
    "    #model = torch.compile(model)   # Windows 환경에서 에러 발생\n",
    "    #print(\"모델 컴파일 완료!\")\n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS,\n",
    "                                start_epoch=START_EPOCH,\n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    torch.save(trained_model.state_dict(), f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained.pth')\n",
    "    print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.image_dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    sampling_percent = 100\n",
    "    DATA_DIR = Path(f\"./datasets/KECV_{sampling_percent}_percent_FaceCrop\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'             #철원\n",
    "    #MODEL_NAME = 'resnet50' \n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'shufflenet_v2'       #철원\n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   #규진님\n",
    "    #MODEL_NAME = 'squeezenet'          #승희님\n",
    "    #MODEL_NAME = 'emotionnet'           # 감정 인식 전용 모델\n",
    "    MODEL_NAME = 'emonet'               # 경량화된 감정 인식 모델\n",
    "\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "    train_transform = None\n",
    "    val_transform = None\n",
    "    \n",
    "    if MODEL_NAME == 'emotionnet':\n",
    "        # 48x48 크기, 흑백(Grayscale), 정규화\n",
    "        # RandomResizedCrop + TrivialAugmentWide (강력한 데이터 증강 방법)\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((48, 48)),\n",
    "            # 원본 이미지의 80% ~ 100% 사이를 무작위로 잘라 48x48 크기로 만듦\n",
    "            transforms.RandomResizedCrop(size=48, scale=(0.8, 1.0)),\n",
    "            # 잘라낸 이미지에 최적의 증강 정책을 자동으로 적용\n",
    "            transforms.TrivialAugmentWide(),\n",
    "            # 흑백으로 변환\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지 정규화\n",
    "        ])\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지는 채널이 1개\n",
    "        ])\n",
    "\n",
    "    elif MODEL_NAME == 'emonet':\n",
    "        # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((256, 256)),\n",
    "            transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "            transforms.TrivialAugmentWide(), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # 증강이 없는 검증/테스트용 Transform 정의\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    else:\n",
    "        # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "        train_transform = transforms.Compose([\n",
    "            #transforms.Resize((224, 224)),\n",
    "            transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "            transforms.TrivialAugmentWide(), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        # 증강이 없는 검증/테스트용 Transform 정의\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    START_EPOCH = 0\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1)   # 7 에폭마다 학습률을 0.1배로 감소\n",
    "\n",
    "    CHECKPOINT_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained.pth'\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"체크포인트를 불러옵니다...\")\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"체크포인트(모델 가중치) 로드 완료! 0 에폭부터 훈련을 시작합니다.\")\n",
    "    else:\n",
    "        print(\"체크포인트가 존재하지 않습니다. 처음부터 훈련을 시작합니다.\")\n",
    "    \n",
    "    #model = torch.compile(model)   # Windows 환경에서 에러 발생\n",
    "    #print(\"모델 컴파일 완료!\")\n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS,\n",
    "                                start_epoch=START_EPOCH,\n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    torch.save(trained_model.state_dict(), f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained.pth')\n",
    "    print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 양을 늘려도 성능이 특정 수준에서 다시 정체된다면, 파인튜닝 세분화를 적용하여 모델의 학습 효율을 극대화\n",
    "# 새로 학습시킬 파라미터와 미세 조정할 파라미터를 분리\n",
    "new_classifier_params = model.emo_fc_3.parameters()\n",
    "pretrained_params = [p for name, p in model.named_parameters() if 'emo_fc_3' not in name]\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': pretrained_params, 'lr': LEARNING_RATE * 0.1}, # 기존 부분은 10분의 1로 미세 조정\n",
    "    {'params': new_classifier_params, 'lr': LEARNING_RATE}      # 새 부분은 원래 학습률로 학습\n",
    "], weight_decay=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feellog-project (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
