{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-22 22:13:37,542] A new study created in memory with name: no-name-a0517188-e336-4e92-b713-4b21da67ee61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 2786\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "사전 훈련된 EmoNet 가중치를 불러옵니다 (Fine-tuning)...\n",
      "'emonet' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "체크포인트를 불러옵니다...\n",
      "체크포인트(모델 가중치) 로드 완료!\n",
      "Epoch 1/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 8.5035 Acc: 0.2188\n",
      "  [Batch 43/43] Train Loss: 10.6351 Acc: 0.1406\n",
      "Train Loss: 10.1729 Acc: 0.1472\n",
      "Val Loss: 11.8670 Acc: 0.1656 Macro-F1: 0.1026\n",
      "  -> Val Loss 개선됨! (11.8670) 모델 저장.\n",
      "Epoch 2/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 9.2195 Acc: 0.2031\n",
      "  [Batch 43/43] Train Loss: 9.9010 Acc: 0.1719\n",
      "Train Loss: 9.8634 Acc: 0.1595\n",
      "Val Loss: 11.4643 Acc: 0.1694 Macro-F1: 0.1048\n",
      "  -> Val Loss 개선됨! (11.4643) 모델 저장.\n",
      "Epoch 3/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 8.9140 Acc: 0.1875\n",
      "  [Batch 43/43] Train Loss: 9.5463 Acc: 0.1875\n",
      "Train Loss: 9.4895 Acc: 0.1675\n",
      "Val Loss: 11.0996 Acc: 0.1757 Macro-F1: 0.1073\n",
      "  -> Val Loss 개선됨! (11.0996) 모델 저장.\n",
      "Epoch 4/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 8.9668 Acc: 0.1719\n",
      "  [Batch 43/43] Train Loss: 8.5137 Acc: 0.1719\n",
      "Train Loss: 9.4865 Acc: 0.1672\n",
      "Val Loss: 11.2122 Acc: 0.1694 Macro-F1: 0.1055\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 5/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 8.2036 Acc: 0.2344\n",
      "  [Batch 43/43] Train Loss: 9.8324 Acc: 0.1094\n",
      "Train Loss: 9.2201 Acc: 0.1639\n",
      "Val Loss: 11.0852 Acc: 0.1782 Macro-F1: 0.1099\n",
      "  -> Val Loss 개선됨! (11.0852) 모델 저장.\n",
      "--------------------------------------------------\n",
      "Training complete in 2m 36s\n",
      "Saved Epoch: 5\n",
      "--------------------------------------------------\n",
      "Saved Train Loss: 9.2201\n",
      "Saved Train Acc: 0.1639\n",
      "Saved Val Loss: 11.0852\n",
      "Saved Val Acc: 0.1782\n",
      "--------------------------------------------------\n",
      "Best Train Loss: 9.2201\n",
      "Best Train Acc: 0.1675\n",
      "Best Val Loss: 11.0852\n",
      "Best Val Acc: 0.1782\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/22 22:16:14 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/22 22:16:14 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/08/22 22:16:19 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/08/22 22:16:19 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "2025/08/22 22:16:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련된 모델 가중치가 저장되었습니다.\n",
      "상세 분석 결과가 저장되었습니다: ./infrastructure/models/weights/checkpoints/emonet_1_percent_trained_metrics.json\n",
      "🏃 View run resilient-gnat-486 at: http://127.0.0.1:5000/#/experiments/957529077469630842/runs/410526dbeda440dfa1f6d6dfe124cfe1\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/957529077469630842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-22 22:16:22,297] Trial 0 finished with value: 11.0852 and parameters: {'lr': 2.6201610636791263e-05, 'optimizer': 'Adam', 'scheduler': 'StepLR'}. Best is trial 0 with value: 11.0852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 2786\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "사전 훈련된 EmoNet 가중치를 불러옵니다 (Fine-tuning)...\n",
      "'emonet' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "체크포인트를 불러옵니다...\n",
      "체크포인트(모델 가중치) 로드 완료!\n",
      "Epoch 1/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 6.9688 Acc: 0.2969\n",
      "  [Batch 43/43] Train Loss: 4.9523 Acc: 0.3750\n",
      "Train Loss: 7.6688 Acc: 0.2089\n",
      "Val Loss: 7.3467 Acc: 0.2635 Macro-F1: 0.1653\n",
      "  -> Val Loss 개선됨! (7.3467) 모델 저장.\n",
      "Epoch 2/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 5.8205 Acc: 0.3594\n",
      "  [Batch 43/43] Train Loss: 5.3711 Acc: 0.3438\n",
      "Train Loss: 5.4944 Acc: 0.3023\n",
      "Val Loss: 5.4809 Acc: 0.3513 Macro-F1: 0.2378\n",
      "  -> Val Loss 개선됨! (5.4809) 모델 저장.\n",
      "Epoch 3/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 3.9655 Acc: 0.3906\n",
      "  [Batch 43/43] Train Loss: 3.9158 Acc: 0.3594\n",
      "Train Loss: 4.2752 Acc: 0.3801\n",
      "Val Loss: 4.4828 Acc: 0.4266 Macro-F1: 0.2968\n",
      "  -> Val Loss 개선됨! (4.4828) 모델 저장.\n",
      "Epoch 4/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 3.5004 Acc: 0.3281\n",
      "  [Batch 43/43] Train Loss: 3.5570 Acc: 0.5469\n",
      "Train Loss: 3.8237 Acc: 0.4172\n",
      "Val Loss: 4.1687 Acc: 0.4605 Macro-F1: 0.3213\n",
      "  -> Val Loss 개선됨! (4.1687) 모델 저장.\n",
      "Epoch 5/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 3.4641 Acc: 0.5312\n",
      "  [Batch 43/43] Train Loss: 3.5020 Acc: 0.4688\n",
      "Train Loss: 3.5686 Acc: 0.4328\n",
      "Val Loss: 4.1027 Acc: 0.4605 Macro-F1: 0.3255\n",
      "  -> Val Loss 개선됨! (4.1027) 모델 저장.\n",
      "--------------------------------------------------\n",
      "Training complete in 2m 45s\n",
      "Saved Epoch: 5\n",
      "--------------------------------------------------\n",
      "Saved Train Loss: 3.5686\n",
      "Saved Train Acc: 0.4328\n",
      "Saved Val Loss: 4.1027\n",
      "Saved Val Acc: 0.4605\n",
      "--------------------------------------------------\n",
      "Best Train Loss: 3.5686\n",
      "Best Train Acc: 0.4328\n",
      "Best Val Loss: 4.1027\n",
      "Best Val Acc: 0.4605\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/22 22:19:07 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/22 22:19:07 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/08/22 22:19:12 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/08/22 22:19:12 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "2025/08/22 22:19:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련된 모델 가중치가 저장되었습니다.\n",
      "상세 분석 결과가 저장되었습니다: ./infrastructure/models/weights/checkpoints/emonet_1_percent_trained_metrics.json\n",
      "🏃 View run luminous-sheep-346 at: http://127.0.0.1:5000/#/experiments/957529077469630842/runs/e5721bae3a38407ea6e350e8d92e4f3a\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/957529077469630842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-22 22:19:15,053] Trial 1 finished with value: 4.1027 and parameters: {'lr': 0.000265968978612794, 'optimizer': 'Adam', 'scheduler': 'CosineAnnealingLR'}. Best is trial 1 with value: 4.1027.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 2786\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "사전 훈련된 EmoNet 가중치를 불러옵니다 (Fine-tuning)...\n",
      "'emonet' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "체크포인트를 불러옵니다...\n",
      "체크포인트(모델 가중치) 로드 완료!\n",
      "Epoch 1/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 2.9919 Acc: 0.4688\n",
      "  [Batch 43/43] Train Loss: 3.6080 Acc: 0.5000\n",
      "Train Loss: 3.5454 Acc: 0.4480\n",
      "Val Loss: 3.9952 Acc: 0.4768 Macro-F1: 0.3376\n",
      "  -> Val Loss 개선됨! (3.9952) 모델 저장.\n",
      "Epoch 2/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 3.6000 Acc: 0.5312\n",
      "  [Batch 43/43] Train Loss: 4.0448 Acc: 0.4062\n",
      "Train Loss: 3.4238 Acc: 0.4658\n",
      "Val Loss: 3.8423 Acc: 0.4893 Macro-F1: 0.3473\n",
      "  -> Val Loss 개선됨! (3.8423) 모델 저장.\n",
      "Epoch 3/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 3.7986 Acc: 0.4062\n",
      "  [Batch 43/43] Train Loss: 3.9550 Acc: 0.4062\n",
      "Train Loss: 3.2927 Acc: 0.4600\n",
      "Val Loss: 3.7580 Acc: 0.4956 Macro-F1: 0.3501\n",
      "  -> Val Loss 개선됨! (3.7580) 모델 저장.\n",
      "Epoch 4/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 3.7037 Acc: 0.3750\n",
      "  [Batch 43/43] Train Loss: 2.9614 Acc: 0.5156\n",
      "Train Loss: 3.2698 Acc: 0.4560\n",
      "Val Loss: 3.6389 Acc: 0.4931 Macro-F1: 0.3501\n",
      "  -> Val Loss 개선됨! (3.6389) 모델 저장.\n",
      "Epoch 5/5\n",
      "----------\n",
      "  [Batch 20/43] Train Loss: 3.6866 Acc: 0.3906\n",
      "  [Batch 43/43] Train Loss: 3.3578 Acc: 0.4375\n",
      "Train Loss: 3.3679 Acc: 0.4488\n",
      "Val Loss: 3.7157 Acc: 0.4981 Macro-F1: 0.3525\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "--------------------------------------------------\n",
      "Training complete in 3m 23s\n",
      "Saved Epoch: 4\n",
      "--------------------------------------------------\n",
      "Saved Train Loss: 3.2698\n",
      "Saved Train Acc: 0.4560\n",
      "Saved Val Loss: 3.6389\n",
      "Saved Val Acc: 0.4931\n",
      "--------------------------------------------------\n",
      "Best Train Loss: 3.2698\n",
      "Best Train Acc: 0.4658\n",
      "Best Val Loss: 3.6389\n",
      "Best Val Acc: 0.4981\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/22 22:22:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/22 22:22:38 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/08/22 22:22:43 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.1+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.7.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/08/22 22:22:43 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "2025/08/22 22:22:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련된 모델 가중치가 저장되었습니다.\n",
      "상세 분석 결과가 저장되었습니다: ./infrastructure/models/weights/checkpoints/emonet_1_percent_trained_metrics.json\n",
      "🏃 View run gifted-frog-180 at: http://127.0.0.1:5000/#/experiments/957529077469630842/runs/7b0422233d7b4f91b76a35ef341e495b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/957529077469630842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-22 22:22:46,016] Trial 2 finished with value: 3.6389 and parameters: {'lr': 3.110968454786276e-05, 'optimizer': 'AdamW', 'scheduler': 'CosineAnnealingLR'}. Best is trial 2 with value: 3.6389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization finished.\n",
      "Best trial:\n",
      "  Value (Best Val Loss): 3.6389\n",
      "  Params: \n",
      "    lr: 3.110968454786276e-05\n",
      "    optimizer: AdamW\n",
      "    scheduler: CosineAnnealingLR\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.image_dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "import optuna\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    \"\"\"Optuna가 최적화할 목표 함수\"\"\"\n",
    "    \n",
    "    # MLflow 실험 시작\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        # --- 1. 하이퍼파라미터 제안 ---\n",
    "        # Optuna가 이 범위 내에서 최적의 값을 찾아 제안합니다.\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\"])\n",
    "        scheduler_name = trial.suggest_categorical(\"scheduler\", [\"StepLR\", \"CosineAnnealingLR\", \"ReduceLROnPlateau\"])\n",
    "        \n",
    "        # MLflow에 제안된 하이퍼파라미터 기록\n",
    "        mlflow.log_params(trial.params)\n",
    "        \n",
    "        # --- 2. 데이터 및 모델 준비 ---\n",
    "        \n",
    "        # CUDA 성능 플래그 최적화\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "        DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "        #MODEL_NAME = 'resnet18'            \n",
    "        #MODEL_NAME = 'resnet50' \n",
    "        #MODEL_NAME = 'mobilenet_v3_small'  \n",
    "        #MODEL_NAME = 'shufflenet_v2'       \n",
    "        #MODEL_NAME = 'efficientnet_v2_s'   \n",
    "        #MODEL_NAME = 'squeezenet'          \n",
    "        #MODEL_NAME = 'emotionnet'  # 감정 인식 전용 모델\n",
    "        MODEL_NAME = 'emonet'       # 경량화된 감정 인식 모델\n",
    "        \n",
    "        sampling_percent = \"1\"\n",
    "\n",
    "        MISCLASSIFIED_DIR = Path(f\"./datasets/misclassified_images/{MODEL_NAME}_{sampling_percent}\") # 오답 이미지를 저장할 폴더 경로 정의\n",
    "        DATA_DIR = Path(f\"./datasets/KECV_{sampling_percent}\")\n",
    "        # 대쉬보드에 사용된 데이터셋을 표시, 상세화면의 Tags와 Parameters에 기록\n",
    "        mlflow.log_param(\"dataset_path\", str(DATA_DIR))\n",
    "        mlflow.set_tag(\"dataset_description\", \"테스트용 1% 데이터\")\n",
    "        NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "        BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "        LEARNING_RATE = 0.001\n",
    "        NUM_EPOCHS = 5\n",
    "        EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "        STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "        train_transform = None\n",
    "        val_transform = None\n",
    "        #scheduler_name = 'CosineAnnealingLR'  # CosineAnnealingLR로 스케줄러 설정\n",
    "        \n",
    "        if MODEL_NAME == 'emotionnet':\n",
    "            # 48x48 크기, 흑백(Grayscale), 정규화\n",
    "            # RandomResizedCrop + TrivialAugmentWide (강력한 데이터 증강 방법)\n",
    "            train_transform = transforms.Compose([\n",
    "                #transforms.Resize((48, 48)),\n",
    "                # 원본 이미지의 80% ~ 100% 사이를 무작위로 잘라 48x48 크기로 만듦\n",
    "                transforms.RandomResizedCrop(size=48, scale=(0.8, 1.0)),\n",
    "                # 잘라낸 이미지에 최적의 증강 정책을 자동으로 적용\n",
    "                transforms.TrivialAugmentWide(),\n",
    "                # 흑백으로 변환\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지 정규화\n",
    "            ])\n",
    "            val_transform = transforms.Compose([\n",
    "                transforms.Resize((48, 48)),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5]) # 흑백 이미지는 채널이 1개\n",
    "            ])\n",
    "\n",
    "        elif MODEL_NAME == 'emonet':\n",
    "            # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "            train_transform = transforms.Compose([\n",
    "                #transforms.Resize((256, 256)),\n",
    "                transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "                transforms.TrivialAugmentWide(), \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            # 증강이 없는 검증/테스트용 Transform 정의\n",
    "            val_transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "        else:\n",
    "            # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "            train_transform = transforms.Compose([\n",
    "                #transforms.Resize((224, 224)),\n",
    "                transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "                transforms.TrivialAugmentWide(), \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "            # 증강이 없는 검증/테스트용 Transform 정의\n",
    "            val_transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "            \n",
    "        # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "        train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "        val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "        # DataLoader I/O 튜닝\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True,\n",
    "            # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "            num_workers=min(8, os.cpu_count()), \n",
    "            pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "            persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "            prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "            drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=False,\n",
    "            num_workers=min(8, os.cpu_count()),\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2\n",
    "        )\n",
    "\n",
    "        NUM_CLASSES = len(train_dataset.classes)\n",
    "        \n",
    "        print(\"데이터 준비 완료!\")\n",
    "        print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "        print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "        # 모델, 손실 함수, 옵티마이저 준비\n",
    "        model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "        if scheduler_name == 'StepLR':\n",
    "            scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "        elif scheduler_name == 'ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "        else:\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "        \n",
    "        CHECKPOINT_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_trained.pth'\n",
    "        if os.path.exists(CHECKPOINT_PATH):\n",
    "            print(\"체크포인트를 불러옵니다...\")\n",
    "            checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "            model.load_state_dict(checkpoint)\n",
    "            print(\"체크포인트(모델 가중치) 로드 완료!\")\n",
    "        else:\n",
    "            print(\"체크포인트가 존재하지 않습니다. 처음부터 훈련을 시작합니다.\")\n",
    "            \n",
    "        # --- 3. 모델 훈련 ---\n",
    "        # trainer가 이제 최고 점수(best_metrics)만 반환하도록 수정했다고 가정\n",
    "        #best_metrics = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, DEVICE, num_epochs=NUM_EPOCHS)\n",
    "        trained_model, saved_metrics = train_model(model, \n",
    "                                    train_loader, \n",
    "                                    val_loader, \n",
    "                                    criterion, \n",
    "                                    optimizer, \n",
    "                                    scheduler,\n",
    "                                    DEVICE, \n",
    "                                    num_epochs=NUM_EPOCHS,\n",
    "                                    patience=EARLY_STOPPING_PATIENCE,\n",
    "                                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                                    misclassified_dir=MISCLASSIFIED_DIR\n",
    "                                    )\n",
    "        \n",
    "        # --- 4. MLflow에 결과 기록 ---\n",
    "        # 최고 검증 손실과 정확도, F1 Score 등을 기록\n",
    "        mlflow.log_metrics({\n",
    "            \"best_train_loss\": float(saved_metrics['train_loss']),\n",
    "            \"best_train_accuracy\": float(saved_metrics['train_accuracy']),\n",
    "            \"best_val_loss\": float(saved_metrics['val_loss']),\n",
    "            \"best_val_accuracy\": float(saved_metrics['val_accuracy']),\n",
    "            \"best_macro_f1\": float(saved_metrics['macro_f1_score']),\n",
    "        })\n",
    "        \n",
    "        # MLflow에 최고 성능 모델 저장\n",
    "        # mlflow.pytorch.log_model(model, \"model\")\n",
    "        \n",
    "        # 훈련된 모델 저장 (옵션)\n",
    "        CHECKPOINT_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_trained.pth'\n",
    "        torch.save(trained_model.state_dict(), CHECKPOINT_PATH)\n",
    "        \n",
    "        # MLflow에 모델 저장\n",
    "        #input_example = np.random.rand(1, 3, 256, 256)\n",
    "        #mlflow.pytorch.log_model(trained_model, name=\"EMONET_1P\", input_example=input_example, pip_requirements=\"pip_requirements.txt\")\n",
    "        mlflow.pytorch.log_model(trained_model, name=\"EMONET_1P\", pip_requirements=\"pip_requirements.txt\")\n",
    "        print(\"훈련된 모델 가중치가 저장되었습니다.\")\n",
    "\n",
    "        # 최고 성능 시점의 상세 분석 결과를 JSON으로 저장\n",
    "        METRICS_PATH = f'./infrastructure/models/weights/checkpoints/{MODEL_NAME}_{sampling_percent}_percent_trained_metrics.json'\n",
    "        with open(METRICS_PATH, 'a', encoding='utf-8') as f:\n",
    "            json.dump(saved_metrics, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"상세 분석 결과가 저장되었습니다: {METRICS_PATH}\")\n",
    "                \n",
    "        # --- 5. Optuna에 목표값 반환 ---\n",
    "        # 우리는 검증 손실(val_loss)을 최소화하는 것을 목표로 함\n",
    "        return float(saved_metrics['val_loss'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #코드 실행 전 아래 명령어를 터미널에서 실행\n",
    "    # mlflow ui\n",
    "    # 위의 명령어만 먼저해보고 에러 MlflowException: When an mlflow-artifacts URI was supplied, the tracking URI must be a valid http or https URI 가 발생하면 아래 명령어 실행.\n",
    "    # mlflow server --host 127.0.0.1 --port 5001\n",
    "    \n",
    "    # MLflow 추적 서버 URI 설정\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")\n",
    "    \n",
    "    # MLflow 실험 이름 설정, 대쉬보드에서 훈련을 구분하여 보기위해 사용.\n",
    "    mlflow.set_experiment(\"Emotion Classification Tuning\")\n",
    "\n",
    "    # Optuna Study 생성: 'minimize' 방향으로 objective 함수를 최적화\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    \n",
    "    # 100번의 다른 하이퍼파라미터 조합으로 실험(Trial) 실행\n",
    "    study.optimize(objective, n_trials=5)\n",
    "    \n",
    "    print(\"Hyperparameter optimization finished.\")\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    \n",
    "    print(f\"  Value (Best Val Loss): {trial.value}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 양을 늘려도 성능이 특정 수준에서 다시 정체된다면, 파인튜닝 세분화를 적용하여 모델의 학습 효율을 극대화\n",
    "# 새로 학습시킬 파라미터와 미세 조정할 파라미터를 분리\n",
    "new_classifier_params = model.emo_fc_3.parameters()\n",
    "pretrained_params = [p for name, p in model.named_parameters() if 'emo_fc_3' not in name]\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': pretrained_params, 'lr': LEARNING_RATE * 0.1}, # 기존 부분은 10분의 1로 미세 조정\n",
    "    {'params': new_classifier_params, 'lr': LEARNING_RATE}      # 새 부분은 원래 학습률로 학습\n",
    "], weight_decay=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feellog-project (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
