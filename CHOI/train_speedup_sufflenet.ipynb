{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 32407\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'shufflenet_v2' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.8385 Acc: 0.2969\n",
      "  [Batch 506/506] Train Loss: 1.0409 Acc: 0.5781\n",
      "Train Loss: 1.2677 Acc: 0.5173\n",
      "Val Loss: 1.1460 Acc: 0.5601\n",
      "\n",
      "  -> Val Loss 개선됨! (1.1460) 모델 저장.\n",
      "Epoch 2/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.1664 Acc: 0.5781\n",
      "  [Batch 506/506] Train Loss: 0.9024 Acc: 0.6406\n",
      "Train Loss: 1.0626 Acc: 0.6006\n",
      "Val Loss: 0.9709 Acc: 0.6317\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9709) 모델 저장.\n",
      "Epoch 3/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.0577 Acc: 0.6250\n",
      "  [Batch 506/506] Train Loss: 1.0164 Acc: 0.6094\n",
      "Train Loss: 1.0112 Acc: 0.6192\n",
      "Val Loss: 0.9610 Acc: 0.6375\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9610) 모델 저장.\n",
      "Epoch 4/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.0106 Acc: 0.6250\n",
      "  [Batch 506/506] Train Loss: 0.9940 Acc: 0.5938\n",
      "Train Loss: 0.9746 Acc: 0.6354\n",
      "Val Loss: 0.9475 Acc: 0.6449\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9475) 모델 저장.\n",
      "Epoch 5/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.0973 Acc: 0.6719\n",
      "  [Batch 506/506] Train Loss: 0.8671 Acc: 0.6406\n",
      "Train Loss: 0.9465 Acc: 0.6443\n",
      "Val Loss: 0.9137 Acc: 0.6612\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9137) 모델 저장.\n",
      "Epoch 6/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.9683 Acc: 0.6406\n",
      "  [Batch 506/506] Train Loss: 0.9010 Acc: 0.7031\n",
      "Train Loss: 0.9221 Acc: 0.6579\n",
      "Val Loss: 0.9809 Acc: 0.6271\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 7/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.2099 Acc: 0.5156\n",
      "  [Batch 506/506] Train Loss: 1.0485 Acc: 0.6094\n",
      "Train Loss: 0.9048 Acc: 0.6568\n",
      "Val Loss: 0.9576 Acc: 0.6454\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 8/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.9246 Acc: 0.6250\n",
      "  [Batch 506/506] Train Loss: 0.7182 Acc: 0.7188\n",
      "Train Loss: 0.8024 Acc: 0.6986\n",
      "Val Loss: 0.8466 Acc: 0.6885\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8466) 모델 저장.\n",
      "Epoch 9/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.8657 Acc: 0.7656\n",
      "  [Batch 506/506] Train Loss: 0.6760 Acc: 0.7500\n",
      "Train Loss: 0.7549 Acc: 0.7177\n",
      "Val Loss: 0.8455 Acc: 0.6902\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8455) 모델 저장.\n",
      "Epoch 10/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6905 Acc: 0.7344\n",
      "  [Batch 506/506] Train Loss: 0.7625 Acc: 0.7188\n",
      "Train Loss: 0.7351 Acc: 0.7228\n",
      "Val Loss: 0.8447 Acc: 0.6914\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8447) 모델 저장.\n",
      "Epoch 11/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.8107 Acc: 0.6719\n",
      "  [Batch 506/506] Train Loss: 0.6759 Acc: 0.7344\n",
      "Train Loss: 0.7196 Acc: 0.7311\n",
      "Val Loss: 0.8549 Acc: 0.6920\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 12/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 1.0603 Acc: 0.5625\n",
      "  [Batch 506/506] Train Loss: 0.6001 Acc: 0.7812\n",
      "Train Loss: 0.6988 Acc: 0.7371\n",
      "Val Loss: 0.8628 Acc: 0.6880\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 13/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6107 Acc: 0.7969\n",
      "  [Batch 506/506] Train Loss: 0.7639 Acc: 0.7031\n",
      "Train Loss: 0.6774 Acc: 0.7470\n",
      "Val Loss: 0.8660 Acc: 0.6943\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 14/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.5396 Acc: 0.8281\n",
      "  [Batch 506/506] Train Loss: 0.7131 Acc: 0.7031\n",
      "Train Loss: 0.6672 Acc: 0.7502\n",
      "Val Loss: 0.8734 Acc: 0.6914\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 15/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6367 Acc: 0.7344\n",
      "  [Batch 506/506] Train Loss: 0.5439 Acc: 0.8281\n",
      "Train Loss: 0.6468 Acc: 0.7577\n",
      "Val Loss: 0.8692 Acc: 0.6911\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/10\n",
      "Epoch 16/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6346 Acc: 0.7344\n",
      "  [Batch 506/506] Train Loss: 0.5873 Acc: 0.8125\n",
      "Train Loss: 0.6436 Acc: 0.7620\n",
      "Val Loss: 0.8701 Acc: 0.6928\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/10\n",
      "Epoch 17/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6850 Acc: 0.6875\n",
      "  [Batch 506/506] Train Loss: 0.4678 Acc: 0.8594\n",
      "Train Loss: 0.6364 Acc: 0.7604\n",
      "Val Loss: 0.8734 Acc: 0.6913\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 7/10\n",
      "Epoch 18/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6209 Acc: 0.7500\n",
      "  [Batch 506/506] Train Loss: 0.5174 Acc: 0.7969\n",
      "Train Loss: 0.6324 Acc: 0.7650\n",
      "Val Loss: 0.8791 Acc: 0.6904\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 8/10\n",
      "Epoch 19/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.5969 Acc: 0.7969\n",
      "  [Batch 506/506] Train Loss: 0.6002 Acc: 0.7812\n",
      "Train Loss: 0.6280 Acc: 0.7664\n",
      "Val Loss: 0.8792 Acc: 0.6918\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 9/10\n",
      "Epoch 20/100\n",
      "----------\n",
      "  [Batch 20/506] Train Loss: 0.6002 Acc: 0.7812\n",
      "  [Batch 506/506] Train Loss: 0.4270 Acc: 0.8125\n",
      "Train Loss: 0.6250 Acc: 0.7671\n",
      "Val Loss: 0.8792 Acc: 0.6908\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 10/10\n",
      "\n",
      "Early stopping! 10 에폭 동안 성능 개선이 없었습니다.\n",
      "--------------------------------------------------\n",
      "Training complete in 11m 15s\n",
      "Saved Epoch: 10\n",
      "--------------------------------------------------\n",
      "Saved Train Loss: 0.7351\n",
      "Saved Train Acc: 0.7228\n",
      "Saved Val Loss: 0.8447\n",
      "Saved Val Acc: 0.6914\n",
      "--------------------------------------------------\n",
      "Best Train Loss: 0.6280\n",
      "Best Train Acc: 0.7664\n",
      "Best Val Loss: 0.8447\n",
      "Best Val Acc: 0.6943\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer_speedup import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_10_percent_verified_processed\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'            #철원\n",
    "    #MODEL_NAME = 'resnet50'            #철원\n",
    "    MODEL_NAME = 'shufflenet_v2'        #철원\n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   #승현님\n",
    "    #MODEL_NAME = 'squeezenet'          #승현님\n",
    "    \n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "\n",
    "    \n",
    "    # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "    #train_transform = transforms.Compose([\n",
    "    #    transforms.Resize((224, 224)),\n",
    "    #    transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "    #    transforms.RandomRotation(15),           # -15도 ~ 15도 사이로 랜덤 회전\n",
    "    #    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 밝기, 대비, 채도 조절\n",
    "    #    transforms.ToTensor(),\n",
    "    #    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    #])\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        # TrivialAugmentWide 추가, 이미지에 다양한 변형(자르기, 색상 왜곡, 회전 등)을 알아서 최적의 강도로 적용, 과적합 방지.\n",
    "        transforms.TrivialAugmentWide(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 증강이 없는 검증/테스트용 Transform 정의\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1) # 7 에폭마다 학습률을 0.1배로 감소\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS, \n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe008d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 17975\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'shufflenet_v2' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.8227 Acc: 0.3906\n",
      "  [Batch 280/280] Train Loss: 1.1848 Acc: 0.5312\n",
      "Train Loss: 1.3459 Acc: 0.4915\n",
      "Val Loss: 1.1255 Acc: 0.5906\n",
      "\n",
      "  -> Val Loss 개선됨! (1.1255) 모델 저장.\n",
      "Epoch 2/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.3269 Acc: 0.5625\n",
      "  [Batch 280/280] Train Loss: 0.9187 Acc: 0.6875\n",
      "Train Loss: 1.0916 Acc: 0.5912\n",
      "Val Loss: 0.9945 Acc: 0.6301\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9945) 모델 저장.\n",
      "Epoch 3/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 1.0729 Acc: 0.5469\n",
      "  [Batch 280/280] Train Loss: 1.0266 Acc: 0.6094\n",
      "Train Loss: 1.0133 Acc: 0.6235\n",
      "Val Loss: 0.9752 Acc: 0.6423\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9752) 모델 저장.\n",
      "Epoch 4/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8237 Acc: 0.7344\n",
      "  [Batch 280/280] Train Loss: 0.8736 Acc: 0.6875\n",
      "Train Loss: 0.9673 Acc: 0.6395\n",
      "Val Loss: 0.9409 Acc: 0.6537\n",
      "\n",
      "  -> Val Loss 개선됨! (0.9409) 모델 저장.\n",
      "Epoch 5/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.9486 Acc: 0.6406\n",
      "  [Batch 280/280] Train Loss: 0.8956 Acc: 0.7031\n",
      "Train Loss: 0.9316 Acc: 0.6529\n",
      "Val Loss: 0.9838 Acc: 0.6323\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 6/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8459 Acc: 0.6406\n",
      "  [Batch 280/280] Train Loss: 0.8367 Acc: 0.6875\n",
      "Train Loss: 0.8963 Acc: 0.6630\n",
      "Val Loss: 0.9903 Acc: 0.6441\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 7/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7655 Acc: 0.7969\n",
      "  [Batch 280/280] Train Loss: 0.7550 Acc: 0.6719\n",
      "Train Loss: 0.8690 Acc: 0.6776\n",
      "Val Loss: 0.9831 Acc: 0.6428\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 8/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6338 Acc: 0.7969\n",
      "  [Batch 280/280] Train Loss: 0.6281 Acc: 0.7969\n",
      "Train Loss: 0.7578 Acc: 0.7199\n",
      "Val Loss: 0.8555 Acc: 0.6847\n",
      "\n",
      "  -> Val Loss 개선됨! (0.8555) 모델 저장.\n",
      "Epoch 9/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8119 Acc: 0.6719\n",
      "  [Batch 280/280] Train Loss: 0.7561 Acc: 0.6875\n",
      "Train Loss: 0.7026 Acc: 0.7401\n",
      "Val Loss: 0.8617 Acc: 0.6881\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 1/10\n",
      "Epoch 10/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.7365 Acc: 0.6875\n",
      "  [Batch 280/280] Train Loss: 0.6805 Acc: 0.7188\n",
      "Train Loss: 0.6649 Acc: 0.7531\n",
      "Val Loss: 0.8724 Acc: 0.6890\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 2/10\n",
      "Epoch 11/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4549 Acc: 0.8594\n",
      "  [Batch 280/280] Train Loss: 0.5585 Acc: 0.8281\n",
      "Train Loss: 0.6485 Acc: 0.7599\n",
      "Val Loss: 0.8763 Acc: 0.6911\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 3/10\n",
      "Epoch 12/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.6836 Acc: 0.7656\n",
      "  [Batch 280/280] Train Loss: 0.7003 Acc: 0.7656\n",
      "Train Loss: 0.6224 Acc: 0.7700\n",
      "Val Loss: 0.8804 Acc: 0.6940\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 4/10\n",
      "Epoch 13/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4089 Acc: 0.8438\n",
      "  [Batch 280/280] Train Loss: 0.5888 Acc: 0.7344\n",
      "Train Loss: 0.6062 Acc: 0.7738\n",
      "Val Loss: 0.9096 Acc: 0.6847\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 5/10\n",
      "Epoch 14/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5337 Acc: 0.7969\n",
      "  [Batch 280/280] Train Loss: 0.4910 Acc: 0.7969\n",
      "Train Loss: 0.5851 Acc: 0.7835\n",
      "Val Loss: 0.9211 Acc: 0.6881\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 6/10\n",
      "Epoch 15/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4797 Acc: 0.7812\n",
      "  [Batch 280/280] Train Loss: 0.5054 Acc: 0.7812\n",
      "Train Loss: 0.5674 Acc: 0.7964\n",
      "Val Loss: 0.9125 Acc: 0.6931\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 7/10\n",
      "Epoch 16/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.5403 Acc: 0.7656\n",
      "  [Batch 280/280] Train Loss: 0.6518 Acc: 0.7969\n",
      "Train Loss: 0.5589 Acc: 0.7943\n",
      "Val Loss: 0.9146 Acc: 0.6940\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 8/10\n",
      "Epoch 17/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.4354 Acc: 0.8281\n",
      "  [Batch 280/280] Train Loss: 0.6078 Acc: 0.7969\n",
      "Train Loss: 0.5546 Acc: 0.7937\n",
      "Val Loss: 0.9184 Acc: 0.6888\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 9/10\n",
      "Epoch 18/100\n",
      "----------\n",
      "  [Batch 20/280] Train Loss: 0.8118 Acc: 0.7500\n",
      "  [Batch 280/280] Train Loss: 0.5671 Acc: 0.7812\n",
      "Train Loss: 0.5554 Acc: 0.7961\n",
      "Val Loss: 0.9206 Acc: 0.6913\n",
      "\n",
      "  -> Val Loss 개선되지 않음. EarlyStopping Counter: 10/10\n",
      "\n",
      "Early stopping! 10 에폭 동안 성능 개선이 없었습니다.\n",
      "--------------------------------------------------\n",
      "Training complete in 10m 41s\n",
      "Saved Epoch: 8\n",
      "--------------------------------------------------\n",
      "Saved Train Loss: 0.7578\n",
      "Saved Train Acc: 0.7199\n",
      "Saved Val Loss: 0.8555\n",
      "Saved Val Acc: 0.6847\n",
      "--------------------------------------------------\n",
      "Best Train Loss: 0.5546\n",
      "Best Train Acc: 0.7964\n",
      "Best Val Loss: 0.8555\n",
      "Best Val Acc: 0.6940\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import os\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer_speedup import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CUDA 성능 플래그 최적화\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # TF32 텐서 코어 사용을 허용하여 Ampere 아키텍처 이상 GPU에서 연산 속도 향상\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_5_percent_verified_processed\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'            #철원\n",
    "    #MODEL_NAME = 'resnet50'            #철원\n",
    "    MODEL_NAME = 'shufflenet_v2'        #철원\n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'efficientnet_v2_s'   #승현님\n",
    "    #MODEL_NAME = 'squeezenet'          #승현님\n",
    "    \n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 64  # 배치 크기를 늘려 GPU 메모리 사용 최적화\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    STEPS_PER_EPOCH = None # 빠른 테스트를 위해 에폭당 배치 수를 제한하려면 숫자로 변경 (예: 100)\n",
    "\n",
    "    \n",
    "    # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "    #train_transform = transforms.Compose([\n",
    "    #    transforms.Resize((224, 224)),\n",
    "    #    transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "    #    transforms.RandomRotation(15),           # -15도 ~ 15도 사이로 랜덤 회전\n",
    "    #    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 밝기, 대비, 채도 조절\n",
    "    #    transforms.ToTensor(),\n",
    "    #    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    #])\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        # TrivialAugmentWide 추가, 이미지에 다양한 변형(자르기, 색상 왜곡, 회전 등)을 알아서 최적의 강도로 적용, 과적합 방지.\n",
    "        transforms.TrivialAugmentWide(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 증강이 없는 검증/테스트용 Transform 정의\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # DataLoader I/O 튜닝\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        # CPU 코어를 최대한 활용하여 데이터를 미리 GPU 메모리로 올리는 작업을 병렬 처리\n",
    "        num_workers=min(8, os.cpu_count()), \n",
    "        pin_memory=True, # GPU로의 데이터 전송 속도 향상\n",
    "        persistent_workers=True, # 워커 프로세스를 계속 유지하여 오버헤드 감소\n",
    "        prefetch_factor=2, # 각 워커가 미리 로드할 배치 수\n",
    "        drop_last=True # 마지막 배치가 배치 사이즈보다 작을 경우 버려서 연산 일관성 유지\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=min(8, os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        weight_decay=1e-4, #과적합 방지를 위한 정규화 기법(Weight Decay), 학습을 방해함으로서 과적합 방지.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1) # 7 에폭마다 학습률을 0.1배로 감소\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS, \n",
    "                                patience=EARLY_STOPPING_PATIENCE,\n",
    "                                steps_per_epoch=STEPS_PER_EPOCH\n",
    "                                )\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feellog-project (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
