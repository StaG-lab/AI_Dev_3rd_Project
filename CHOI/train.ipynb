{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0abd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 2210\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'resnet18' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 2.2997\n",
      "  Batch 20/139 Loss: 1.8917\n",
      "  Batch 30/139 Loss: 1.9244\n",
      "  Batch 40/139 Loss: 1.7704\n",
      "  Batch 50/139 Loss: 1.8724\n",
      "  Batch 60/139 Loss: 2.2426\n",
      "  Batch 70/139 Loss: 1.9314\n",
      "  Batch 80/139 Loss: 1.7778\n",
      "  Batch 90/139 Loss: 1.8801\n",
      "  Batch 100/139 Loss: 2.2224\n",
      "  Batch 110/139 Loss: 1.9506\n",
      "  Batch 120/139 Loss: 2.0037\n",
      "  Batch 130/139 Loss: 2.0805\n",
      "Train Loss: 1.9858 Acc: 0.2054\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.7440\n",
      "  Batch 20/139 Loss: 2.0319\n",
      "  Batch 30/139 Loss: 1.7554\n",
      "  Batch 40/139 Loss: 2.0765\n",
      "  Batch 50/139 Loss: 1.9001\n",
      "  Batch 60/139 Loss: 1.8847\n",
      "  Batch 70/139 Loss: 1.8932\n",
      "  Batch 80/139 Loss: 2.0150\n",
      "  Batch 90/139 Loss: 1.6017\n",
      "  Batch 100/139 Loss: 1.8648\n",
      "  Batch 110/139 Loss: 1.8648\n",
      "  Batch 120/139 Loss: 1.6670\n",
      "  Batch 130/139 Loss: 1.8681\n",
      "Train Loss: 1.8417 Acc: 0.2566\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.5745\n",
      "  Batch 20/139 Loss: 1.4889\n",
      "  Batch 30/139 Loss: 1.8685\n",
      "  Batch 40/139 Loss: 1.6589\n",
      "  Batch 50/139 Loss: 1.9467\n",
      "  Batch 60/139 Loss: 1.8381\n",
      "  Batch 70/139 Loss: 1.9682\n",
      "  Batch 80/139 Loss: 1.6769\n",
      "  Batch 90/139 Loss: 2.2120\n",
      "  Batch 100/139 Loss: 1.7285\n",
      "  Batch 110/139 Loss: 1.7819\n",
      "  Batch 120/139 Loss: 1.6649\n",
      "  Batch 130/139 Loss: 1.5752\n",
      "Train Loss: 1.7517 Acc: 0.3000\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.7170\n",
      "  Batch 20/139 Loss: 1.5808\n",
      "  Batch 30/139 Loss: 1.6664\n",
      "  Batch 40/139 Loss: 2.1334\n",
      "  Batch 50/139 Loss: 1.7587\n",
      "  Batch 60/139 Loss: 1.5442\n",
      "  Batch 70/139 Loss: 1.9926\n",
      "  Batch 80/139 Loss: 1.8019\n",
      "  Batch 90/139 Loss: 1.7292\n",
      "  Batch 100/139 Loss: 1.4548\n",
      "  Batch 110/139 Loss: 1.5367\n",
      "  Batch 120/139 Loss: 1.5021\n",
      "  Batch 130/139 Loss: 1.8778\n",
      "Train Loss: 1.7275 Acc: 0.3068\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.6799\n",
      "  Batch 20/139 Loss: 1.4963\n",
      "  Batch 30/139 Loss: 1.4990\n",
      "  Batch 40/139 Loss: 1.7744\n",
      "  Batch 50/139 Loss: 1.3053\n",
      "  Batch 60/139 Loss: 1.5804\n",
      "  Batch 70/139 Loss: 1.8529\n",
      "  Batch 80/139 Loss: 1.5451\n",
      "  Batch 90/139 Loss: 1.6882\n",
      "  Batch 100/139 Loss: 1.7703\n",
      "  Batch 110/139 Loss: 1.8102\n",
      "  Batch 120/139 Loss: 1.6431\n",
      "  Batch 130/139 Loss: 1.6229\n",
      "Train Loss: 1.6184 Acc: 0.3561\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.6439\n",
      "  Batch 20/139 Loss: 1.6650\n",
      "  Batch 30/139 Loss: 1.4744\n",
      "  Batch 40/139 Loss: 1.0515\n",
      "  Batch 50/139 Loss: 1.4190\n",
      "  Batch 60/139 Loss: 1.4464\n",
      "  Batch 70/139 Loss: 1.2032\n",
      "  Batch 80/139 Loss: 1.2906\n",
      "  Batch 90/139 Loss: 1.3129\n",
      "  Batch 100/139 Loss: 1.2697\n",
      "  Batch 110/139 Loss: 1.5059\n",
      "  Batch 120/139 Loss: 1.5786\n",
      "  Batch 130/139 Loss: 1.6224\n",
      "Train Loss: 1.5124 Acc: 0.4027\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.6966\n",
      "  Batch 20/139 Loss: 1.3173\n",
      "  Batch 30/139 Loss: 1.6651\n",
      "  Batch 40/139 Loss: 1.5164\n",
      "  Batch 50/139 Loss: 1.6724\n",
      "  Batch 60/139 Loss: 1.3267\n",
      "  Batch 70/139 Loss: 1.4337\n",
      "  Batch 80/139 Loss: 1.4264\n",
      "  Batch 90/139 Loss: 1.2887\n",
      "  Batch 100/139 Loss: 1.6234\n",
      "  Batch 110/139 Loss: 1.4829\n",
      "  Batch 120/139 Loss: 1.1760\n",
      "  Batch 130/139 Loss: 1.2673\n",
      "Train Loss: 1.5019 Acc: 0.4131\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.6744\n",
      "  Batch 20/139 Loss: 1.9031\n",
      "  Batch 30/139 Loss: 1.6828\n",
      "  Batch 40/139 Loss: 1.5752\n",
      "  Batch 50/139 Loss: 0.9901\n",
      "  Batch 60/139 Loss: 1.6776\n",
      "  Batch 70/139 Loss: 1.2592\n",
      "  Batch 80/139 Loss: 1.2263\n",
      "  Batch 90/139 Loss: 1.6237\n",
      "  Batch 100/139 Loss: 1.5152\n",
      "  Batch 110/139 Loss: 1.5350\n",
      "  Batch 120/139 Loss: 1.5981\n",
      "  Batch 130/139 Loss: 2.1237\n",
      "Train Loss: 1.3958 Acc: 0.4652\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.3410\n",
      "  Batch 20/139 Loss: 1.2155\n",
      "  Batch 30/139 Loss: 1.1363\n",
      "  Batch 40/139 Loss: 1.4297\n",
      "  Batch 50/139 Loss: 1.0371\n",
      "  Batch 60/139 Loss: 1.2811\n",
      "  Batch 70/139 Loss: 1.2290\n",
      "  Batch 80/139 Loss: 1.2277\n",
      "  Batch 90/139 Loss: 1.5300\n",
      "  Batch 100/139 Loss: 1.5069\n",
      "  Batch 110/139 Loss: 0.9735\n",
      "  Batch 120/139 Loss: 1.0941\n",
      "  Batch 130/139 Loss: 1.6833\n",
      "Train Loss: 1.2786 Acc: 0.5086\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.1399\n",
      "  Batch 20/139 Loss: 1.5772\n",
      "  Batch 30/139 Loss: 1.0595\n",
      "  Batch 40/139 Loss: 1.5928\n",
      "  Batch 50/139 Loss: 0.8005\n",
      "  Batch 60/139 Loss: 1.8577\n",
      "  Batch 70/139 Loss: 1.3426\n",
      "  Batch 80/139 Loss: 1.8157\n",
      "  Batch 90/139 Loss: 1.5928\n",
      "  Batch 100/139 Loss: 1.3507\n",
      "  Batch 110/139 Loss: 1.3500\n",
      "  Batch 120/139 Loss: 1.6024\n",
      "  Batch 130/139 Loss: 1.5181\n",
      "Train Loss: 1.2260 Acc: 0.5281\n",
      "\n",
      "Training complete in 41m 32s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 설정값 정의\n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_1_percent\")\n",
    "    MODEL_NAME = 'resnet18'\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    # 데이터 준비\n",
    "    dataset = EmotionDataset(data_dir=DATA_DIR)\n",
    "    # 실제 클래스 수를 데이터셋에서 가져와 업데이트\n",
    "    NUM_CLASSES = len(dataset.classes)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, dataloader, criterion, optimizer, num_epochs=NUM_EPOCHS)\n",
    "    \n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96604cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 1781\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'resnet18' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/10\n",
      "----------\n",
      "Train Loss: 1.6376 Acc: 0.3751\n",
      "Val Loss: 1.7085 Acc: 0.3849\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Loss: 1.3697 Acc: 0.4818\n",
      "Val Loss: 1.4288 Acc: 0.4430\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Loss: 1.2256 Acc: 0.5441\n",
      "Val Loss: 1.4799 Acc: 0.4645\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Loss: 1.0943 Acc: 0.6070\n",
      "Val Loss: 1.4241 Acc: 0.4602\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Loss: 0.9580 Acc: 0.6536\n",
      "Val Loss: 2.0359 Acc: 0.4258\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Loss: 0.8120 Acc: 0.7041\n",
      "Val Loss: 2.3757 Acc: 0.3871\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Loss: 0.7360 Acc: 0.7367\n",
      "Val Loss: 1.6722 Acc: 0.4860\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Loss: 0.4806 Acc: 0.8293\n",
      "Val Loss: 2.2134 Acc: 0.4624\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Loss: 0.4149 Acc: 0.8518\n",
      "Val Loss: 1.9103 Acc: 0.4796\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Loss: 0.3017 Acc: 0.9051\n",
      "Val Loss: 2.1917 Acc: 0.5032\n",
      "\n",
      "Training complete in 18m 5s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_1_percent_verified_processed\")\n",
    "    MODEL_NAME = 'resnet18'\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\")\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\")\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, DEVICE, num_epochs=NUM_EPOCHS)\n",
    "    \n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ca893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 1781\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'resnet18' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/10\n",
      "----------\n",
      "Train Loss: 1.6543 Acc: 0.3678\n",
      "Val Loss: 1.5184 Acc: 0.4258\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Loss: 1.4312 Acc: 0.4576\n",
      "Val Loss: 1.7822 Acc: 0.4409\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Loss: 1.3846 Acc: 0.4874\n",
      "Val Loss: 1.6696 Acc: 0.4409\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Loss: 1.3004 Acc: 0.5126\n",
      "Val Loss: 1.3269 Acc: 0.5011\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Loss: 1.2705 Acc: 0.5222\n",
      "Val Loss: 1.3473 Acc: 0.5118\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Loss: 1.2593 Acc: 0.5334\n",
      "Val Loss: 1.4601 Acc: 0.4903\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Loss: 1.2002 Acc: 0.5503\n",
      "Val Loss: 1.5093 Acc: 0.4667\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Loss: 1.1056 Acc: 0.6036\n",
      "Val Loss: 1.5257 Acc: 0.4989\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Loss: 1.1043 Acc: 0.5856\n",
      "Val Loss: 1.3051 Acc: 0.5656\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Loss: 1.1080 Acc: 0.5794\n",
      "Val Loss: 1.2696 Acc: 0.5677\n",
      "\n",
      "Training complete in 1m 53s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_1_percent_verified_processed\")\n",
    "    MODEL_NAME = 'resnet18'\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "        transforms.RandomRotation(15),           # -15도 ~ 15도 사이로 랜덤 회전\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 밝기, 대비, 채도 조절\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 증강이 없는 검증/테스트용 Transform 정의\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, DEVICE, num_epochs=NUM_EPOCHS)\n",
    "    \n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1f153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 1781\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'resnet18' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/10\n",
      "----------\n",
      "Train Loss: 1.7537 Acc: 0.3217\n",
      "Val Loss: 1.7876 Acc: 0.3419\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Loss: 1.4793 Acc: 0.4301\n",
      "Val Loss: 1.5791 Acc: 0.4172\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Loss: 1.3821 Acc: 0.4739\n",
      "Val Loss: 1.7380 Acc: 0.4301\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Loss: 1.3518 Acc: 0.4941\n",
      "Val Loss: 1.6685 Acc: 0.3591\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Loss: 1.2540 Acc: 0.5255\n",
      "Val Loss: 1.5061 Acc: 0.4301\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Loss: 1.2534 Acc: 0.5182\n",
      "Val Loss: 1.2826 Acc: 0.5247\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Loss: 1.1835 Acc: 0.5609\n",
      "Val Loss: 1.3643 Acc: 0.5161\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Loss: 1.1819 Acc: 0.5615\n",
      "Val Loss: 1.3488 Acc: 0.5183\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Loss: 1.1253 Acc: 0.5705\n",
      "Val Loss: 1.3381 Acc: 0.4968\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Loss: 1.1238 Acc: 0.5879\n",
      "Val Loss: 1.5432 Acc: 0.4731\n",
      "\n",
      "Training complete in 1m 25s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_1_percent_verified_processed\")\n",
    "    MODEL_NAME = 'resnet18'\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "        transforms.RandomRotation(15),           # -15도 ~ 15도 사이로 랜덤 회전\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 밝기, 대비, 채도 조절\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 증강이 없는 검증/테스트용 Transform 정의\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # 각 데이터셋에 맞는 Transform 적용\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) \n",
    "\n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, DEVICE, num_epochs=NUM_EPOCHS)\n",
    "    \n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66595eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_5_percent_verified_processed\")\n",
    "    # 사용하고자 하는 모델 하나만 남기고 다른 MODEL_NAME 앞에 # 붙여서 주석처리\n",
    "    #MODEL_NAME = 'resnet18'             #철원\n",
    "    #MODEL_NAME = 'resnet50' \n",
    "    #MODEL_NAME = 'mobilenet_v3_small'  #승현님\n",
    "    #MODEL_NAME = 'shufflenet_v2'       #철원\n",
    "    MODEL_NAME = 'efficientnet_v2_s'   #규진님\n",
    "    #MODEL_NAME = 'squeezenet'          #승희님\n",
    "    \n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 10 # 10번 연속 성능 개선이 없으면 조기 종료\n",
    "    \n",
    "    # 데이터 증강을 포함한 훈련용 Transform 정의\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우 반전\n",
    "        transforms.RandomRotation(15),           # -15도 ~ 15도 사이로 랜덤 회전\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # 밝기, 대비, 채도 조절\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # 증강이 없는 검증/테스트용 Transform 정의\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\", transform=train_transform)\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\", transform=val_transform)\n",
    "\n",
    "    # 데이터로더를 각각 생성. (검증용은 섞을 필요가 없음)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        #weight_decay=1e-4, #과적합 방지를 위한 가중치 감쇠를 넣었으나 오히려 학습에 방해가 되고 있음.\n",
    "        lr=LEARNING_RATE \n",
    "        ) \n",
    "    scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, \n",
    "                                train_loader, \n",
    "                                val_loader, \n",
    "                                criterion, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                DEVICE, \n",
    "                                num_epochs=NUM_EPOCHS, \n",
    "                                patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feellog-project (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
