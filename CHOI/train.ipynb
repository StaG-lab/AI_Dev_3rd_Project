{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0abd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 준비 완료!\n",
      "훈련 데이터셋 크기: 2210\n",
      "클래스 수: 7 -> ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
      "'resnet18' 모델, 손실 함수, 옵티마이저 준비 완료!\n",
      "\n",
      "모델 훈련을 시작합니다...\n",
      "Epoch 1/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 2.2997\n",
      "  Batch 20/139 Loss: 1.8917\n",
      "  Batch 30/139 Loss: 1.9244\n",
      "  Batch 40/139 Loss: 1.7704\n",
      "  Batch 50/139 Loss: 1.8724\n",
      "  Batch 60/139 Loss: 2.2426\n",
      "  Batch 70/139 Loss: 1.9314\n",
      "  Batch 80/139 Loss: 1.7778\n",
      "  Batch 90/139 Loss: 1.8801\n",
      "  Batch 100/139 Loss: 2.2224\n",
      "  Batch 110/139 Loss: 1.9506\n",
      "  Batch 120/139 Loss: 2.0037\n",
      "  Batch 130/139 Loss: 2.0805\n",
      "Train Loss: 1.9858 Acc: 0.2054\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.7440\n",
      "  Batch 20/139 Loss: 2.0319\n",
      "  Batch 30/139 Loss: 1.7554\n",
      "  Batch 40/139 Loss: 2.0765\n",
      "  Batch 50/139 Loss: 1.9001\n",
      "  Batch 60/139 Loss: 1.8847\n",
      "  Batch 70/139 Loss: 1.8932\n",
      "  Batch 80/139 Loss: 2.0150\n",
      "  Batch 90/139 Loss: 1.6017\n",
      "  Batch 100/139 Loss: 1.8648\n",
      "  Batch 110/139 Loss: 1.8648\n",
      "  Batch 120/139 Loss: 1.6670\n",
      "  Batch 130/139 Loss: 1.8681\n",
      "Train Loss: 1.8417 Acc: 0.2566\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.5745\n",
      "  Batch 20/139 Loss: 1.4889\n",
      "  Batch 30/139 Loss: 1.8685\n",
      "  Batch 40/139 Loss: 1.6589\n",
      "  Batch 50/139 Loss: 1.9467\n",
      "  Batch 60/139 Loss: 1.8381\n",
      "  Batch 70/139 Loss: 1.9682\n",
      "  Batch 80/139 Loss: 1.6769\n",
      "  Batch 90/139 Loss: 2.2120\n",
      "  Batch 100/139 Loss: 1.7285\n",
      "  Batch 110/139 Loss: 1.7819\n",
      "  Batch 120/139 Loss: 1.6649\n",
      "  Batch 130/139 Loss: 1.5752\n",
      "Train Loss: 1.7517 Acc: 0.3000\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.7170\n",
      "  Batch 20/139 Loss: 1.5808\n",
      "  Batch 30/139 Loss: 1.6664\n",
      "  Batch 40/139 Loss: 2.1334\n",
      "  Batch 50/139 Loss: 1.7587\n",
      "  Batch 60/139 Loss: 1.5442\n",
      "  Batch 70/139 Loss: 1.9926\n",
      "  Batch 80/139 Loss: 1.8019\n",
      "  Batch 90/139 Loss: 1.7292\n",
      "  Batch 100/139 Loss: 1.4548\n",
      "  Batch 110/139 Loss: 1.5367\n",
      "  Batch 120/139 Loss: 1.5021\n",
      "  Batch 130/139 Loss: 1.8778\n",
      "Train Loss: 1.7275 Acc: 0.3068\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.6799\n",
      "  Batch 20/139 Loss: 1.4963\n",
      "  Batch 30/139 Loss: 1.4990\n",
      "  Batch 40/139 Loss: 1.7744\n",
      "  Batch 50/139 Loss: 1.3053\n",
      "  Batch 60/139 Loss: 1.5804\n",
      "  Batch 70/139 Loss: 1.8529\n",
      "  Batch 80/139 Loss: 1.5451\n",
      "  Batch 90/139 Loss: 1.6882\n",
      "  Batch 100/139 Loss: 1.7703\n",
      "  Batch 110/139 Loss: 1.8102\n",
      "  Batch 120/139 Loss: 1.6431\n",
      "  Batch 130/139 Loss: 1.6229\n",
      "Train Loss: 1.6184 Acc: 0.3561\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.6439\n",
      "  Batch 20/139 Loss: 1.6650\n",
      "  Batch 30/139 Loss: 1.4744\n",
      "  Batch 40/139 Loss: 1.0515\n",
      "  Batch 50/139 Loss: 1.4190\n",
      "  Batch 60/139 Loss: 1.4464\n",
      "  Batch 70/139 Loss: 1.2032\n",
      "  Batch 80/139 Loss: 1.2906\n",
      "  Batch 90/139 Loss: 1.3129\n",
      "  Batch 100/139 Loss: 1.2697\n",
      "  Batch 110/139 Loss: 1.5059\n",
      "  Batch 120/139 Loss: 1.5786\n",
      "  Batch 130/139 Loss: 1.6224\n",
      "Train Loss: 1.5124 Acc: 0.4027\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.6966\n",
      "  Batch 20/139 Loss: 1.3173\n",
      "  Batch 30/139 Loss: 1.6651\n",
      "  Batch 40/139 Loss: 1.5164\n",
      "  Batch 50/139 Loss: 1.6724\n",
      "  Batch 60/139 Loss: 1.3267\n",
      "  Batch 70/139 Loss: 1.4337\n",
      "  Batch 80/139 Loss: 1.4264\n",
      "  Batch 90/139 Loss: 1.2887\n",
      "  Batch 100/139 Loss: 1.6234\n",
      "  Batch 110/139 Loss: 1.4829\n",
      "  Batch 120/139 Loss: 1.1760\n",
      "  Batch 130/139 Loss: 1.2673\n",
      "Train Loss: 1.5019 Acc: 0.4131\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.6744\n",
      "  Batch 20/139 Loss: 1.9031\n",
      "  Batch 30/139 Loss: 1.6828\n",
      "  Batch 40/139 Loss: 1.5752\n",
      "  Batch 50/139 Loss: 0.9901\n",
      "  Batch 60/139 Loss: 1.6776\n",
      "  Batch 70/139 Loss: 1.2592\n",
      "  Batch 80/139 Loss: 1.2263\n",
      "  Batch 90/139 Loss: 1.6237\n",
      "  Batch 100/139 Loss: 1.5152\n",
      "  Batch 110/139 Loss: 1.5350\n",
      "  Batch 120/139 Loss: 1.5981\n",
      "  Batch 130/139 Loss: 2.1237\n",
      "Train Loss: 1.3958 Acc: 0.4652\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.3410\n",
      "  Batch 20/139 Loss: 1.2155\n",
      "  Batch 30/139 Loss: 1.1363\n",
      "  Batch 40/139 Loss: 1.4297\n",
      "  Batch 50/139 Loss: 1.0371\n",
      "  Batch 60/139 Loss: 1.2811\n",
      "  Batch 70/139 Loss: 1.2290\n",
      "  Batch 80/139 Loss: 1.2277\n",
      "  Batch 90/139 Loss: 1.5300\n",
      "  Batch 100/139 Loss: 1.5069\n",
      "  Batch 110/139 Loss: 0.9735\n",
      "  Batch 120/139 Loss: 1.0941\n",
      "  Batch 130/139 Loss: 1.6833\n",
      "Train Loss: 1.2786 Acc: 0.5086\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "  Batch 10/139 Loss: 1.1399\n",
      "  Batch 20/139 Loss: 1.5772\n",
      "  Batch 30/139 Loss: 1.0595\n",
      "  Batch 40/139 Loss: 1.5928\n",
      "  Batch 50/139 Loss: 0.8005\n",
      "  Batch 60/139 Loss: 1.8577\n",
      "  Batch 70/139 Loss: 1.3426\n",
      "  Batch 80/139 Loss: 1.8157\n",
      "  Batch 90/139 Loss: 1.5928\n",
      "  Batch 100/139 Loss: 1.3507\n",
      "  Batch 110/139 Loss: 1.3500\n",
      "  Batch 120/139 Loss: 1.6024\n",
      "  Batch 130/139 Loss: 1.5181\n",
      "Train Loss: 1.2260 Acc: 0.5281\n",
      "\n",
      "Training complete in 41m 32s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 설정값 정의\n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_1_percent\")\n",
    "    MODEL_NAME = 'resnet18'\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    # 데이터 준비\n",
    "    dataset = EmotionDataset(data_dir=DATA_DIR)\n",
    "    # 실제 클래스 수를 데이터셋에서 가져와 업데이트\n",
    "    NUM_CLASSES = len(dataset.classes)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, dataloader, criterion, optimizer, num_epochs=NUM_EPOCHS)\n",
    "    \n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96604cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m EmotionDataset(data_dir\u001b[38;5;241m=\u001b[39mDATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 데이터로더를 각각 생성합니다. (검증용은 섞을 필요가 없습니다)\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m NUM_CLASSES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mclasses)\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 388\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work_Dev\\AI-Dev\\Projects\\3rd_Project_local\\feellog-project\\.venv\\lib\\site-packages\\torch\\utils\\data\\sampler.py:156\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from core.models.model_factory import create_model\n",
    "from core.data.dataset import EmotionDataset\n",
    "from core.training.trainer import train_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 설정값 정의\n",
    "    # 장치 설정: 사용 가능한 경우 GPU(cuda)를, 그렇지 않으면 CPU를 사용\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    DATA_DIR = Path(\"./datasets/korean_emotion_complex_vision_1_percent_preprocessed\")\n",
    "    MODEL_NAME = 'resnet18'\n",
    "    NUM_CLASSES = 7  # 데이터셋의 클래스 수에 맞게 조정해야 합니다. ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # 훈련용과 검증용 데이터셋을 각각 생성합니다.\n",
    "    train_dataset = EmotionDataset(data_dir=DATA_DIR / \"train\")\n",
    "    val_dataset = EmotionDataset(data_dir=DATA_DIR / \"val\")\n",
    "    \n",
    "    # 데이터로더를 각각 생성합니다. (검증용은 섞을 필요가 없습니다)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    NUM_CLASSES = len(train_dataset.classes)\n",
    "    \n",
    "    print(\"데이터 준비 완료!\")\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)}\")\n",
    "    print(f\"클래스 수: {NUM_CLASSES} -> {train_dataset.classes}\")\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 준비\n",
    "    model = create_model(model_name=MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "    # 모델을 지정된 장치로 이동\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    print(f\"'{MODEL_NAME}' 모델, 손실 함수, 옵티마이저 준비 완료!\")\n",
    "\n",
    "    # 모델 훈련 시작\n",
    "    print(\"\\n모델 훈련을 시작합니다...\")\n",
    "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, DEVICE, num_epochs=NUM_EPOCHS)\n",
    "    \n",
    "    # 훈련된 모델 저장 (옵션)\n",
    "    # torch.save(trained_model.state_dict(), f'{MODEL_NAME}_trained.pth')\n",
    "    # print(\"훈련된 모델 가중치가 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feellog-project (3.9.23)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
